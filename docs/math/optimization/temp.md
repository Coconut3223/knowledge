# ?

### Dynamic Step Size Adjusting:

- Decrease the step size by ratio $Î³\in(0,1)$  every K epochs.
- **Epoch Doubling Strategy**

    Run K epochs with step sizeÂ ð›¼, then, run 2K epochs with step sizeÂ ð›¼/2, ......

## Noise Reduction Methods

åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæœ€å¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•å°±æ˜¯SGD(éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•)ï¼Œæˆ‘ä»¬å‘çŽ°å®ƒåœ¨ä½†æ˜¯SGDç”±äºŽæ¯æ¬¡è¿­ä»£æ–¹å‘éƒ½æ˜¯éšæœºé€‰æ‹©çš„ï¼Œæ‰€ä»¥äº§ç”Ÿäº†ä¸‹é™æ–¹å‘çš„å™ªéŸ³çš„æ–¹å·®ã€‚æ­£æ˜¯ç”±äºŽè¿™ä¸ªæ–¹å·®çš„å­˜åœ¨ï¼Œæ‰€ä»¥å¯¼è‡´äº†SGDåœ¨æ¯æ¬¡è¿­ä»£ï¼ˆä½¿ç”¨å›ºå®šæ­¥é•¿Î±ï¼‰æ—¶ï¼Œæœ€ç»ˆæ˜¯æ”¶æ•›ä¸åˆ°æœ€ä¼˜å€¼çš„ï¼Œæœ€ä¼˜é—´éš”æœ€ç»ˆè¶‹äºŽä¸€ä¸ªå’Œæ–¹å·®æœ‰å…³çš„å®šå€¼ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é‡‡å–ä¸€ç§åŠžæ³•ï¼Œæ˜¯æƒ³è¦è®©æ–¹å·®å‘ˆé€’å‡è¶‹åŠ¿ä¸‹é™ï¼Œé‚£ä¹ˆæœ€ç»ˆç®—æ³•çš„å°†ä¼šä»¥çº¿æ€§é€Ÿåº¦æ”¶æ•›äºŽæœ€ä¼˜å€¼ã€‚

ðŸ’¡ To reduce the $\colorbox{aqua}{\text{B}}\text{ in } \colorbox{aqua}{\text{(1)}}$

### **Dynamic Sampling**

### **Gradient Aggregation**

ðŸ’¡ Correct the bias of the stochastic estimation of gradients using historical gradients estimates. ä½¿ç”¨åŽ†å²æ¢¯åº¦ä¼°è®¡æ¥çº æ­£æ¢¯åº¦éšæœºä¼°è®¡çš„åå·®ã€‚

**Stochastic Variance Reduced Gradient Method, SVRGï¼Œ** éšæœºæ–¹å·®ç¼©å‡æ¢¯åº¦ä¸‹é™æ³•

ä¸»è¦æŠ€å·§æ˜¯ç¼“å­˜ä¸€ä¸ªæ¢¯åº¦å‘é‡ï¼Œé€šè¿‡ç¼“å­˜å‘é‡ä¸Žæœ€æ–°æ¢¯åº¦çš„è·ç¦»èƒ½ç»™å‡ºæ¢¯åº¦æ–¹å·®çš„ä¸Šç•Œï¼Ž

Correct the bias of the stochastic estimation of gradients **using full batch gradient every m iterations.**

çœ‹åˆ°SVRGçš„è®¡ç®—é‡ç›¸æ¯”è¾ƒSGDå¤§å¾—å¤š

**Stochastic Average Gradient Accelerated, SAGA**

Correct the bias of the stochastic estimation of gradients **using historical stochastic gradient information.**

å…¶åŽï¼ŒSAGAï¼»Defazio et al.ï¼Œ2014ï¼½æ”¹è¿›äº†SAGç®—æ³•ï¼Œä½¿ç”¨éšæœºæ–¹å·®ç¼©å‡æ¢¯åº¦ä¸‹é™æ³•ï¼ˆSVRGï¼‰çš„æŠ€å·§å°†SAGä¸­çš„æ¢¯åº¦æ”¹æˆäº†æ— åçš„ä¼°è®¡ï¼Ž

### **Iteration Averaging**

- ðŸ“‘Â ref
Introductory Lectures on Convex Programming, Volume I: Basic course, Yu. Nesterov July 2, 1998

# Optimization Problem Setting

$$
\argmin\limits_{\theta\in\R^s}R_n(\theta):=\cfrac{1}{n}\sum\limits_{i=1}^nl(\theta;X_i,Y_i)
$$

Assumption:

1. $f\text{ is L-smooth}$ 
    
    $\colorbox{aqua}{\text{L-smooth}}\\\quad\begin{cases}f(\theta) \text{ is continuously differentialble}\iff f\in C^2(\R^n)\\\nabla f(\theta) \text{ is Lipschitz continuous}\iff \exist L>0, \Vert\nabla f(x)-\nabla f(y)\Vert\le L\Vert x-y\Vert\\\end{cases}\\\quad\implies f \text{ is L-smooth}$
    
    - proof:
        
        $\colorbox{aqua}{\text{Lemma 3.1}} \text{ Given a L-smooth function }f\\\quad \forall x,y\in\text{dom}(f),f(y)\le f(x)+\nabla f(x)^T(y-x)+\cfrac{L}{2}\Vert y-x\Vert^2$
        
        $**\implies \alpha_k:=\frac{1}{L}$ is feasible and optimal**
        
        - proof
            
            ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c93ca89b-e83a-457c-b37c-670a742cf27c/Untitled.jpeg)
            
            ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9ad40e5e-8076-42b1-9ad1-2396446b23bf/Untitled.jpeg)
