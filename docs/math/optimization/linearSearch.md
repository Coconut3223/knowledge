# Linear Search

![](./pics/GD_1.png)

## Background

### pre-info

==Structure Risk==. $\min\limits_{\theta\in\R^s}f(\theta):=\cfrac{1}{n}\sum\limits_{i=1}^nL_i(Y_i;f(X_i;\theta))$
==Gradient in multiple dimensions==. $\nabla f(x):=\begin{bmatrix}
\frac{\partial f}{\partial x_1}(x)\\
\vdots\\
\frac{\partial f}{\partial x_p}(x)\\
\end{bmatrix}=[Df(x)]^T \in \R^{p\times1}$

==L-smooth function==. $\begin{cases}f(\theta)\text{ is continuously differentiable }\\\nabla f(\theta)\text{ is Lipschitz continuous }\iff\Vert\nabla f(x)-\nabla f(y)\Vertâ‰¤L\Vert x-y\Vert,\text{ for some }L>0\end{cases}\\\qquad\implies f\text{ is  L-smooth }$
**Lemma 3.1** Given a L-smooth function $f,\forall x,y\in\text{dom}(f),f(y)\le f(x)+\nabla f(x)^T(y-x)+\cfrac{L}{2}\Vert y-x\Vert^2$

---

### Problem Setting

Assume: $f(\theta)$ is L-smooth
$$\min\limits_{\theta\in\R^s}f(\theta):=\cfrac{1}{n}\sum\limits_{i=1}^nL_i(Y_i;f(X_i;\theta))$$

!!! p "given a point $x^k$<br>1. find a descent direction $d^k$ <br>2. find a stepsize $\alpha^k$<br> $$x^{k+1}=x^{k}+\alpha^kd^k$$"
    å‡è®¾åœ¨æŸç‚¹ï¼Œå¯»æ‰¾æ–¹å‘ direction å’Œæ­¥é•¿ stepsize ä½¿å¾—æœ€å°ï¼Œå¦‚æœç¡®å®šåˆ™åªéœ€è¦è§£å†³ä¸€ç»´æœ€ä¼˜åŒ–é—®é¢˜å°±å¯ä»¥æ‰¾åˆ°ä¸‹ä¸€ä¸ªæœç´¢ç‚¹.
    é¦–å…ˆé€‰æ‹©æ–¹å‘Â $d^k$ é€šè¿‡è§£å†³ä¸€ç»´æœ€ä¼˜åŒ–é—®é¢˜æ‰¾åˆ°æ­¥é•¿Â $\alpha^k$

---

## Content

given a point $x^k$:

1. find a descent direction $d^k$
2. find a stepsize $\alpha^k$
3. $x^{k+1}=x^{k}+\alpha^kd^k$

==Descent Direction $d^k$==, $f\in C^1(\R^n),x\in \R^n$ A $d\in\R^n$ is said to be a **descent direction** of $f$ at $x\impliedby\red{[\nabla f(x)]^Td < 0}$.

- More generally, if $D\succeq0$, then $d = -D\nabla f(x)$ is a descent direction.
$\iff$ ä»»ä¸€æ–¹å‘ $d$ åªè¦èƒ½åˆ†è§£æˆä¸€ä¸ªæ­£å®šçŸ©é˜µ $D$ å’Œè´Ÿæ¢¯åº¦ $-\nabla f(x)$ çš„ä¹˜ç§¯ï¼Œé‚£ä¹ˆè¿™ä¸ªæ–¹å‘ä¸€å®šæ˜¯ä¸‹é™æ–¹å‘
Proofï¼š$[\nabla f(x)]^T\cdot\big(-D\nabla f(x)\big)=-\big(\nabla f(x)^TD\nabla f(x)\big)\\\qquad\qquad
    \because \nabla f(x)\not=0,\therefore <0$

!!! p "æ˜¯ä¸æ˜¯ä¸‹é™æ–¹å‘å°±çœ‹ï¼š$[\nabla f(x)]^Td < 0$"
    > At an x that is **not stationary**,
    > > d = $-\nabla f(x)$ is a descent directionï¼Ÿ
    >
    > yes. $[\nabla f(x)]^T\cdot-\nabla f(x)=-\Vert\nabla f(x)\Vert_2^2<0$
    >
    > > is the Newton direction $-[\nabla^2f(x)]^{-1}\nabla f(x)$  a **descent direction?**
    >
    > A: Not necessary. $\because d=[\nabla^2f(x)]^{-1}\nabla f(x),\therefore D=[\nabla^2f(x)]^{-1} ? \text{positive definite}\begin{cases}\in &\text{yes}\\\notin&\text{no}\end{cases}$

|  | $d^k=-D^k\nabla f(x^k),D\succeq0$ | descent direction |  |
| --- | --- | --- | --- |
| ç‰›é¡¿æ³• | $-[\nabla^2f(x^k)]^{-1}\nabla f(x^k)$ | ä¸€é˜¶å¯¼=0 $d^k= -[\nabla^2f(x^k)]^{-1}\cdot\nabla f(x^k)\text{ (not necessary) }\\\text{only }[\nabla^2f(x^k)]\succeq0$ | ä»…ä»…ä¾èµ–å‡½æ•°å€¼å’Œæ¢¯åº¦çš„ä¿¡æ¯(å³ä¸€é˜¶ä¿¡æ¯) |
| æœ€é€Ÿä¸‹é™æ³• | $-\nabla f(x^k)$ | è´Ÿæ¢¯åº¦æ–¹å‘ $d^k=-1I\cdot \nabla f(x^k),\checkmark$ |  |
| æ‹Ÿç‰›é¡¿æ³• | $-B^k\nabla f(x^k)$ | $d^k= -B^k\cdot\nabla f(x^k)\text{ (not necessary) }\\\text{only }B^k\succeq0$ |  |
| å…±è½­æ¢¯åº¦æ³• |  |  |  |

### direction

#### Gradient Descent, GD, Steepest descent æœ€é€Ÿä¸‹é™æ³•

==æ¢¯åº¦==,æŸä¸€ç‚¹å¤„çš„æ¢¯åº¦æ–¹å‘æ˜¯å‡½æ•°å€¼å¢é•¿æœ€å¿«çš„æ–¹å‘

$$\text{Start from some }\theta^0\in\R^s,\\\text{ GD updates as}\theta^{k+1}=\theta^k-\alpha_k\nabla f(\theta^k),\\\text{ until }\Vert\nabla f(\theta^{k+1})\Vert\le\epsilon,\text{ for some }\epsilon>0$$

!!! p "Steepest descent with exact line searchï¼š"
    å¸Œæœ›å¾—åˆ°ä¸€ä¸ª**åœ¨è¯¥ç‚¹ä¸‹é™æœ€å¿«çš„æ–¹å‘**ï¼Œæ¥ä½¿å¾—æˆ‘ä»¬çš„è¿­ä»£è¿‡ç¨‹å°½å¯èƒ½çš„é«˜æ•ˆã€‚
    **æ¢¯åº¦çš„åæ–¹å‘å°±æ˜¯å‡½æ•°å€¼ä¸‹é™æœ€å¿«çš„æ–¹å‘**ã€‚

!!! question "1. In practice: Always use analytic gradient, but check implementation with numerical gradient. This is called aÂ gradient check."

##### å­˜åœ¨æ€§è¯æ˜ï¼šè´Ÿæ¢¯åº¦æ–¹å‘å°±æ˜¯ä¸‹é™æœ€å¿«æ–¹å‘

==Taylorå±•å¼€==. $f\in C^1(\R), \exist \xi\in\{x+td:t\in(0,1)\}, f(x+d)=f(x)+[\nabla f(x)]^Td+[\nabla f(\xi)-\nabla f(x)]^Td$.
å…¶ä¸­æœ¬æ¥äºŒé˜¶å¯¼çš„åœ°æ–¹ï¼š $\frac{1}{2}d^T\nabla^2 f(\xi)=[\nabla f(\xi)-\nabla f(x)]^T;\xi\text{ depends on }d$
å¦‚æœ $\nabla f(x)â‰ 0 \iff x\text{ is not a stationary}$, thenï¼Œæˆ‘ä»¬å– $\red{d=-\alpha\nabla f(x) \text{ for some }\alpha>0}$
thenï¼Œ$f(x+d)=f(x)+[\nabla f(x)]^Td+[\nabla f(\xi)-\nabla f(x)]^Td$ å˜æˆ
$$f(x-\alpha\nabla f(x))=f(x)-\alpha\Vert\nabla f(x)\Vert_2^2-\alpha([\nabla f(\xi)-\nabla f(x)]^T\cdot\nabla f(x))$$
å…¶ä¸­ç¬¬2é¡¹ï¼š$\nabla f(x)\cdot \alpha\nabla f(x)=\alpha\Vert\nabla f(x)\Vert_2^2ï¼›\xi \text{ depends on }\alpha$
å…¶ä¸­ç¬¬3é¡¹ï¼š$\alpha([\nabla f(\xi)-\nabla f(x)]^T\cdot\nabla f(x))=0$
$$\implies f(x-\alpha\nabla f(x))=f(x)-\alpha\Vert\nabla f(x)\Vert_2^2$$
$$\therefore \text{ for sufficiently small }\alpha>0, f(x-\alpha\nabla f(x))<f(x)\space\text{(æ˜¯ä¸‹é™æ–¹å‘)}$$
$\implies\red{-\nabla f(x)} $ is called **the steepest descent direction**

- ä¸ºä»€ä¹ˆ $\alpha d:=-\alpha \nabla f(x) \text{ for some }\alpha>0,$(æ­¤å¤„çš„dèŒƒå›´æ›´ç¼©å°ä¸€ç‚¹ï¼ŒæŒ‡æ–¹å‘
    åœ¨æ²¡æœ‰ç»™å®šdä¹‹å‰ï¼š$f(x+d)=f(x)+\alpha\nabla f(x)^Td$
    âˆµ Given $ f\& x \implies f(x)\&\nabla f(x)^T\in$ å¸¸é‡
    $\min\limits_\alpha f(x+d)=f(x)+\alpha\nabla f(x)^Td$ æ˜¯å…³äº$\alpha$çš„å‡½æ•°ï¼Œè¦éšç€$\alpha$å¢åŠ è€Œå‡å°ï¼Œä¸”å‡å°‘å¾—å°½å¯èƒ½å¿«ï¼Œ
    $$\therefore d^k=\min_{d^k} \cfrac{\partial f}{\partial \alpha} = \max_{d^k} -\cfrac{\partial f}{\partial \alpha}$$
    Recallï¼š ==Cauchyä¸ç­‰å¼==. $-\frac{\partial f}{\partial \alpha}=-\nabla f(x)^Td=(-\nabla f(x),d)=\Vert-\nabla f(x)\Vert\cdot\Vert d\Vert\cdot\cos\theta_k$.  $\theta_k$å°±æ˜¯æœç´¢æ–¹å‘då’Œè´Ÿæ¢¯åº¦æ–¹å‘çš„è§’åº¦ï¼Œå½“$\theta_k=0Â°$ æ—¶ï¼Œæœ€å¤§ï¼Œæ‰€ä»¥å°±æ˜¯æœ€é€Ÿ.
    $$\implies \red{d=- \nabla f(x)}$$

##### Steepest descent with exact line search

$\text{Start at }x^0\in R^n. \text{ For each }k=0,1,â€¦$

1. $\text{Set }d^k=-\nabla f(x^k)\qquad\text{(the search direction)}$
2. $\text{pick } \alpha_k\in\argmin\{f(x^k+\alpha d^k):\alpha>0\}\qquad\text{(step size | learning rate)}$

å…¶ä¸­ $\alpha_k$ is chosen according to the exact line search criterion é€šè¿‡**ç²¾ç¡®çº¿æœç´¢**ç¡®å®šæ­¥é•¿ï¼ˆéšå«åœ°å‡å®šï¼Œå¯¹äºç²¾ç¡®çº¿æœç´¢ï¼Œå­˜åœ¨ä¸€ä¸ªæœ€å°åŒ–å™¨$\alpha_k$ã€‚ï¼‰

##### Convergence of Steepest descent with constant stepsize

$\begin{cases}f\in C^2(\R^n)\\\inf f >-\infin\\\exist L>0, L\ge\Vert\nabla^2f(x)\Vert_2\\x^{k+1}=x^k-\cfrac{\gamma}{L}\nabla f(x^k)&(1)\end{cases}\implies\forall x, \text{fix any }\gamma\in(0,2)$ any accumulation point of $\{x^k\}$ is a stationary point of $f$
$(1)$: the formula as which sequence generated
ä¹Ÿå°±æ˜¯è¯´ï¼Œåªè¦ stepsize $Î± \in(0, \cfrac{2}{L}),$ æˆ‘ä»¬å°±èƒ½è¾¾åˆ°æ”¶æ•›çš„ç›®æ ‡ã€‚

- proof:

==Lemmma==
$f\text{ is L-smooth }\\
\implies \forall x,y\in \text{dom}(f), f(y)\le f(x)+\nabla f(x)^T(y-x)+\cfrac{L}{2}\Vert y-x\Vert^2\\
\implies f(y)-f(x)\le\nabla f(x)^T(y-x)+\cfrac{L}{2}\Vert y-x\Vert^2$
ç”±ä¸Šé¢çš„å¼•ç†ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹ stepsize è¿›è¡Œè®¨è®º, ç›®æ ‡æ˜¯ï¼š$f(\theta^{k+1})-f(\theta^k)<0$
$$\begin{align*}f(\theta^{k+1})-f(\theta^k)&=f(\theta^k-\alpha_k\nabla f(\theta^k))-f(\theta^k)\\&\le\nabla f(\theta^k)^T(-\alpha_k\nabla f(\theta^k))+\cfrac{L}{2}\Vert\alpha_k\nabla f(\theta^k)\Vert^2\\&\le(\cfrac{L}{2}\alpha_k-1)\cdot\alpha_k\Vert \nabla f(\theta^k)\Vert^2\quad \colorbox{aqua}{\text{res-1}}\end{align*}\\\implies \cfrac{L\alpha_k}{2}-1<0\implies\alpha_k<\cfrac{2}{L}\rightarrow\cfrac{\gamma}{L}
$$

ä½†æ˜¯ **what is the optimal step size in the constant step-size strategyï¼Ÿ**

$\colorbox{aqua}{\text{res-1}} \quad \Delta= f(\theta^{k+1})-f(\theta^k)\le(\cfrac{L}{2}\alpha_k-1)\cdot\alpha_k\Vert \nabla f(\theta^k)\Vert^2\\\qquad\Delta_{\alpha_k}:= \cfrac{L\alpha_k^2}{2}-\alpha_k<0,\therefore  \alpha_k^*=\argmin\limits_{\alpha_k }\cfrac{L\alpha_k^2}{2}-\alpha_k \\\qquad\quad \cfrac{\partial\Delta_{\alpha_k}}{\partial\alpha_k}=L\alpha_k-1\xlongequal{SET}0\implies \alpha_k^*=\cfrac{1}{L}$

**Convergence of GD with optimal constant step-size** $\alpha=\frac{1}{L}$

$\begin{cases}f\text{ is L-smooth }\\\inf f>-\infin\\\{\theta^k\}_{k=0}^T:=\text{ the sequence by GD with }\alpha=\cfrac{1}{L}\end{cases}\\\qquad\implies \forall\theta^0, \min\limits_{1\le k\le T}\Vert\nabla f(\theta^k)\Vert^2\le\cfrac{2L(f(\theta^0)-\inf f)}{T}$

ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœæˆ‘ä»¬å–è¿™ä¸ª optimal step sizeï¼Œå¯¹ä»»ä½•çš„åˆå§‹å‚æ•°(åˆ†å­ constant )ï¼Œåªè¦ run è¶³å¤Ÿå¤šæ¬¡(åˆ†æ¯ T large enough), è¿™ä¸ªæ¢¯åº¦ $\nabla f(x)$ æ€»èƒ½æ”¶æ•›åˆ°æ¥è¿‘0ï¼Œä¹Ÿå°±æ˜¯ $f(x)$ è¾¾åˆ°æˆ‘ä»¬çš„ local minimizerã€‚

æ­¥é•¿ä¿å®ˆ the constant stepsizeï¼Œä¸‹é™ç¼“æ…¢ potentially

##### Expensive computation

!!! danger "åœ¨å¾ˆé«˜ç»´çš„æƒ…å†µä¸‹ï¼Œè®¡ç®—å¾ˆå¤æ‚å¾ˆå¥½è€—æ—¶é—´ã€‚ ç”¨å…¨éƒ¨æ ·æœ¬**to estimate the exact gradient of a random variableã€‚**"
    ==while computing gradient in GD==ã€‚ç®—æ¯ä¸€ä¸ªæ ·æœ¬ç‚¹çš„æ¢¯åº¦ï¼Œå¦‚æœæœ‰ n ä¸ªæ ·æœ¬ï¼Œå°±è¦ç®— n ä¸ªæ¢¯åº¦ï¼Œ**ä½¿ç”¨äº†æ‰€æœ‰è®­ç»ƒæ•°æ®çš„è¯¯å·®,** ç„¶åå†ç®—å¥¹ä»¬çš„å¹³å‡å€¼æ¥å……å½“å‚æ•°ã€‚ This computation isÂ expensiveÂ ifÂ n is hugeÂ !!!
    $$\nabla f(\theta^k)=\cfrac{1}{n}\sum\limits_{i=1}^n\nabla_\theta l(\theta^k;X_i,Y)$$

æ‰€ä»¥ç°åœ¨ä¸æ˜¯çœŸçš„å»ç®—å‡ºæ¥ï¼Œè€Œæ˜¯ç®—ç”¨ä¼°è®¡çš„æ³•å­ã€‚
Instead of computing the exact gradient, we consider $g(\theta,\xi)$, which is **a estimation** satisfying $\mathop{E}\limits_\xi[g(\theta,\xi)]=\nabla f(\theta)$

$$\mathop{E}\limits_Î¾[g(\theta,Î¾)]\xlongequal{\text{estimate}}\nabla f(\theta)$$

|  | Assume Î¾ : | $g(\theta,Î¾)$ |notes|
| --- | --- | --- |---|
| Noisy gradients | a random noise satisfying E[Î¾]=0 | $\nabla f(\theta)+Î¾$||
| Stochastic gradients | an index uniformly sampling from {1, 2, ..., n} | $\nabla_\theta l(\theta;X_Î¾,Y_Î¾)$ |åŸºäº Batchsize çš„åˆ†ç±»å’Œè¿›åŒ–|

**assumption of** $g(\theta,\xi)$
$\begin{cases}f \text{ is convex }\\\mathbb E_\xi[g(\theta,\xi)]=\nabla f(\theta)&\text{(mean)}\\\text{given B, }\forall\theta,\mathbb E_\xi[\Vert g(\theta,\xi)\Vert^2]\le B^2&\text{(variance)}\end{cases}$

å› ä¸ºç”¨éšæœºå˜é‡æ¥ estimateï¼Œæ‰€ä»¥æœŸæœ›æœ¬æ¥å°±æ˜¯æˆ‘ä»¬æƒ³è¦çš„ $=\nabla f(\theta)$, æ–¹å·®è¿™é‡Œéœ€è¦è¢« bounded by B $\iff \sup \mathbb E_\xi[\Vert g(\theta,\xi)\Vert^2]\le B^2$

$\mathbb E_\xi\Vert g(\theta^k,\xi)\Vert$

#### æŠ½æ ·ä¼°è®¡æ€è·¯çš„æ¯”è¾ƒ

|  | Gradient Descent, GD | Stochastic Gradient Descent, SGD | Batch Gradient Descent, BGD | mini-batch SGD |
| --- | --- | --- | --- | --- |
| Batch_Size | None, All train data å¾—åˆ°æ¢¯åº¦ | 1, stochastic | - | subset, Batch Size |
| T(n) | ä½†æ•°æ®é‡å¤§æ—¶ï¼Œè®¡ç®—éå¸¸è€—æ—¶ï¼ŒåŒæ—¶ç¥ç»ç½‘ç»œå¸¸æ˜¯éå‡¸çš„ï¼Œç½‘ç»œæœ€ç»ˆå¯èƒ½æ”¶æ•›åˆ°åˆå§‹ç‚¹é™„è¿‘çš„å±€éƒ¨æœ€ä¼˜ç‚¹ | ^ |^  |^  |
| æ¢¯åº¦å‡†ç¡®æ€§ | Accurate but slow (è¿­ä»£æ¬¡æ•°å°‘ï¼Œè¿­ä»£ä¸€æ¬¡æ…¢ | Fast but fluctuant (è¿­ä»£ä¸€æ¬¡å¿«ï¼Œè¿­ä»£æ¬¡æ•°å¤šï¼Œè¿­ä»£è¿™ä¹ˆå¤šæ¬¡çš„whole process è¿˜æ˜¯ä¼šæ¯”GDå¿« | ^ | æ¢¯åº¦å‡†ç¡®äº†ï¼Œå­¦ä¹ ç‡è¦åŠ å¤§ã€‚ |
| ^ |^  |^  |^  | åˆ©ç”¨å™ªå£°æ¢¯åº¦ï¼Œä¸€å®šç¨‹åº¦ä¸Šç¼“è§£äº†GDç®—æ³•ç›´æ¥æ‰è¿›åˆå§‹ç‚¹é™„è¿‘çš„å±€éƒ¨æœ€ä¼˜å€¼ |

![](./pics/SGD_2.png)
![](./pics/SGD_3.png)

!!! danger "SGD åœ¨è®­ç»ƒ DNN æ—¶å¹¶ä¸å®¹æ˜“æ‰¾åˆ°å…¨å±€æœ€å°å€¼ç‚¹ã€‚"
    SGDæ˜¯åŸºäºéšæœºé‡‡æ ·çš„ï¼Œæ¯æ¬¡è¿­ä»£ä»…ä½¿ç”¨ä¸€ä¸ªæ ·æœ¬æˆ–ä¸€å°æ‰¹æ ·æœ¬æ¥ä¼°è®¡æ¢¯åº¦ï¼Œå› æ­¤å®ƒåœ¨å‚æ•°ç©ºé—´ä¸­æœç´¢æœ€å°å€¼çš„è·¯å¾„æ˜¯ä¸ç¡®å®šçš„ã€‚
    å¯¹äºå¤æ‚çš„æŸå¤±å‡½æ•°å’Œé«˜ç»´çš„å‚æ•°ç©ºé—´ï¼ŒDNNsé€šå¸¸å­˜åœ¨è®¸å¤šå±€éƒ¨æœ€å°å€¼å’Œéç‚¹ã€‚å±€éƒ¨æœ€å°å€¼æ˜¯æŸå¤±å‡½æ•°çš„å±€éƒ¨æå°å€¼ï¼Œè€Œéç‚¹æ˜¯åœ¨æŸä¸ªç»´åº¦ä¸Šæ˜¯å±€éƒ¨æå°å€¼ï¼Œè€Œåœ¨å¦ä¸€ä¸ªç»´åº¦ä¸Šæ˜¯å±€éƒ¨æå¤§å€¼ã€‚è¿™äº›å±€éƒ¨æœ€å°å€¼å’Œéç‚¹å¯èƒ½å¯¼è‡´SGDé™·å…¥å±€éƒ¨æœ€å°å€¼æˆ–æ”¶æ•›åˆ°ä¸ç†æƒ³çš„è§£ã€‚
    ä¸ºäº†å…‹æœè¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶äººå‘˜å‘å±•äº†ä¸€äº›æ”¹è¿›çš„ä¼˜åŒ–ç®—æ³•ï¼Œå¦‚éšæœºæ¢¯åº¦ä¸‹é™çš„å˜ç§ï¼ˆå¦‚åŠ¨é‡æ¢¯åº¦ä¸‹é™ã€Adamç­‰ï¼‰å’Œè‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³•ã€‚è¿™äº›ç®—æ³•é€šè¿‡å¼•å…¥åŠ¨é‡ã€è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡ç­‰æŠ€æœ¯æ¥æ›´æœ‰æ•ˆåœ°æœç´¢å‚æ•°ç©ºé—´ï¼Œå¹¶æœ‰åŠ©äºé¿å…é™·å…¥å±€éƒ¨æœ€å°å€¼

#### Stochastic Gradient Descent, SGD, éšæœºæ¢¯åº¦ä¸‹é™

Instead of computing the exact gradient, we consider $g(\theta,\xi)$, which is **a stochastic estimation** satisfying $\mathop{E}\limits_\xi[g(\theta,\xi)]=\nabla f(\theta)$

SGDå°±æ˜¯ä¸€æ¬¡è·‘**ä¸€å€‹æ¨£æœ¬**æˆ–æ˜¯å°æ‰¹æ¬¡(mini-batch)æ¨£æœ¬ç„¶å¾Œç®—å‡ºä¸€æ¬¡æ¢¯åº¦æˆ–æ˜¯å°æ‰¹æ¬¡æ¢¯åº¦çš„å¹³å‡å¾Œå°±æ›´æ–°ä¸€æ¬¡ï¼Œé‚£é€™å€‹æ¨£æœ¬æˆ–æ˜¯å°æ‰¹æ¬¡çš„æ¨£æœ¬æ˜¯éš¨æ©ŸæŠ½å–çš„ï¼Œæ‰€ä»¥æ‰æœƒç¨±ç‚º==éš¨æ©Ÿæ¢¯åº¦ä¸‹é™æ³•==ã€‚

==Stochastic Gradient==. $\xi$ is an index uniformly <u>sampling from </u> {1, 2, ..., n} è¢«éšæœºé€‰ä¸­çš„æ•°æ®ç´¢å¼•
 $ g(\theta,\xi)=\nabla_\theta l(\theta;X_\xi,Y_\xi)\rightarrow \mathbb E_\xi g(\theta,\xi)=\nabla f(\theta)$

==Stochastic Gradient Descent, SGD==
$$\text{Start from some }\theta^0\in\R^s,\\\text{ SGD updates as }\theta^{k+1}=\theta^k-\alpha_kg(\theta^k,Î¾_k)=\theta^k-\alpha_k\cdot\nabla f_{\theta_k}(x_{\xi_k})\\\xrightarrow{\text{approximate}}\theta^k-\alpha_k\nabla f(\theta_k)$$
$g(\theta^k,\xi_k):=$ the stochastic gradient computed at $\theta^k$
$\xi_k :=$ the selected index at k round

1. Sampling strategy to computeÂ ğ‘”(ğœƒ$, ğœ‰$).
2. Choose step sizeÂ ğ›¼$Â > 0Â **.**

    !!! danger "We need to choose decreasing step size. å¦‚æœæ˜¯constant å°±ä¸èƒ½ find ultimate value"
       - if $\alpha_j\equiv \alpha>0$ âŒÂ (é™¤éBé€’å‡
        $\xRightarrow{ \alpha_j\equiv \alpha>0}\mathbb E[f(\overline{\theta}^T)-f^*]\le\cfrac{\Vert\theta^0-\theta^*\Vert^2+B^2(T+1)\alpha^2}{2(T+1)}$
        $\xrightarrow{T->\infin}\hat{\theta}^T\text{ will be in a ball with radius }\frac{B^2\alpha^2}{2}$
        ![](./pics/SGD_4.png)

       - if $\alpha_j\downarrow$ e.g. $\alpha_t=\frac{1}{t+1}(\text{decreasing})$  â­•
        $\xRightarrow{\alpha_t=\frac{1}{t+1}}\begin{cases}\sum\limits_{t=0}^\infin\alpha_t=\sum\limits_{t=1}^\infin\cfrac{1}{t}=\infin\\\sum\limits_{t=0}^\infin\alpha_t^2=\sum\limits_{t=1}^\infin\cfrac{1}{t^2}=\cfrac{\pi^2}{6}<\infin\end{cases}$
        $\implies\lim\limits_{T\rightarrow\infin}\mathbb E[f(\overline{\theta}^T)-f^*]=0$

**Convergence of SGD with step-size** $\alpha>0$

$\begin{cases}f \text{ is convex }\iff f(\lambda u + (1-\lambda )v)\le\lambda f(u) + (1-\lambda )f(v).\\E_Î¾[g(\theta,Î¾)]=\nabla f(\theta)\\E_Î¾[\Vert g(\theta,Î¾)\Vert^2]\le B^2,\forall \theta, \text{given B}\\\{\theta^k\}:=\text{ the sequence by SGD with }\alpha>0\end{cases}$
$\implies E[f(\overline{\theta}^T)-f^*]\le\cfrac{\Vert\theta^0-\theta^*\Vert^2+B^2\sum\limits_{j=0}^T\alpha_j^2}{2\sum\limits_{j=0}^T\alpha_j},\begin{cases}\:\lambda_k=\sum\limits_{j=0}^k\alpha_j\\\overline{\theta}^k=\lambda_k^{-1}\sum\limits_{j=0}^k\alpha_j\theta^j\end{cases}$

$\xRightarrow{ \alpha_j=\alpha>0}E[f(\overline{\theta}^T)-f^*]\le\cfrac{\Vert\theta^0-\theta^*\Vert^2+B^2(T+1)\alpha^2}{2(T+1)}$

$\xrightarrow{T->\infin}\hat{\theta}^T=\cfrac{B^2\alpha^2}{2}$

$\xRightarrow{\alpha_t=\cfrac{1}{t+1}(\text{decreasing})}\begin{cases}\sum\limits_{t=0}^\infin\alpha_t=\sum\limits_{t=1}^\infin\cfrac{1}{t}=\infin\\\sum\limits_{t=0}^\infin\alpha_t^2=\sum\limits_{t=1}^\infin\cfrac{1}{t^2}=\cfrac{\pi^2}{6}<\infin\end{cases}$

!!! question "proof"

**Advantage:**

1. è®¡ç®—å¿«ï¼Œæœ€é€Ÿä¸‹é™æ³•æ›´æ–°1æ¬¡å‚æ•°çš„æ—¶é—´ï¼Œéšæœºæ¢¯åº¦ä¸‹é™æ³•å¯ä»¥æ›´æ–°næ¬¡

**Limitation:**

1. Slow convergenceï¼Œå› ä¸ºéšæœºï¼Œæ‰€ä»¥æ–¹å‘æ‘‡æ‘†ä¸å®šï¼Œæ”¶æ•›å¾ˆæ…¢,ç‹‚èµ°ä¹‹å­—è·¯çº¿ï¼Œå¿«çš„æ˜¯è®¡ç®—è¿‡ç¨‹ â‡’ <u>SGD with momentum</u>
2. converge to the local optimal solution
ä¸ç»æ„é—´æ”¶æ•›åˆ°localå°±åœäº†ï¼Œï¼ˆéšæœºæ¢¯åº¦ä¸‹é™æ³•ç”±äºè®­ç»ƒæ•°æ®æ˜¯éšæœºé€‰æ‹©çš„ï¼Œæ›´æ–°å‚æ•°æ—¶ä½¿ç”¨çš„åˆæ˜¯é€‰æ‹©æ•°æ®æ—¶çš„æ¢¯åº¦ï¼Œæ‰€ä»¥å®¹æ˜“é™·å…¥ç›®æ ‡å‡½æ•°çš„å±€éƒ¨æœ€ä¼˜è§£ã€‚
3. converge to saddle points
4. åªèƒ½ go to some neighbourhoods of the optimal solution
$\mathbb E[f(\overline{\theta}^T)-f^*]\le\cfrac{\Vert\theta^0-\theta^*\Vert^2+B^2(T+1)\alpha^2}{2(T+1)}\rightarrow\hat{\theta}^T\text{ on a ball with radius}\cfrac{B^2\alpha^2}{2}$

æ¯æ¬¡æ›´æ–°å‚æ•°æ—¶åªä½¿ç”¨ä¸€ä¸ªæ ·æœ¬æ¥è®¡ç®—æ¢¯åº¦ï¼Œè¿™æ ·å°±é¿å…äº†BGDè®¡ç®—éå¸¸ç¼“æ…¢çš„é—®é¢˜ï¼ŒåŒæ—¶SGDæ¯æ¬¡è®¡ç®—æ¢¯åº¦çš„æ ·æœ¬ä¸åŒï¼Œæ‰€ä»¥è®¡ç®—å‡ºæ¥çš„æ¢¯åº¦ä¸ç¨³å®šï¼Œä¼šå‡ºç°æŠ–åŠ¨ï¼Œæ­£æ˜¯è¿™ç§ä¸ç¨³å®šäº§ç”Ÿçš„æŠ–åŠ¨ï¼Œä½¿ç®—æ³•å¯èƒ½è·³å‡ºéç‚¹ä»è€Œæ‰¾åˆ°æ›´ä¼˜è§£ã€‚SGDæ¯æ¬¡åªä½¿ç”¨ä¸€ä¸ªæ ·æœ¬æ›´æ–°æ¢¯åº¦ï¼Œè®¡ç®—æ›´å¿«ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯æ–°å¢æ ·æœ¬ï¼Œå› æ­¤é€‚åˆonlineè®­ç»ƒã€‚

æ¯æ¬¡ä½¿ç”¨ä¸€ä¸ªæ ·æœ¬è®¡ç®—æ¢¯åº¦å…·æœ‰é«˜æ–¹å·®æ€§ï¼Œå®¹æ˜“å—åˆ°ç¦»ç¾¤ç‚¹æˆ–å¼‚å¸¸æ•°æ®çš„å¹²æ‰°ï¼Œåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ä¼šå‡ºç°ä¸¥é‡çš„æŠ–åŠ¨ï¼Œè¿™ç§éšæœºæ€§ä¾¿æ˜¯å…¶åå­—çš„ç”±æ¥

#### mini-batch SGD

è®¾éšæœºé€‰æ‹©mä¸ªè®­ç»ƒæ•°æ®çš„ç´¢å¼•çš„é›†åˆä¸ºKï¼Œ

å‡è®¾è®­ç»ƒæ•°æ®æœ‰100ä¸ªï¼Œé‚£ä¹ˆåœ¨m=10æ—¶ï¼Œåˆ›å»ºä¸€ä¸ªæœ‰10ä¸ªéšæœºæ•°çš„ç´¢å¼•çš„é›†åˆï¼Œä¾‹å¦‚K={61, 53, 59, 16, 30, 21, 85, 31, 51, 10}ï¼Œç„¶åé‡å¤æ›´æ–°å‚æ•°

==mini--batch Stochastic Gradient Descent, SGD==.
$$\text{Start from some }\theta^0\in\R^s,\\
\text{ SGD updates as }\large\theta^{k+1}=\theta^k-\alpha_k\mathop{\mathbb E}\limits_{\xi_k\in\Xi_k}g(\theta^k,Î¾_k)=\theta^k-\alpha_k\cdot\mathop{\mathbb E}\limits_{\xi_k\in\Xi_k}\nabla f_{\theta_k}(x_{\xi_k})$$
$g(\theta^k,\xi_k):=$ the stochastic gradient computed at $\theta^k$
$\Xi_k :=$ the selected indexes at k round(mini-batch)

In each epoch,Â $n_E$Â SGD updates will be executed. Usually, we select

$n_E=\text{ceil}(\frac{n}{p}),\quad\begin{cases}n:= \#\text{ samples in each epoch} = \#\text{ all samples}\\p:= \#\text{ samples in each mini-batch} \\n_E:=\#\text{batch in each epoch}\end{cases}$

ä¸åŒä»£ä¹‹é—´æ•°æ®è¦shuffle

é‡‡ç”¨ä¸€ä¸ª**å°æ‰¹é‡**çš„æ•°æ®è¿›è¡Œæ¢¯åº¦çš„è®¡ç®—ï¼Œå…¶ç›®çš„æ˜¯åœ¨ä¿è¯è®¡ç®—é€Ÿåº¦çš„åŒæ—¶ï¼Œé¿å…SGDçš„éšæœºæ€§ï¼Œé™ä½å‚æ•°æ›´æ–°æ—¶çš„æ–¹å·®ï¼Œä½¿æ”¶æ•›æ›´ç¨³å®šã€‚

MBGDå®¹æ˜“å—å­¦ä¹ ç‡çš„å½±å“ï¼šè®¾ç½®å¾—å¤ªå¤§å®¹æ˜“å‡ºç°ä¸SGDç±»ä¼¼çš„ä¸ç¨³å®šç°è±¡ï¼Œä¼šåœ¨**éç‚¹å¤„æŒ¯è¡**ï¼Œç”šè‡³åç¦»æœ€ä¼˜è§£ï¼›è®¾ç½®å¾—å¤ªå°ï¼Œä¼šé€ æˆæ”¶æ•›é€Ÿåº¦è¿‡æ…¢ã€‚
MBGDçš„å­¦ä¹ ç‡å¯¹æ‰€æœ‰çš„å‚æ•°æ›´æ–°éƒ½æ˜¯ä¸€æ ·çš„ï¼Œå¦‚æœæ•°æ®æ˜¯ç¨€ç–çš„ï¼Œæˆ‘ä»¬æ›´å¸Œæœ›å¯¹å‡ºç°é¢‘ç‡ä½çš„ç‰¹å¾è¿›è¡Œè¾ƒå¤§çš„æ›´æ–°ï¼Œå¹¶ä¸”å­¦ä¹ ç‡ä¼šéšç€æ›´æ–°æ¬¡æ•°é€æ¸å‡å°ã€‚æ˜¾ç„¶ï¼ŒMBGDå¹¶ä¸èƒ½æ»¡è¶³è¿™äº›éœ€æ±‚ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦èƒ½å¤Ÿè‡ªé€‚åº”å­¦ä¹ ç‡çš„ç®—æ³•ã€‚

#### SGD with Momentum

==Momentum==
We often think of Momentum as a means of dampening oscillations and speeding up the iterations, leading to faster convergence.
Momentum proposes the following tweak to gradient descent. We give gradient descent a short-term memory:

> ã€ä¸é‚£ä¹ˆæ­£ç¡®ä½†å¾ˆå¥½ç†è§£çš„ä¾‹å­ã€‘Momentum is a heavy ball rolling down the same hill. The added inertia acts both as a smoother and an accelerator, dampening oscillations and causing us to **barrel through narrow valleys, small humps and local minima. åŠ¨é‡æ˜¯ä¸€ä¸ªæ²‰é‡çš„çƒæ»šä¸‹åŒä¸€åº§å±±ã€‚å¢åŠ çš„æƒ¯æ€§æ—¢æ˜¯å¹³æ»‘çš„ï¼Œä¹Ÿæ˜¯åŠ é€Ÿå™¨ï¼ŒæŠ‘åˆ¶æŒ¯è¡ï¼Œå¯¼è‡´æˆ‘ä»¬ç©¿è¿‡ç‹­çª„çš„å±±è°·ã€å°é©¼å³°å’Œå±€éƒ¨æœ€å°å€¼ã€‚**

$$\colorbox{grey}{\text{(def)}} \text{ SGD with momentum }\\\quad \text{Start from some }\theta^0\in\R^s, v_0=g(\theta^0,\xi_0), \text{ for }k\ge0,\\[1em]\qquad v^{k+1}=\red\gamma v^k+\red{(1-\gamma)}g(\theta^k,\xi_k),\quad (\gamma \text{ usually 0.9}\\[1ex]\qquad \theta^{k+1}=\theta^k-v^{k+1}$$

SGDå’Œmomentumåœ¨æ›´æ–°åƒæ•¸æ™‚ï¼Œéƒ½æ˜¯ç”¨åŒä¸€å€‹å­¸ç¿’ç‡(*Î³*)

![](./pics/SGD_6.png)

ä¸»è¦æ˜¯ç”¨åœ¨è¨ˆç®—åƒæ•¸æ›´æ–°æ–¹å‘å‰æœƒè€ƒæ…®å‰ä¸€æ¬¡åƒæ•¸æ›´æ–°çš„æ–¹å‘ $(v_{t-1})$ï¼Œå¦‚æœç•¶ä¸‹æ¢¯åº¦æ–¹å‘å’Œæ­·å²åƒæ•¸æ›´æ–°çš„æ–¹å‘ä¸€è‡´ï¼ˆå› ä¸ºæ˜¯ç´¯ç§¯æ±‚å’Œçš„ï¼‰ï¼Œå‰‡æœƒå¢å¼·é€™å€‹æ–¹å‘çš„æ¢¯åº¦ï¼Œè‹¥ç•¶ä¸‹æ¢¯åº¦æ–¹å‘å’Œæ­·å²åƒæ•¸æ›´æ–°çš„æ–¹å‘ä¸ä¸€è‡´ï¼Œå‰‡æ¢¯åº¦æœƒè¡°é€€ã€‚ç„¶å¾Œæ¯ä¸€æ¬¡å°æ¢¯åº¦ä½œæ–¹å‘å¾®èª¿ã€‚é€™æ¨£å¯ä»¥å¢åŠ å­¸ç¿’ä¸Šçš„ç©©å®šæ€§(æ¢¯åº¦ä¸æ›´æ–°å¤ªå¿«)ï¼Œé€™æ¨£å¯ä»¥å­¸ç¿’çš„æ›´å¿«ï¼Œä¸¦ä¸”æœ‰æ“ºè„«å±€éƒ¨æœ€ä½³è§£çš„èƒ½åŠ›ã€‚

<div class="grid" markdown>
![](./pics/SGD_5.png)

<p>åŠ ä¸ŠåŠ¨é‡ä¹‹åçš„SGDä¼˜åŒ–ç®—æ³•ä¼šæ²¿ç€æ¢¯åº¦çš„æ–¹å‘è¶Šæ¥è¶Šå¿«åœ°è¿›è¡Œæ›´æ–°ï¼Œè€Œä¸ç›¸å…³çš„æ–¹å‘å°†é€æ¸å¾—åˆ°æŠ‘åˆ¶ï¼Œå› æ­¤èƒ½å‡å°‘ä¼˜åŒ–è¿‡ç¨‹ä¸­å‡ºç°çš„â€œä¹‹â€å­—å½¢è·¯çº¿</p>
</div>

**Limitation:**
momentum may be wrongã€‚å¦‚æœä¸€è¾†æ±½è½¦ä¸€ç›´åŠ é€Ÿï¼Œåœ¨é‡åˆ°éšœç¢ç‰©æ—¶æ˜¯å¦èƒ½åœä¸‹æ¥ï¼Ÿæ¢¯åº¦æ›´æ–°ä¹Ÿæ˜¯ä¸€æ ·çš„ï¼ŒMomentumè™½ç„¶èƒ½åŠ é€ŸSGDç®—æ³•ï¼Œä½†æ˜¯å¾ˆå®¹æ˜“â€œå†²ä¸Šæ–œå¡â€

momentum èƒ½é€ƒç¦»å±€éƒ¨æœ€å°ç‚¹ï¼Œå†²ä¸Šå°å±±è°·ï¼Œä½†æ˜¯ä¹Ÿæœ‰å¯èƒ½å› ä¸ºä¸æ–­ç´¯ç§¯çš„åŠ¨é‡å¤ªå¤§ï¼Œé”™è¿‡å…¨å±€æœ€å°ï¼Œä¸€å†²å†²å‡ºå»

![](./pics/SGD_1.png)

#### SGD with Nesterov momentum

å‡è®¾æœ‰ä¸€ä¸ªâ€œæ™ºèƒ½â€çš„é›ªçƒä»æ–œå¡ä¸Šå¾€ä¸‹æ»šï¼Œå®ƒåœ¨æ»šåŠ¨è¿‡ç¨‹ä¸­ä¸ä»…ä¼šè€ƒè™‘åŠ¨é‡ä¸ºè‡ªå·±åŠ é€Ÿï¼Œè¿˜ä¼šæ€è€ƒâ€œä¸‹ä¸€æ—¶åˆ»æ˜¯å¦ä¼šæ’å¢™â€ï¼Œä»è€Œå®ç°å‡é€Ÿ â€”â€” åœ¨è®¡ç®—å‚æ•°çš„æ¢¯åº¦æ—¶ï¼Œåº”åœ¨æŸå¤±å‡½æ•°ä¸­å‡å»åŠ¨é‡é¡¹

SGD with Nesterov  momentum

$$\text{Start from some }\theta^0\in\R^s, v_0=g(\theta^0,\xi_0), \text{ for } k\ge0,\\
\redÎ½^k=\theta^k-\beta_kv^k \\
v^{k+1}=\red\gamma v^k+\red{(1-\gamma)}g(\redÎ½^k,\xi_k), \\
\theta^{k+1}=\theta^k-v^{k+1}$$

$\redÎ½^k:=$ è¿‘ä¼¼å½“ä½œå‚æ•°ä¸‹ä¸€æ­¥ä¼šå˜æˆçš„å€¼
$\gamma$ usually 0.9

![](./pics/SGD_7.png)

difference between Origin and Nesterov:

- Origin momentum: åœ¨ $Î¸^k$ ä¸Šæ—¢è®¡ç®— gradient  $g(Î¸^k,\xi_k)$, åˆè®¡ç®— momentum $v^k$
    - Momentum SGDå…ˆè®¡ç®—å½“å‰çš„æ¢¯åº¦ï¼Œæ›´æ–°åçš„ç´¯ç§¯æ¢¯åº¦ä¼šå‡ºç°ä¸€ä¸ªå¤§çš„è·³è·ƒ
- Nesterov momentum: åœ¨ $Î¸^k$ ä¸Šå…ˆè®¡ç®— momentum $Ï…^k$, å†åœ¨æ–°çš„ç‚¹ä¸Šè®¡ç®— gradient   $g(Î½^k,\xi_k)$ .ï¼Œåœ¨NAGç®—æ³•ä¸­ä¸æ˜¯è®¡ç®—å½“å‰ä½ç½®çš„æ¢¯åº¦ï¼Œè€Œæ˜¯è®¡ç®—æœªæ¥ä½ç½®ä¸Šï¼ˆä¸‹ä¸€æ—¶åˆ»ï¼‰çš„æ¢¯åº¦
    - NAG SGDä¼šåœ¨å‰ä¸€æ­¥æ¢¯åº¦çš„åŸºç¡€ä¸Šåšä¿®æ­£ï¼Œä»è€Œå¾—åˆ°æœ€ä¸‹é¢çš„æ¢¯åº¦ï¼Œé¿å…é€Ÿåº¦è¶Šæ¥è¶Šå¿«ã€‚with a shorter step to prevent overshoot
    - NAGç®—æ³•åœ¨å¾ªç¯ç¥ç»ç½‘ç»œã€LSTMç­‰ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚
- Momentum SGDå’ŒNAG SGDè¿™ä¸¤ç§åŠ é€Ÿç®—æ³•åªé’ˆå¯¹æ¢¯åº¦è¿›è¡Œä¼˜åŒ–ï¼Œå¹¶æ²¡æœ‰**é’ˆå¯¹å‚æ•°é‡è¦æ€§è¿›è¡Œä¸åŒç¨‹åº¦çš„æ›´æ–°ï¼Œä¹Ÿæ²¡æœ‰å­¦ä¹ ç‡çš„è‡ªåŠ¨è°ƒæ•´**

#### Adagrad: Adaptive Learning Rates

==å­¸ç¿’ç‡è¡°æ¸›(Learning rate decay)==ã€‚å¤§çš„å­¸ç¿’ç‡å¯ä»¥è¼ƒå¿«èµ°åˆ°æœ€ä½³å€¼æˆ–æ˜¯è·³å‡ºå±€éƒ¨æ¥µå€¼ï¼Œä½†è¶Šå¾Œé¢åˆ°è¦æ‰¾åˆ°æ¥µå€¼å°±éœ€è¦å°çš„å­¸ç¿’ç‡ã€‚

Rescale the learning rate of **each coordinate** by the **historical progress**.

==Adagrad==
$$ \text{Start from some }\theta^0\in\R^s, n_g^0=0,\text{ for }k\ge0\\[1em]\qquad n_g^{k+1}= n_g+<g(\theta^k,\xi_k),g(\theta^k,\xi_k)>\\[1ex]\qquad \theta^{k+1}=\theta^k-\cfrac{\alpha_k}{\red{n_g+10^{-8}}}g(\theta^k,\xi_k) \qquad\text{ (rescale)}$$

The learning rate (step size) goes to zero quickly.

#### RMSProp

Discount the accumulated norm of the gradients.

==RMSProp==ã€‚
$$\text{Start from some }\theta^0\in\R^s, n_g^0=0,\text{ for }k\ge0\\
n_g^{k+1}=\red\gamma n_g+\red{(1-\gamma)}<g(\theta^k,\xi_k),g(\theta^k,\xi_k)>\\
\theta^{k+1}=\theta^k-\cfrac{\alpha_kg(\theta^k,\xi_k)}{\red{n_g+10^{-8}}} \qquad\text{ (rescale)}$$

ä»ç„¶ä½¿ç”¨æŒ‡æ•°åŠ æƒå¹³å‡ï¼Œæ—¨åœ¨æ¶ˆé™¤æ¢¯åº¦ä¸‹é™ä¸­çš„æ‘†åŠ¨ï¼Œä¸ Momentum çš„æ•ˆæœä¸€æ ·ã€‚å¦‚æœæŸä¸ªç»´åº¦çš„å¯¼æ•°æ¯”è¾ƒå¤§ï¼Œåˆ™æŒ‡æ•°åŠ æƒå‡å€¼å°±å¤§ï¼›å¦‚æœæŸä¸ªç»´åº¦çš„å¯¼æ•°æ¯”è¾ƒå°ï¼Œåˆ™å…¶æŒ‡æ•°åŠ æƒå‡å€¼å°±å°ã€‚è¿™æ ·å¯ä»¥ä¿è¯å„ä¸ªç»´åº¦çš„å¯¼æ•°éƒ½åœ¨ä¸€ä¸ªé‡çº§ï¼Œä»è€Œå‡å°‘æ‘†åŠ¨

#### ADAM, Adaptive Moment Estimation

Consider **momentum** and **adaptive learning rate** (second-order momentum) together.

æ˜¯å¦å¤–ä¸€ç§è®¡ç®—æ¯ä¸ªå‚æ•°çš„è‡ªé€‚åº”å­¦ä¹ ç‡çš„ç®—æ³•ï¼Œæ¯”Adadeltaç®—æ³•å’ŒRMSpropç®—æ³•æ›´æ¿€è¿›ï¼Œä¸ä»…è€ƒè™‘äº†æŒ‡æ•°è¡°å‡å‡å€¼ï¼Œé™ä½å­¦ä¹ ç‡è¿‡åº¦è¡°å‡çš„é—®é¢˜ï¼Œè¿˜åŠ å…¥äº† Momentum çš„åŠ¨é‡æ€æƒ³ã€‚
$m_t=\beta_1m_{t-1}+(1-\beta_1)g_t$ tæ—¶åˆ»çš„åŠ¨é‡
$v_t=\beta_2v_{t-1}+(1-\beta_2)g_t^2$tæ—¶åˆ»çš„æŒ‡æ•°è¡°å‡å‡å€¼ã€‚
å¦‚æœ $m_tï¼Œv_t$ åˆå§‹åŒ–ä¸º0ï¼Œåˆ™ä¼šå‘0åç½®ï¼Œå› æ­¤Adamåšäº†åå·®çº æ­£
$\hat{m}_t=\cfrac{m_t}{1-\beta_1^t}$
$\hat{v}_t=\cfrac{v_t}{1-\beta_2^t}$
æœ€ç»ˆæ¢¯åº¦æ›´æ–°
$$\theta_{t+1}=\theta_t-\cfrac{\iota}{\sqrt{\hat{v}_t}+\epsilon}\hat{m}_t$$

å»ºè®®:$\alpha=0.001,\beta_1=0.9,\beta_2=0.999,\epsilon=10^{-8}$

å¤§é‡äº‹ä»¶è¡¨æ˜ Adam ã€‹ Adadelta & RMSprop. <u>Transformer in CV å¾ˆçˆ±ç”¨ADAM</u>

![](./pics/SGD_8.png)

å½“äºŒé˜¶åŠ¨é‡å‚æ•° $\beta_2$ å¾ˆå¤§ä¸”ä¸€é˜¶åŠ¨é‡å‚æ•° $\beta_1 < \sqrt{\beta_2}<1$ æ—¶ï¼ŒAdam å¯ä»¥æ”¶æ•›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªè¯æ˜å…·æœ‰ä»»æ„å¤§ $\beta_1$ çš„ Adam å¯ä»¥åœ¨æ²¡æœ‰ä»»ä½•å½¢å¼çš„æœ‰ç•Œæ¢¯åº¦å‡è®¾çš„æƒ…å†µä¸‹æ”¶æ•›ã€‚è¿™ä¸ªç»“æœè¡¨æ˜ï¼Œæ²¡æœ‰ä»»ä½•ä¿®æ”¹çš„Adamåœ¨ç†è®ºä¸Šä»ç„¶å¯ä»¥æ”¶æ•›ã€‚å½“$\beta_2$ è¾ƒå°æ—¶ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æŒ‡å‡ºAdam å¯ä»¥å‘æ•£åˆ°æ— ç©·ã€‚æˆ‘ä»¬çš„å‘æ•£ç»“æœè€ƒè™‘äº†ä¸æ”¶æ•›ç»“æœç›¸åŒçš„è®¾å®šï¼ˆæå‰å›ºå®šä¼˜åŒ–é—®é¢˜ï¼‰ï¼Œè¿™è¡¨æ˜å½“å¢åŠ  $\beta_2$ æ—¶å­˜åœ¨ä»å‘æ•£åˆ°æ”¶æ•›çš„ç›¸å˜ã€‚è¿™äº›ç»“æœå¯èƒ½ä¼šä¸ºæ›´å¥½åœ°è°ƒæ•´ Adam çš„è¶…å‚æ•°æä¾›æŒ‡å¯¼ã€‚

### convergence  of ADAM

But the convergence analysis contains someÂ **mistakes**Â in the original paper. ADAM can beÂ **non-convergent**!

![](./pics/SGD_9.png)

---

#### Newtonâ€™s method ç‰›é¡¿è¿­ä»£æ³•

<u>æ–¹æ³•æœ¬èº«ï¼šæ±‚è§£éçº¿æ€§æ–¹ç¨‹ $g(x)=0$ çš„==è¿‘ä¼¼æ ¹== $x^*$</u>
åœ¨ Descent Direction ä¸Šçš„åº”ç”¨ï¼šæ±‚è§£  $\red{\text{æ–°}\cdot g(x) = \nabla f(x^*)=0}$

ä½¿ç”¨**å‡½æ•°çš„æ³°å‹’çº§æ•°**çš„å‰é¢å‡ é¡¹æ¥å¯»æ‰¾æ–¹ç¨‹çš„æ ¹ã€‚

##### æ–¹æ³•æœ¬èº«

- **èƒŒæ™¯**
å¤šæ•°æ–¹ç¨‹ä¸å­˜åœ¨æ±‚æ ¹å…¬å¼ï¼Œå› æ­¤æ±‚ç²¾ç¡®æ ¹éå¸¸å›°éš¾ï¼Œç”šè‡³ä¸å¯èƒ½ï¼Œä»è€Œå¯»æ‰¾æ–¹ç¨‹çš„<u>è¿‘ä¼¼æ ¹</u>å°±æ˜¾å¾—ç‰¹åˆ«é‡è¦ã€‚æ–¹ç¨‹ç”¨äºŒæ¬¡å‡½æ•°çš„å½¢å¼è¡¨ç¤ºå‡ºæ¥ï¼Œæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡ä¸Šé¢çš„åŠæ³•å¤§è¸æ­¥çš„å‰è¿›äº†ï¼ç”±æ­¤æˆ‘ä»¬ç¥­å‡ºå°†ä»»æ„Né˜¶å¯å¯¼å‡½æ•°åŒ–ä¸ºNæ¬¡å¤šé¡¹å¼çš„ç¥å™¨ï¼š**Né˜¶æ³°å‹’å±•å¼€**
- **æ€è·¯**
è®¾ $x^*$ æ˜¯ $g(x)=0$ çš„è¿‘ä¼¼æ ¹ï¼Œå°† $g(x)$ åœ¨ $x^k$ é™„è¿‘**ç”¨ä¸€é˜¶æ³°å‹’å¤šé¡¹å¼è¿‘ä¼¼**
$$g(x)=g(x^{k})+ \nabla g(x^k)^T\cdot (x-x^k)+o(\vert x-x_0\vert)$$
èˆå»é«˜é˜¶é¡¹ï¼š$g(x)=g(x^{k})+ \nabla g(x^k)^T\cdot (x-x^k)$
å°†è¿‘ä¼¼æ ¹ä»£å…¥ï¼š
<a id="eq1"> $$\begin{align*} g(x^*)=g(x^{k})+ \nabla g(x^k)^T\cdot (x^*-x^k)=0\tag{1}\\x^*=x^k-\cfrac{g(x^k)}{g'(x^k)}\tag{2}\end{align*}$$ </a>
ä¸èƒ½ä¸€æ­¥å¾—åˆ°ï¼Œæ‰€ä»¥éœ€è¦è¿­ä»£  âˆ´è¿­ä»£å…¬å¼ï¼š$\red{x^{k+1}=x^k-\cfrac{f(x^k)}{f'(x^k)}}$

    !!! p "ã€è¯´äººè¯ã€‘"
        1. å…ˆéšæœºé€‰ä¸€ä¸ªç‚¹ï¼Œ
        2. ç„¶åæ±‚å‡º$f(x)$åœ¨è¯¥ç‚¹çš„åˆ‡çº¿ã€‚
        3. è¯¥åˆ‡çº¿ä¸xè½´ç›¸äº¤çš„ç‚¹ä¸ºä¸‹ä¸€æ¬¡è¿­ä»£çš„å€¼ã€‚
        ç›´è‡³é€¼è¿‘$f(x)=0$çš„ç‚¹ã€‚
- **åœæ­¢æ ‡å‡†**
  - $\vert x_{k+1}-x_k\vert <\epsilon_1$
  - $\vert f(x) \vert<\epsilon_2$: $f(x)$å¾ˆå°ï¼Œå°äºç²¾åº¦ï¼Œä¸èƒ½ä¿è¯xçš„ç²¾åº¦
    å±€é™æ€§ï¼šå¯¹äºæŸäº›ç‰¹æ®Šå‡½æ•°ï¼Œå°åŒºé—´æ€¥é€Ÿå˜åŒ–
- **å‡ ä½•æœ¬è´¨**
    **åœ¨åŸå‡½æ•°çš„æŸä¸€ç‚¹å¤„ç”¨ä¸€ä¸ªäºŒæ¬¡å‡½æ•°è¿‘ä¼¼åŸå‡½æ•°ï¼Œç„¶åç”¨è¿™ä¸ªäºŒæ¬¡å‡½æ•°çš„æå°å€¼ç‚¹ä½œä¸ºåŸå‡½æ•°çš„ä¸‹ä¸€ä¸ªè¿­ä»£ç‚¹ã€‚** åŸºäºå½“å‰è¿­ä»£ç‚¹çš„æ¢¯åº¦ä¿¡æ¯è¿›è¡Œæœç´¢æ–¹å‘çš„é€‰æ‹©çš„ï¼Œç‰›é¡¿æ³•æ˜¯é€šè¿‡HessiançŸ©é˜µåœ¨æ¢¯åº¦ä¸Šè¿›è¡Œçº¿æ€§å˜æ¢å¾—åˆ°æœç´¢æ–¹å‘

##### æ”¶æ•›

fast local convergence å¿«é€Ÿçš„å±€éƒ¨æ”¶æ•›  + Quadratic convergence äºŒé˜¶æ”¶æ•›æ€§
$\iff$ ç‰›é¡¿æ³•**é è¿‘æœ€ä¼˜ç‚¹**æ—¶æ˜¯**äºŒæ¬¡**æ”¶æ•›çš„

$\begin{cases}g\in C^2(\red{\R})\\g(x^*)=0\\gâ€™(x^*) â‰ 0.
\end{cases}\implies\exist \varepsilon>0, \vert x^0-x^* \vert<\varepsilon.$ And with Newtonâ€™s iterate: $x^{k+1}=x^k-\cfrac{g(x^{k})} {g'(x^{k+1})}$is well defined.
$$\implies\exist M>0, \vert x^{k+1}-x^k\vertâ‰¤M\Vert x^k-x^*\Vert_2$$

!!! p "ã€è¯´äººè¯ã€‘å°±æ˜¯è¯´å¦‚æœ $x^0$ é€‰çš„å¥½ï¼Œé‚£ä¹ˆç‰›é¡¿æ³•å¾ˆå¥½ç”¨ï¼Œæ”¶æ•›é€Ÿåº¦å¾ˆå¿«ï¼Œæ¯æ¬¡è¿­ä»£ä¹‹åï¼Œå¦‚æœ $x^0$ çš„åˆå§‹åŒ–è¶³å¤Ÿæ¥è¿‘ä¸€ä¸ªå¥½çš„è§£å†³æ–¹æ¡ˆï¼Œé‚£ä¹ˆç‰›é¡¿æ–¹æ³•çš„å®šä¹‰å¾ˆå¥½ï¼Œæ”¶æ•›é€Ÿåº¦ä¹Ÿéå¸¸å¿«ï¼šæ¯æ¬¡è¿­ä»£çš„æ­£ç¡®æ•°å­—æ•°é‡å¤§çº¦ç¿»ä¸€ç•ªã€‚ï¼ˆç”šè‡³æ­¥é•¿éƒ½ä¸éœ€è¦ç¡®å®šï¼‰ã€‚æ‰€ä»¥ç‰›é¡¿æ³•å¯¹å‡½æ•°åœ¨è¿­ä»£ç‚¹å¤„çš„ä¿¡æ¯åˆ©ç”¨æ›´åŠ å……åˆ†ï¼Œç›´è§‚æ¥çœ‹ï¼Œç›¸æ¯”äºæ¢¯åº¦ä¸‹é™æ³•ï¼Œå‡½æ•°è¶³å¤Ÿæ­£åˆ™çš„æƒ…å†µä¸‹ç‰›é¡¿æ³•è¿­ä»£å¾—æ›´åŠ å‡†ç¡®ï¼Œæ”¶æ•›é€Ÿç‡ä¹Ÿä¼šæ›´å¿«ã€‚"
    å½“xåœ¨ä»¥ $x^*$ ä¸ºåŸç‚¹ï¼Œ$\varepsilon$ ä¸ºåŒºé—´çš„é‚»åŸŸå†…è¿›è¡Œè¿­ä»£ï¼Œæ‰€æœ‰è¿­ä»£è¿‡æ¥çš„ $x^k$ éƒ½ä»¥äºŒæ¬¡æ”¶æ•›çš„é€Ÿåº¦æ”¶æ•›äº $x^*$ã€å±€éƒ¨ã®äºŒæ¬¡ã®æ”¶æ•›ã€‘ï¼Œå…¶ä¸­$M=\frac{\tau}{2\delta}$

##### å¤±æ•ˆ

1. $x^0$é€‰çš„ä¸å¥½ï¼Œç¦»$x^*$å¾ˆè¿œï¼Œ$\exist x^k\in(x^0,x^*),g'(x^k)=0$ï¼Œå‡ ä½•ä¸Šæ²¡æœ‰å‡é™çš„ç©ºé—´ï¼Œè¿ç®—ä¸Šåˆ†æ¯ä¸º0å¤±æ•ˆï¼ˆæ›´è¿œäº†ï¼‰
2. due to cycling

##### åœ¨ Descent Direction ä¸Šçš„åº”ç”¨

!!! p "ç›®æ ‡ï¼š$\red{\text{æ–°}\cdot g(x) = \nabla f(x^*)=0}$"
    $\exist x^{k+1}, \nabla f(x^{k+1})=0 \implies \nabla f(x)=\nabla f(x^k)+\nabla^2f(x^{k})(x-x^k)$

**è¿­ä»£æ–¹ç¨‹ï¼š**

<a href="#eq1">$$\begin{align*} g(x^*)=g(x^{k})+ \nabla g(x^k)^T\cdot (x^*-x^k)=0\tag{1}\\x^*=x^k-\cfrac{g(x^k)}{g'(x^k)}\tag{2}\end{align*}\\\Downarrow$$ </a>
$$\begin{align*} \nabla f(x^*)=\nabla f(x^{k})+ \nabla^2 f(x^k)^T\cdot (x^*-x^k)=0\tag{3}\\x^*=x^k-\cfrac{\nabla f(x^k)}{\nabla^2f(x^k)}\tag{4}\end{align*}$$
==ç»å…¸ç‰›é¡¿æ³•==:
$$d^k=-\cfrac{\nabla f(x^k)}{\nabla^2f(x^k)}, \alpha\equiv1$$

è¦æ±‚ï¼š

1. $\forall k,\nabla^2f(x^k)$å¯é€†$\iff  \in\text{sigular éå¥‡å¼‚çŸ©é˜µ}$ äºŒé˜¶å¯å¾®å‡½æ•°
2. è®¡ç®—$\cfrac{\nabla f(x^k)}{\nabla^2f(x^k)}$ ç®€å•

!!! p "$\text{ For k=0,1,2â€¦,}\text{ update }x^{k+1}=x^k-\cfrac{\nabla f(x^k)}{\nabla^2f(x^k)}$.<br> ä¸è¦å»æ±‚è§£$(\nabla^2f)^{-1}$ç„¶åå†ä¹˜ï¼Œè€Œæ˜¯æŠŠ$d=\cfrac{\nabla f(x^k)}{\nabla^2f(x^k)}$ï¼Œè§£$\nabla^2f(x^k)d=\nabla f(x^k)$"
â­ ç‰›é¡¿æ³•ä¹Ÿåªæ˜¯æ‰¾åˆ°ä¸€é˜¶å¯¼ä¸º0ï¼Œä¹Ÿå°±æ˜¯è¯´æœç€æå€¼çš„$d^k$ï¼Œä¸ä¸€å®šæ˜¯å‡½æ•°å€¼çš„ä¸‹é™æ–¹å‘ï¼Œè¿˜è¦verify **é€šè¿‡$\nabla^2 f(x^*)$å»éªŒè¯$X=x^*$æ˜¯å¦local minimizer**
â­ å¯ä»¥ç”¨æ›´å°‘çš„è¿­ä»£æ¬¡æ•°å¤§è¸æ­¥åœ°å‰è¿›ï¼Œå¹¶ä¸”å‰è¿›çš„æ–¹å‘ä¹Ÿæ›´è¶‹å‘äºå‡½æ•°çš„å…¨å±€æœ€ä¼˜è§£ï¼ˆå³æœ€å€¼è€Œéæå€¼ç‚¹ï¼‰ï¼ŒåŒæ—¶ä¹Ÿèƒ½å¤Ÿæ‘†è„±ä¸Šé¢æ¢¯åº¦ä¸‹é™æ³•ä¸­ç¡®å®šÎ±çš„ç—›è‹¦

Here we discuss just the local rate-of-convergence properties of Newtonâ€™s method. We know that for all x in the vicinity of a solution point xâˆ— such thatâˆ‡2 f(xâˆ—) is positive deï¬nite, the Hessian âˆ‡2 f(x) will also be positive deï¬nite. Newtonâ€™s method will be well deï¬ned in this region and will converge quadratically, provided that the step lengths Î±k are eventually always 1.

##### ç¼ºç‚¹

1. æ¯ä¸€æ­¥è¿­ä»£éœ€è¦æ±‚è§£ä¸€ä¸ªÂ $n$Â ç»´çº¿æ€§æ–¹ç¨‹ç»„ï¼Œè¿™å¯¼è‡´åœ¨é«˜ç»´é—®é¢˜ä¸­è®¡ç®—é‡å¾ˆå¤§.æµ·ç‘ŸçŸ©é˜µÂ $\nabla^2Â fÂ (x^kÂ )$Â æ—¢ä¸å®¹æ˜“è®¡ç®—åˆä¸å®¹æ˜“å‚¨å­˜.
2. Â $\nabla^2 f (x^k )$Â ä¸æ­£å®šæ—¶ï¼Œç”±ç‰›é¡¿æ–¹ç¨‹ç»™å‡ºçš„è§£Â *dk*Â çš„æ€§è´¨é€šå¸¸æ¯”è¾ƒå·®.ä¾‹å¦‚å¯ä»¥éªŒè¯å½“æµ·ç‘ŸçŸ©é˜µæ­£å®šæ—¶ï¼Œ*dk*Â æ˜¯ä¸€ä¸ªä¸‹é™æ–¹å‘ï¼Œè€Œåœ¨å…¶ä»–æƒ…å†µä¸‹Â *dk*Â ä¸ä¸€å®šä¸ºä¸‹é™æ–¹å‘.
3. We need to assume ğ‘“ to be convex to guarantee the direction is a descent direction.
4. Newtonâ€™s method **only converges locally, even for strongly convex functions.**

- **advantage**:

1. Newtonâ€™s method enjoys **a local quadratic convergence rate** under some assumptions:
fast local convergence å¿«é€Ÿçš„å±€éƒ¨æ”¶æ•›  + Quadratic convergence äºŒé˜¶æ”¶æ•›æ€§
$\iff$ ç‰›é¡¿æ³•**é è¿‘æœ€ä¼˜ç‚¹**æ—¶æ˜¯**äºŒæ¬¡**æ”¶æ•›çš„.å½“xåœ¨ä»¥$x^*$ä¸ºåŸç‚¹ï¼Œ$\varepsilon$ä¸ºåŒºé—´çš„é‚»åŸŸå†…è¿›è¡Œè¿­ä»£ï¼Œæ‰€æœ‰è¿­ä»£è¿‡æ¥çš„$x^k$éƒ½ä»¥äºŒæ¬¡æ”¶æ•›çš„é€Ÿåº¦æ”¶æ•›äº$x^*$ã€å±€éƒ¨ã®äºŒæ¬¡ã®æ”¶æ•›ã€‘
$\exist M>0 ,\vert \theta^{k+1}-x^*\vertâ‰¤M\Vert x^k-x^*\Vert^2$

![](./pics/SGD_10.png)

If the current iteration achieves an accuracy of the order ğŸğŸ^-ğŸ‘, we can expect an accuracy of the order ğŸğŸ^-ğŸ” for the next iteration!!!

#### Conjugate gradient method å…±è½­æ¢¯åº¦æ³•

![](./pics/Untitled.png)

![](./pics/Untitled2.png)

Flops per iteration isÂ $O(n^2)$; It converges inÂ at mostÂ *n*Â steps;
It keeps track ofÂ *O*(1)Â vectors of dimensionÂ *n*Â per iteration.

!!! p "idea: Modify the steepest descent direction to fit theÂ (ellipse) geometry."

==Projection onto v==. $u\in\R^n,v\in\R^n\setminus\{0\}$. The projection of u onto v $\text{proj}_v(u):=w:=\cfrac{u^Tv}{\Vert v\Vert_2^2}v$
ä»å‡ ä½•çš„è§’åº¦ï¼š$\Vert w\Vert_2=\Vert u\Vert_2\cos\theta=\Vert u\Vert_2\cfrac{u^Tv}{\Vert u\Vert_2\Vert v\Vert_2}=\cfrac{u^Tv}{\Vert v\Vert_2}\implies$ Unit vector along w is $\cfrac{v}{\Vert v\Vert_2}$

==Gram-Schmidt process==. Given a set of linearly independent vectors $\{v^0,...,v^k\} \subset\R^n, w^0\xlongequal{SET} v^0.$
$\forall j = 1,...,k, w^k=v^k-\sum\limits_{j=0}^{k-1}\cfrac{{(v^k)}^Tw^j}{\Vert w^j\Vert_2^2}w^j, \begin{cases}\forall i,w^i\ne 0\\\forall i\ne j,{(w^i)}^Tw^j=0\end{cases}\\\implies\text{Span}\{v^0,...,v^k\}=\text{Span}\{w^0,...,w^i\}$

==Generalized Gram-Schmidt process==. Given $A\in\R^n,A\succ0$ and a set of linearly independent vectors $\{v^0,...,v^k\} \subset\R^n, w^0\xlongequal{SET} v^0.$
$\forall j = 1,...,k, w^k=v^k-\sum\limits_{j=0}^{k-1}\cfrac{{(v^k)}^TAw^j}{{(w^j)}^TA w^j}w^j, \begin{cases}\forall i,w^i\ne 0\\\forall i\ne j,{(w^i)}^TAw^j=0\end{cases}\\\implies\text{Span}\{v^0,...,v^k\}=\text{Span}\{w^0,...,w^i\}$

##### Conjugate gradient method: Conceptual version

Start at $x^0\in\R^n$Â and $d^0 =-\nabla f(x^0)=b-Ax^0$.

For eachÂ *k*Â =Â 0,1,2,...,

- IfÂ $d^kÂ =Â 0$, terminate.
- PickÂ $\alpha_k\text{ s.t. }\alpha_k\in\min\{f(x^k +\alpha d^k): Î±\ge0\}$.
- $x^{k+1}\xlongequal{SET}x^k +Î±_kd^k, d^{k+1} = -\nabla f(x^{k+1})-\blue{\sum\limits_{j=0}^k\cfrac{[-\nabla f(x^{k+1})]^T Ad^j}{ (d^j)^TAd^j}}$

##### Conjugate gradient method: Formal version

Start at $x^0\in\R^n$Â and $d^0 =-\nabla f(x^0)=b-Ax^0$.

For eachÂ *k*Â =Â 0,1,2,...,

- IfÂ $d^kÂ =Â 0$, terminate.
- PickÂ $\alpha_k\text{ s.t. }\alpha_k\in\min\{f(x^k +\alpha d^k): \alpha\ge0\}$.
- $x^{k+1}\xlongequal{SET}x^k +\alpha_kd^k, d^{k+1} = -\nabla f(x^{k+1})-\blue{\cfrac{\Vert\nabla f(x^{k+1})\Vert_2^2}{\Vert\nabla f(x^k)\Vert_2^2}d^k}$

##### Conjugate gradient method: Actual version

**è¿­ä»£è¿‡ç¨‹ï¼š**

Start at $x^0\in\R^n$Â and $\blue{r^0}=d^0 =-\nabla f(x^0)=b-Ax^0$.

For eachÂ *k*Â =Â 0,1,2,...,

- IfÂ $\Vert d^k\Vert \blue{\text{ is below a tolerance}}$, terminate.
- $Î±_k=\cfrac{(r^k)^Tr^k}{(d^k)^TAd^k}\:,\:x^{k+1}=x^k+\alpha_kd^k\:,\:r^{k+1}=r^k-\alpha_kAd^k\qquad\text{(excat line search)}$.
- $\beta_{k} =\cfrac{(r^{k+1})^Tr^{k+1}}{(r^k)^Tr^k}\:,\:d^{k+1}=r^{k+1}+\beta_kd^k\qquad\text{Update} d^{k+1}$

**ä¼˜ç‚¹:**

- One matrix-vector multiplication per iteration ifÂ $Ad^k$Â is saved.
- Keeping track ofÂ four vectors,Â $x^k, r^k, d^k, Ad^k$Â saved.

Newton-CGå•Šï¼Œå…¶å®æŒºç®€å•çš„ã€‚ä¼ ç»Ÿçš„ç‰›é¡¿æ³•æ˜¯æ¯ä¸€æ¬¡è¿­ä»£éƒ½è¦æ±‚HessiançŸ©é˜µçš„é€†ï¼Œè¿™ä¸ªå¤æ‚åº¦å°±å¾ˆé«˜ï¼Œä¸ºäº†é¿å…æ±‚çŸ©é˜µçš„é€†ï¼ŒNewton-CGå°±ç”¨CGå…±è½­æ¢¯åº¦æ³•æ¥æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„ï¼Œä»è€Œé¿å…äº†æ±‚çŸ©é˜µé€†ã€‚

#### Truncated Newtonâ€™s method (Hessian-Free Optimization)ä¿®æ­£ç‰›é¡¿æ³•

==Projection onto $S_+^n$==. $A\in S^n,A = UDU^T$ be its eigenvalue decomposition.
$A_+ := UD_+U^T$
$D_+$ is the diagonal matrix with $(d_+)_{ii}=\max\{d_{ii},0\}, \forall i.$
$A_+$  is the **unique** solution of $ \min\Vert Y-A\Vert_F \text{ s.t.}Y\succeq0$

##### å®šä¹‰

1. Pick $\sigma\in(0,1), \beta\in(0,1), \overline{\alpha}_k\equiv1$, a small  $\eta>0$ and a huge  $M > 0$. Initialize at $x^0 \in\R^n$
2. For $k = 0,1,2,...,$  
    1. let $UDU^T$ be an eigenvalue decomposition of $\nabla^2f(x^k).$
    2. Let $\varLambda$ be diagonal with $\lambda_{ii} = \max\{\min\{M,d_{ii}\},\eta\} \qquad\text{(Project } d_{ii} \text{ on }[\eta,M])$  
    3. Set $D^k \coloneqq U\varLambda U^T$ and $d^k\coloneqq -D^k\nabla f(x^k).$.
    4. Update $x^{k+1}=x^k+\alpha^kd^k\\\qquad \alpha^k\text{ is obtained via the Armijo line search by backtracking }$

Let $f\in C^2(\R^n)$ with $\inf f > 1$ and let $\{x^k\}$ be generated by **the truncated Newtonâ€™s method.** Then any accumulation point of $\{x^k\}$ is a stationary point of f.

##### Computational concerns

### æ‹Ÿç‰›é¡¿ç±»ç®—æ³•

!!! p ""
    å¯¹äºå¤§è§„æ¨¡é—®é¢˜ï¼Œå‡½æ•°çš„æµ·ç‘ŸçŸ©é˜µè®¡ç®—ä»£ä»·ç‰¹åˆ«å¤§æˆ–è€…éš¾ä»¥å¾—åˆ°ï¼Œå³ä¾¿å¾—åˆ°æµ·ç‘ŸçŸ©é˜µæˆ‘ä»¬è¿˜éœ€è¦æ±‚è§£ä¸€ä¸ªå¤§è§„æ¨¡çº¿æ€§æ–¹ç¨‹ç»„.
    å®ƒèƒ½å¤Ÿåœ¨æ¯ä¸€æ­¥ä»¥è¾ƒå°çš„è®¡ç®—ä»£ä»·ç”Ÿæˆ**è¿‘ä¼¼çŸ©é˜µ**ï¼Œå¹¶ä¸”ä½¿ç”¨**è¿‘ä¼¼çŸ©é˜µä»£æ›¿æµ·ç‘ŸçŸ©é˜µè€Œäº§ç”Ÿçš„è¿­ä»£åºåˆ—**ä»å…·æœ‰è¶…çº¿æ€§æ”¶æ•›çš„æ€§è´¨.
    ä¸è®¡ç®—æµ·ç‘ŸçŸ©é˜µÂ $âˆ‡^2Â fÂ (x)$ï¼Œè€Œæ˜¯æ„é€ å…¶**è¿‘ä¼¼çŸ©é˜µ**Â $B^k$Â æˆ–**å…¶é€†çš„è¿‘ä¼¼çŸ©é˜µ**Â $H^k$

#### Basic idea: Secant equations

- **æ€è·¯**

    ç›®çš„ï¼š$g(x)=0ï¼Œg\in C^1(\R)$

    ==Taylor Formula==. $g(x^{k+1})=g(x^{k})+\nabla g(x^k)(x^{k+1}-x^k)=0$
    $\implies x^{k+1}=x^k-\cfrac{g(x^k)}{\nabla g(x^k)}$

    ä½†å½“ä¸€é˜¶å¯¼$\nabla g(x)$å¤ªéš¾æ±‚ï¼Œæˆ‘ä»¬å°±æƒ³åˆ°äº†å‰²çº¿æ–¹ç¨‹ Secant equationã€‚Use finite difference to approximate $\nabla g(x)$

    ==Secant equations==. $\nabla g(x^k)\approx\cfrac{g(x^k)-g(x^{k-1})}{x^k-x^{k-1}}$
    $\implies x^{k+1}=x^k-g(x^k)\cfrac{x^k-x^{k-1}}{g(x^k)-g(x^{k-1})}$

  - Notes:
    1. è¿™é‡ŒåŒæ—¶æœ‰k+1ï¼Œkï¼Œk-1. initialized at $x^0,x^{-1},g(x^0)â‰ g(x^{-1})$
    2. The local convergence rate of theÂ secant methodÂ is **typically slower** than Newtonâ€™s method. However, **theÂ computational cost** per iterationÂ can be smaller whenÂ $g'$Â is hard to compute compared withÂ *g*

> > Find the square root of 2 using the secant method, starting atÂ $x^{-1}Â =Â 1.4,Â x^0Â =Â 1.5$, up toÂ 4 decimal places.

#### åœ¨ descent direction ä¸Šçš„åº”ç”¨

ç›®çš„ï¼š$\nabla f(x)=0$

same ideas: $\nabla g(x^k)(x^k-x^{k-1})\approx g(x^k)-g(x^{k-1})\Downarrow$
$$\nabla^2f(x^{k+1})(x^{k+1}-x^k)\approx\nabla f(x^{k+1})-\nabla f(x^k)$$

Notation: $s^k:=x^{k+1}-x^k,\space y^k=\nabla f(x^{k+1})-\nabla f(x^k)\implies \nabla^2f(x^{k+1})s^k=y^k$

**æˆåŠŸçš„å…³é”®ï¼š** æˆ‘ä»¬èƒ½å¤Ÿè¿ç»­ä¸æ–­åœ°æ„é€ çŸ©é˜µ$\begin{cases}\text{Method 1: }B^{k+1}\approx \nabla^2f(x^{k+1})\\\text{Method 2 }H^{k+1}\approx \cfrac{1}{\nabla^2f(x^{k+1})}\end{cases}$ å»æ‹Ÿåˆæµ·å¡çŸ©é˜µï¼Œä½¿å¾—$\begin{cases}  B^{k+1}s^k=y^k \\H^{k+1}y^k=s^k\end{cases}$, å› ä¸ºæˆ‘ä»¬æ˜¯è¦è¿­ä»£çš„ï¼Œæ‰€ä»¥å°±æ˜¯èƒ½è¿ç»­ç”Ÿæˆè¿­ä»£

**é—®é¢˜ï¼šæ€ä¹ˆè¿­ä»£ï¼Œè¿­ä»£æœ‰ä»€ä¹ˆè¦æ±‚?**

1. InitializeÂ $B^0$Â (orÂ $*H^0*$) at aÂ **positive definite** matrix.
   ==proposition of BFGS==. $\begin{cases}H_k\succ0\\{y^k}^Ts^k>0\\H_{k+1}\text{ is given by BFGS update}\end{cases}\implies H_{k+1}\succ 0.$
   Same for B
2. SinceÂ $B^0$Â andÂ $H^0$Â wereÂ **symmetric**Â to start with, by induction, allÂ $B^k$andÂ $H^k$Â areÂ **symmetric**.
3. **Popular update formula**

- Note:
    1. DFP and BFGS are rank-2 updates, while SR1 is rank-1 update.
    2. In practice, **BFGS** usually performs better.
- Verify the secant equation for BFGS.

### Quasi-Newton method

Given $f\in C^1(\R^n).$

Initialize at $x^0\in\R^n$ and $B_0,H_0\succ 0$, is **symmetric and positive definite**

#### Quasi-Newton based on $B_k$

For $k = 0,1,2,...$  

1. Find $d^k =-B_k^{-1}\nabla f(x^k).$
2. Update $x^{k+1} = x^k +d^k\times 1$,
3. Set $y^k =\nabla f(x^{k+1})-\nabla f(x^k)$  and $s^k = x^{k+1}-x^k$.
4. Compute $B_{k+1}$ by **Popular update formula**

### BFGS

#### Quasi-Newton based on $H_k$

For $k = 0,1,2,...$  

1. Find $d^k =-H_k\nabla f(x^k).$
2. Update $x^{k+1} = x^k +d^k\times 1$,
3. Set $y^k =\nabla f(x^{k+1})-\nabla f(x^k)$  and $s^k = x^{k+1}-x^k$.
4. Compute $H_{k+1}$ by **Popular update formula**

## StepSize $\alpha_k$

!!! p "$\text{ given } x^{k} \text{ and }d^k$ <br> å˜æˆäº†å•å˜é‡ä¼˜åŒ–ï¼š$\alpha^k =\min\limits_{\alpha>0}\varphi(\alpha) =f(x^k+\alpha d^k)$.å®ƒæ˜¯ç›®æ ‡å‡½æ•°Â *f*Â (*x*)Â åœ¨å°„çº¿Â $\{x^k + Î±d^k:Î± > 0\}$ ä¸Šçš„é™åˆ¶"

$$\alpha^k =\min\limits_{\alpha>0} f(x^k+\alpha d^k)$$

### åˆ†ç±»: exact & inexact

==exact line search strategy==. ç­‰äºæ±‚æå°å€¼ç‚¹é—®é¢˜ã€‚

 $\nabla\varphi(\alpha)=[\nabla f(x^k+\alpha d^k)]^Td^k\xlongequal{SET}0\implies[\nabla f_{k+1}]^Td^k=0$

é€šå¸¸éœ€è¦å¾ˆå¤§è®¡ç®—é‡ï¼Œåœ¨å®é™…åº”ç”¨ä¸­è¾ƒå°‘ä½¿ç”¨

==inexact line search strategy==ã€‚å¯»æ‰¾**æ­¥é•¿$\alpha$çš„ä¸€ä¸ªåŒºé—´**ï¼Œé€šè¿‡é€æ­¥è¿­ä»£çš„æ–¹æ³•å»å¯»æ‰¾**ä»…ä»…æ˜¯æ»¡è¶³æ¡ä»¶çš„ç‚¹**ã€‚å½“æœç´¢ç»“æŸæ—¶ï¼Œéœ€è¦æ»¡è¶³è¯¥æ­¥é•¿èƒ½å¤Ÿå¯¹ç›®æ ‡å‡½æ•°å¸¦æ¥**å……åˆ†çš„ä¸‹é™**ã€‚More practical strategies perform an inexact line search to identify a step length that achieves adequate reductions in f **at a minimal cost**.

### inexact line search strategy

#### Termination conditions çº¿æœç´¢å‡†åˆ™

å› ä¸ºè¿­ä»£ï¼š
$$f(x^k + Î±^kd^k) = \min\limits_{Î±â‰¥0}f(x^k + Î±d^k)$$

!!! p "ä¸ºæé«˜éç²¾ç¡®ç®—æ³•çš„æœç´¢æ•ˆç‡ï¼Œéœ€è¦ç¡®å®šä¸€äº›termination conditions å»åˆ¤æ–­æ˜¯å¦è¿­ä»£åˆ° $\alpha^*$ï¼Œç¡®ä¿**è¿­ä»£çš„æ”¶æ•›æ€§ã€‚**"

##### Sufficient Decrease condition (Armijo condition) å……åˆ†ä¸‹é™æ¡ä»¶

- **å®šä¹‰ï¼š**

    $$\begin{cases}x\in \R^n, d\in \R^n\\\alpha>0, c_1\in(0,1)\\f(x^k+\alpha d^k)â‰¤f(x^k)+c_1\alpha[\nabla f(x^k)]^Td^k
    \end{cases}\implies \alpha\text{ satisifies Armijo rule}$$

    å…¶ä¸­ï¼š$d^k$: descent direction;$c_1=10^{-4}$is chosen to be quite small

    !!! p "**alone is not sufficient** to ensure that the algorithm makes reasonable progress along the given search direction:"
        *Î±*Â =Â 0Â æ˜¾ç„¶æ»¡è¶³æ¡ä»¶ï¼Œè€Œè¿™æ„å‘³ç€è¿­ä»£åºåˆ—ä¸­çš„ç‚¹å›ºå®šä¸å˜ï¼Œç ”ç©¶è¿™æ ·çš„æ­¥é•¿æ˜¯æ²¡æœ‰æ„ä¹‰çš„
        æ˜¯ the Wolfe conditions $1^{st}$  condition
        æ˜¯ the Goldstein conditions $2^{nd}$ inequality
        æ˜¯ Backtracking line search çš„åœæ­¢æ ‡å‡†stopping criterionï¼Œ alone is ok

- **å­˜åœ¨æ€§è¯æ˜:**

    $\alpha å­˜åœ¨ \iff \text{Armijo rule is not valid}$ï¼Œé€‰å–ç¬¦åˆArmijo rule ç¡®å®ä¼šä½¿å¾—å‡½æ•°å€¼ä¸‹é™

    Let $f\in C^1(\R^n), x\in\R^n, d\in\R^n \text{ be a descent direction at }x$. Let $\sigma\in(0,1)$. Then there $\exist \alpha_1 > 0$ so that $\forall \alpha\in[0,\alpha_1],f(x + \alpha d)â‰¤f(x)+\alpha\sigma[\nabla f(x)]^Td.$

- **How to execute Armijo rule in practice?**

    **Fix  $\sigma\in(0,1)$ and $\beta\in(0,1)$. Given  $x\in\R^n, d\in\R^n, \overline{\alpha}>0$. Find the smallest nonnegative integer $j = j_0$ so that**

    $$f(x +\overline{\alpha}\beta^jd)â‰¤ f(x)+\overline{\alpha}\beta^j\sigma[\nabla f(x)]^Td$$
    normally: $\sigma= 10^{-4}, \beta=\cfrac{1}{2}, \overline{\alpha}\beta^{j_0}$ is the step size

    **Note:**

    1. $d$ is a descent direction +  $j$ is sufï¬ciently large  $\rightarrow \beta^j$ is sufï¬ciently small â†’ Armijo rule satisfiedã€‚
    2. å¯è¯ $\overline{\alpha}\beta^j \text{ is decreasing}\therefore \text{it is called backtracking}$
    3. $\overline{\alpha}$  é€‰æ‹©å¯¹æ”¶æ•›æ•ˆç‡æ¥è¯´å¾ˆå…³é”®

- **Convergence under Armijo rule**
    $\begin{cases}f\in C^1(\R^n),\inf f>-\infin\\
    \{\overline\alpha_k\}\subset\R ,0<\inf\limits_k\overline{\alpha}_kâ‰¤\sup\limits_k\overline{\alpha}_k<\infin\\
    \sigma\in(0,1),\beta\in(0,1)\\
    \{D_k\}\succ 0,
    d^k=-D_k\nabla f(x^k)\\
    x^k \text{ is non-stationary}\\
    x^{k+1} = x^k + \alpha_kd^k
    \end{cases}$ $\alpha_k$ is generated via the Armijo line search by backtracking with $ x = x^k, d = d^k, \overline\alpha =\overline\alpha_k$ Then any accumulation point of  $\{x^k\}$ is a stationary point of  $f$.normally $\sigma=10^{-4},\beta=\frac{1}{2} $

    <u>for BFGS:</u>

    $\exist M>0,\Vert H_k\Vert_2\Vert H_k^{-1}\Vert_2â‰¤M,\forall k\\\qquad\implies \lim\limits_{k\rightarrow\infin}\Vert_2=0$

    $\cos\theta_k=\cfrac{{d^k}^TH_k^{-1}d^k}{\Vert d^k\Vert_2\Vert H_k^{-1}d^k\Vert_2}\ge\cfrac{{d^k}^T{H_k}^{-1}d^k}{\Vert H_k^{-1}\Vert \Vert d^k\Vert_2^2}\ge\cfrac{\lambda_{\min}(H_k^{-1})}{\Vert H_k^{-1}\Vert_2}=\cfrac{1}{\lambda_{\max}(H_k)\Vert H_k^{-1}\Vert_2}=\cfrac{1}{\Vert H_k^{-1}\Vert_2\Vert H_k\Vert_2}\ge\cfrac{1}{M}$

- **Sufficient Decrease and Backtracking approach**
use **just the sufficient decrease** condition to terminate the line search procedure

##### Wolfe conditions

[sufficient decrease condition]

- **å®šä¹‰**
$1^{st}$ ï¼šsufficient decrease conditionï¼š$f(x^k+\alpha^k d^k)â‰¤f(x^k)+c_1\alpha^k[\nabla f(x^k)]^Td^k$
$2^{nd}$ ï¼šcurvature conditionï¼š$\nabla f(x^k+\alpha^kd^k)^Td^kâ‰¥c_2\nabla f_k^Td^k$
with $0<c_1<c_2<1ï¼Œc_1\text{ usually }10^{-3},c_2\text{ usually }0.9$
*Ï†*(*Î±*)Â åœ¨ç‚¹Â *Î±*Â å¤„åˆ‡çº¿çš„æ–œç‡ä¸èƒ½å°äºÂ *Ï†*â€²(0)Â çš„Â $*c_2*$Â å€

- ==curvature condition==
$\nabla f(x^k+\alpha^kd^k)^Td^kâ‰¥c_2\nabla f_k^Td^k\\ \qquad\qquad\parallel\qquad\qquad\qquad\parallel\\\qquad\space
\nabla\varphi(\alpha^k)\qquad\qquad c_2\nabla\varphi(0)$
å…¶ä¸­ï¼š$c_2=0.9$ in Newton or quasi-Newton method, $c_2=0.1$ in a nonlinear conjugate gradient method

- **Wolfe conditions å­˜åœ¨æ€§è¯æ˜ï¼šæ˜¯æœ‰åŒºé—´èƒ½æ»¡è¶³ Wolfe conditions**

- **The strong Wolfe conditions**
<u>modify the curvature condition</u> to force $Î±^k$ to lie in at least a broad neighborhood of a local minimizer or stationary point of Ï†.  The only difference with the Wolfe conditions is that we no longer allow the derivative $Ï†â€² (Î±^k )$  to be too positive.
$1^{st}$ ï¼šsufficient decrease conditionï¼š$f(x^k+\alpha^k d^k)â‰¤f(x^k)+c_1\alpha^k[\nabla f(x^k)]^Td^k$
$2^{nd}$ ï¼š<u>**modified** curvature condition</u>ï¼š$\red{\vert}\nabla f(x^k+\alpha^kd^k)^Td^k\red{\vertâ‰¤} c_2\red{\vert}\nabla f_k^Td^k\red{\vert}$
with $0<c_1<c_2<1$

- **Convergence under Wolfe conditions**
==Zoutendijkâ€™s theorem==. $f\in C^1(\R^n),\inf f >-\infin,x^0\in\R^n,\\\{x^k\} \text{is a sequence of non-stationary points generated as }x^{k+1}+\alpha_kd^k, \\\begin{cases}f\in C^1(\R^n),\inf f >-\infin\text{ (ä¸‹æœ‰ç•Œï¼Œè¿ç»­å¯å¾®)}\\\exist\ell>0,\Vert \nabla f(x)-\nabla f(y)\Vert_2\le\ell\Vert x-y\Vert_2ï¼Œ\forall x,y\in\R^n\text{ (æ¢¯åº¦æ»¡è¶³L-åˆ©æ™®å¸ŒèŒ¨è¿ç»­)}\\d^k\text{ is a descent direction}\\\alpha_k \text{ satisfies the Wolfe conditions }\text{(Wolfe )}\end{cases}\\\qquad\implies\sum\limits_{k=0}^\infin\cos^2\theta_k\Vert\nabla f(x^k)\Vert_2^2<\infin,\\\qquad\implies\exist\delta, \text{so that }  \cos\theta_k=\cfrac{-[\nabla f(x^k)]^Td^k}{\Vert\nabla f(x^k)\Vert_2\Vert d^k\Vert_2}\ge\delta,\forall k\text{(independent of k)}$
$\Vert\nabla f(x^*)\Vert=0 \rightarrow \Vert\nabla f(x^n)\Vert<\varepsilon$

##### Goldstein conditions æ¡ä»¶

- å®šä¹‰
$f(x^k)+(1-c)\alpha^k[\nabla f(x^k)]^Td^kâ‰¤f(x^k+\alpha^k d^k)â‰¤f(x^k)+c\alpha^k[\nabla f(x^k)]^Td^k$
with $0<c<\cfrac{1}{2}$
$2^{nd}$ â‰¤ ï¼šsufficient decrease condition
å¿…é¡»åœ¨ä¸¤æ¡ç›´çº¿ä¹‹é—´

are often used in **Newton-type methods** but are not well suited for quasi-Newton methods that maintain a positive definite Hessian approximation

Goldstein å‡†åˆ™èƒ½å¤Ÿä½¿å¾—å‡½æ•°å€¼å……åˆ†ä¸‹é™ï¼Œä½†æ˜¯å®ƒå¯èƒ½**é¿å¼€äº†æœ€ä¼˜çš„å‡½æ•°å€¼**.

[sufficient decrease condition]:  https://www.notion.so/line-search-53b5a2ab0bea46bd88c2ac43ac9fc52d?pvs=21
[Hyperparameter Tuning Black Magic]: https://community.alteryx.com/t5/Data-Science/Hyperparameter-Tuning-Black-Magic/ba-p/449289

[æ©Ÿå™¨/æ·±åº¦å­¸ç¿’-åŸºç¤æ•¸å­¸(ä¸‰):æ¢¯åº¦æœ€ä½³è§£ç›¸é—œç®—æ³•(gradient descent optimization algorithms)]: https://chih-sheng-huang821.medium.com/æ©Ÿå™¨å­¸ç¿’-åŸºç¤æ•¸å­¸-ä¸‰-æ¢¯åº¦æœ€ä½³è§£ç›¸é—œç®—æ³•-gradient-descent-optimization-algorithms-b61ed1478bd7
[å¦‚ä½•ç†è§£SAG,SVRG,SAGAä¸‰ç§ä¼˜åŒ–ç®—æ³•]: https://zhuanlan.zhihu.com/p/51184012
