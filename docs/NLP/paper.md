# Temp

## [Automatic Spelling Correction with Transformer for CTC-based End-to-End Speech Recognition]

==Connectionist Temporal Classification CTC==。CTC based end-to-end SR system usually need to incorporate an external language model by using WFST-based decoding in order to achieve promising results. This is more essential to Mandarin speech recognition since it owns a special phenomenon, namely homophone, which causes a lot of substitution errors.

!!! quote Abstract
    Connectionist Temporal Classification CTC based end-to-end SR system usually need to incorporate an external language model by using WFST-based decoding in order to achieve promising results. This is more essential to Mandarin speech recognition since it owns a special phenomenon, namely homophone, which causes a lot of substitution errors. The linguistic information introduced by language model will help to distinguish these substitution errors. **In this work, we propose a ==transformer-based== ==spelling correction== model to automatically correct errors especially the substitution errors made by ==CTC-based Mandarin speech recognition system==. Specifically, we investigate using the recognition results generated by CTC-based systems as input and the ground-truth transcriptions as output to train a transformer with encoder-decoder architecture, which is much similar to machine translation.**  Results in a 20,000 hours Mandarin speech recognition task show that the proposed spelling correction model can achieve a CER of 3.41%, which results in 22.9% and 53.2% relative improvement compared to the baseline CTC-based systems decoded with and without language model respectively.

[Automatic Spelling Correction with Transformer for CTC-based End-to-End Speech Recognition]:https://arxiv.org/pdf/1904.10045.pdf

## [Bi-encoder Transformer Network for Mandarin-English Code-switching Speech Recognition using Mixture of Experts]

[Bi-encoder Transformer Network for Mandarin-English Code-switching Speech Recognition using Mixture of Experts]:https://x-lance.sjtu.edu.cn/en/papers/2020/yzl23-lu-is2020.pdf

!!! quote Abstract
    Code-switching speech recognition is a challenging task which has been studied in many previous work, and one main challenge for this task is the lack of code-switching data. **In this paper, we study ==end-to-end== models for Mandarin-English ==code-switching== ==automatic speech recognition==.** **External monolingual data** are utilized to alleviate the data sparsity problem. More importantly, we propose **a bi-encoder transformer network based ==Mixture of Experts (MoE)== architecture** to better leverage these data. We **decouple Mandarin and English modeling with two separate encoders** to better capture language-specific information, and **a gating network is employed to explicitly handle the language identification task**. For the gating network, different models and training modes are explored to learn the better **MoE interpolation coefficients**. Experimental results show that compared with the baseline transformer model, the proposed new MoE architecture can obtain up to 10.4% relative error reduction on the code-switching test set.

In this work we focus on leveraging rich resources monolingual data to achieve a better code-switching ASR performance, and a new MoE structure is proposed to better handle Mandarin and English modeling.

The new approach mainly includes three parts: bi-encoder bilingual model pretraining, mixture of experts architecture construction and a gating network for MoE interpolation coefficients.
We train the transformer model with joint CTC-attention [11, 12] framework to exploit the advantages from both CTC and S2S models.
For the modeling units, we combine Chinese characters and English BPE subwords [31] as final units. We also apply SpecAugment [14] for all data through out our experiments.

<figure markdown="span">![](./pics/Bi-encoder_Transformer_+_MOE_1.png)
<p class="notes"><b>Figure 1：The proposed bi-encoder transformer network based MoE architecture:</b><br> (1) pretrained bi-encoder bilingual model; (2) mixture of experts architecture for code-switching ASR; (3) gating network for MoE interpolation coefficients;</p></figure>

$$h^{cn}=\text{Mandarin Encoder}(x)\\h^{en}=\text{English Encoder}(x)$$

$$h_t^{mix}=\alpha_t^{cn}h_t^{cn}+\alpha_t^{en}h_t^{en}$$

$\begin{cases}\alpha_t^{cn},\alpha_t^{en}\in[0,1]\\\alpha_t^{cn}+\alpha_t^{en}=1\end{cases}$

1. Pretrain a bi-encoder bilingual model with only monolingual Mandarin and English data.
两个编码器结构可以不一样。
Since language identity for monolingual data can be obtained in advance, we are able to decouple Mandarin and English language with two separate encoder. As shown in the left part of Figure 1, when given acoustic features inputs, prior LID information is used to decide which encoder to use. Denote Xcn and Xen as the col- lection of all Mandarin inputs and English inputs separately, we formulate this procedure as:

1. **<u>Gating network</u> for MoE interpolation coefficients.**
    处理成 LID classification 二分类问题。
    output = probability of each language 直接是权重 $\alpha_t^{cn},\alpha_t^{en}$
    1. **External LID method.** Train a self attention network (SAN) based model
    input = raw features $x$
    2. **Built-in LID method**.
    input = the outputs of the two separate encoders $h^{cn}\& h^{en} $ 知道所谓的专家输出 = 高纬表达
    For this build-in LID method, the ASR and LID modules in this MoE architecture can be trained jointly, and the objective loss is changed to:

The LID and ASR are trained independently. To improve the performance of LID classifier, we adopt transfer learning strategy and a pretrained CTC model is used for initialization.

### Experimental setup

- Dataset:【CN:EN:CS=25:23:10】
**ASRU 2019 Mandarin-English code-switching Challenge** dataset (500 hours Mandarin data and 200 hours code-switching data) & a subset of 460 hours English data from **Librispeech corpus**
**DEV:** Additional 20 hours code-switching data
**TEST:** 3test sets: CN & EN & CS
- Data Preprocess
For acoustic feature, 80 dimensional log-mel filterbanks are extracted with a step size of 10ms and window size of 25ms, and utterance-level CMVN is applied on the fbank features.
    - modeling unit.
    we combine Chinese characters and English BPE subword units [31].

    ```mermaid
    graph LR
    A[Mandarin character]
    B{训练集<br>>25次}
    C[UNK]
    D[English data]
    E[modeling unit 4006]
    F[BPE units]
    G[SOS&EOS]
    A-->B--Y(3003)-->E
    B--N-->C--(1)-->E
    D-->F--(1000)-->E
    G--(2)-->E
    ```

- EVA
character error rate (CER) 纯 Mandarin
word error rate (WER) 纯 English
mix error rate (MER) 各自语言用各自统计

## [New Datasets and Controllable Iterative Data Augmentation Method for Code-switching ASR Error Correction]

[New Datasets and Controllable Iterative Data Augmentation Method for Code-switching ASR Error Correction]: https://aclanthology.org/2023.findings-emnlp.543.pdf

!!! quote Abstract
    With the wide use of automatic speech recognition(ASR) systems, researchers pay more attention to the ASR error correction task to improve the quality of recognition results. In particular, ASR in bilingual or multilingual settings, namely code-switching ASR, has greater challenges and research value. In this paper, we first present code-switching ASR correction datasets obtained from solid ASR systems and automatic annotators. **The datasets contain Chinese-English code-switching dialogues of bilingual speakers in Singapore, Malaysia, and Hong Kong.** **Based on this task, we propose a controllable iterative (CI) data augmentation method for improving the performance of mainstream ASR error correction systems.** With a small amount of training data, our proposed method has the ability to iteratively produce abundant pseudo parallel data from the monolingual corpus for Chinese-English code-switching ASR correction. **Results of experiments show that our method achieves the best performance compared with the rulebased, back-translation-based data augmentation methods and large language model ChatGPT**

With a similar scale of augmented data, our proposed controllable iterative method achieves the best performance in both MaxMatch (M2) scorer (Dahlmeier and Ng, 2012) and MER metrics on SEAME-C and ASCEND-C datasets.

Besides, we find that this task is challenging, and LLM method is far from achieving satisfactory results at now

Our contributions are summarized as follows:

1. We propose two datasets **SEAME-C & ASCEND-C** for the challenging Chinese-English code-switching ASR error correction task.
2. To address the problem of lacking sufficient training data, we propose the controllable iterative data augmentation method that can iteratively generate abundant code-switching ASR error correction instances from the monolingual corpus with small-scale labeled training data.
3. Extensive experiments show the superiority of our proposed controllable iterative method. Moreover, combining the pseudo data produced by the rule-based and controllable iterative methods can further improve the performance of error correction models.

data augmentation method in text error correction.

<figure markdown="span">![alt text](./pics/CIDA_1.png)<p>对话的内容形式，基本一个句子10个 tokens 以上</p><figure>

- 跟随原数据集的划分
- remove bad cases（太短，单语）

[SEAME: a Mandarin-English  Code-switching Speech Corpus in South-East Asia]: https://www.researchgate.net/publication/221481268_Mandarin-English_code-switching_speech_corpus_in_South-East_Asia_SEAME

## [SEAME: a Mandarin-English Code-switching Speech Corpus in South-East Asia]

Mandarin-English  code-switching

!!! quote Abstract
    In **Singapore and  Malaysia**, people often  speak a  mixture of Mandarin and English within a single sentence. We call such sentences ==intra-sentential code-switch sentences==. **In this paper, we  report  on  the  development of  a Mandarin-English  code-switching spontaneous speech corpus: SEAME. The corpus is developed as part of a multilingual speech recognition project and  will  be  used  to  examine  how  Mandarin-English  code-switch  speech  occurs  in the  spoken  language in  South-East Asia.**  Additionally,  it  can  provide  insights  into  the development  of  ==large  vocabulary  continuous  speech recognition (LVCSR)== for code-switching speech. The corpus collected consists of intra-sentential code-switching utterances that  are  recorded  under  **both  interview  and  conversational settings**.  This  paper  describes  the  corpus  design  and  the analysis of collected corpus.

language boundary detection (LBD), language identification (LID)

As  the corpus  is  developed  for  spontaneous  code-switching  speech research,  our  recordings  consist  of  interviews  and conversations  without  scripts.

UTF-8 code

there are two speakers in each interview setup, an interviewer who asks  questions  and  an  interviewee  who  answers.
Only  the interviewee‟s speech is recorded using a close talk microphone.

> 既要多样性回答，又要引导回答者 双语回复：hobbies, movies, books, university life, working life,  special topics  and  others
> 你 叁加 什麽 CCA (Which co-curricular activity do you participate in?)
> 谈谈 你 喜欢 的fruits (talk about your favorite fruits)

mainly informal and non-speech  sounds  often  occur,  

Target speech: this category dictates that the utterance is intra-sentential  code-switching  speech,  and  it  contains  both Mandarin and English segments within one utterance.

Abbreviation and proper noun: eg. „CCA‟, is the abbreviation for co-curricular activity and „Choa Chu Kang‟, is the name of a road Singapore.

From  the SEAME corpus, we find that on average, the number of turns of language switch for each code-switching utterance is 2.8 for Malaysian  and  3.1  for  Singaporean  speakers.

> This  example  has  3 language turn
> 你们 那些 guys，每次 唱 的 时候，sing so much louder.
> It is straightforward to count the  number of English word in each turn,  but it is not as easy to  do so for Chinese text. For consistency, we first segment a Chinese phrase/segment into lexical words with a forward maximal-length matching algorithm as shown in Example
> In general, people tend to switch to English just for one word. This accounts for 50% and 70% of the total sentences from Singapore and Malaysia respectively. This observation of speaking style in code-switching utterance coincides with what are reported in  Hong Kong and Taiwan