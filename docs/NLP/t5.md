# T5s

## T5

<kbd>pre-trained model</kbd>, <kbd>encoder-decoder</kbd>

把所有的 NLP tasks 映射成一个带有任务前缀的 text2text 问题。在有监督和自监督任务进行预训练。

!!! p "有在下游任务上进行预训练，所以可以开箱即用"

### 训练语料

!!! p 改成 seq2seq 版本

T5 的预训练包含无监督和有监督两部分。无监督部分使用的是 Google 构建的近 800G 的语料（论文称之为 C4），而训练目标则跟 BERT 类似，只不过改成了 Seq2Seq 版本，我们可以将它看成一个高级版的完形填空问题：

> [In]：明月几时有，[M0] 问青天，不知 [M1]，今夕是何年。我欲[M2]归去，唯恐琼楼玉宇，高处 [M3]；起舞 [M4] 清影，何似在人间。
> [OUT]：[M0] 把酒 [M1] 天上宫阙 [M2] 乘风 [M3] 不胜寒 [M4] 弄
> [In]：识别该句子的情感倾向：这趟北京之旅我感觉很不错。
> [OUT]：正面
> [In]：下面是一则什么新闻？八个月了，终于又能在赛场上看到女排姑娘们了。
> [OUT]：体育
> [In]：阅读理解：特朗普与拜登共同竞选下一任美国总统。根据上述信息回答问题：特朗普是哪国人？
> [OUT]：美国

[Paper: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer]
[那个屠榜的T5模型，现在可以在中文上玩玩了]

## mT5, Multilingual T5

<kbd>pre-trained model</kbd>, <kbd>encoder-decoder</kbd>, <kbd>multilingual</kbd>

!!! p "还没填完[那个屠榜的T5模型，现在可以在中文上玩玩了]"
    它主要的改动来自论文 GLU Variants Improve Transformer [3]，主要是借用了Language Modeling with Gated Convolutional Networks [4] 的 GLU（Gated Linear Unit）来增强 FFN 部分的效果。具体来说，原来 T5 的 FFN 为（T5 没有 Bias）：

!!! danger "mT5 没有像 T5 在下游任务进行预训练，所以一定要 fine-tune"
    mt5 cannot generate coherent sentences out-of-the-box because it's only be pretrained on the span-mask filling task and not on any down-stream tasks.
    [Generating from mT5 #8704](:https://github.com/huggingface/transformers/issues/8704)

mT5 涵盖了 101 种语言，总词表有 25 万，而且它采用的 T5.1.1结构的 Softmax 还不共享参数，这就导致了 Embedding 层占用了相当多的参数量。
比如 mT5 small 的参数量为 3 亿，其中 Embedding 相关的就占了 2.5 亿，关键是里边的大部分参数我们都用不上，纯粹是不必要的浪费。因此，对于主要关心中文任务的我们来说，有必要精简一下这个 Embedding 层了。

!!! p "还没细看 [那个屠榜的T5模型，现在可以在中文上玩玩了]"
    对模型的精简很简单，只需要在两个 Embedding 矩阵中删除不需要的行就行了，关键在于如何决定要保留的 token，以及如何得到一个精简后的 sentencepiece 模型。
    决定要保留的 token，简单来想就是把中文的 token 保留下来，但是也不只是中文，英文的也要保留一部分，看上去似乎只是一个正则表达式的问题，实际上没那么简单，用英文字母的也不一定是英语，用中文字的也不一定是中文，这是个让人纠结的事情。
    于是笔者想了另外一个办法：用这个 25 万 token 的 tokenizer 对笔者收集的几十 G 中文语料分词，统计分词结果，然后按照词频选择前面的部分（最后保留了 3 万多个 token）。这样虽然费时一些，但是比较靠谱，能确保把我们比较需要的 token 保留下来。决定词表后，就要修改得到一个新的 sentencepiece 模型，这也有点麻烦，但最终经过搜索后还是把这个事情解决了，处理方法都分享在 Github 上。

[Paper: mT5: A massively multilingual pre-trained text-to-text transformer]

[Paper: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer]: https://arxiv.org/pdf/1910.10683
[那个屠榜的T5模型，现在可以在中文上玩玩了]:https://www.jiqizhixin.com/articles/2020-11-17-3
[Paper: mT5: A massively multilingual pre-trained text-to-text transformer]:https://arxiv.org/abs/2010.11934
