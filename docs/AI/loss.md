# Loss

**What do we care aboutï¼Ÿ**

1. What types of errors do we care aboutï¼Ÿ ä»€ä¹ˆæ ·çš„é”™è¯¯æ˜¯æˆ‘æ›´å…³æ³¨çš„
    ä¸€äº›å¾ˆå¾®å°çš„ errors éœ€è¦åœ¨æ„å—ï¼Ÿ
    ä¸€äº›é”™å¾—å¾ˆç¦»è°±çš„errors æˆ‘ä»¬è¦æ€ä¹ˆè°ƒæ•´å®ƒçš„æƒé‡å‘¢ï¼Ÿ(outlier?)
    which class data do we care aboutï¼Ÿ
2. How much do we care about the errorsï¼Ÿ æˆ‘ä»¬éå¸¸åœ¨æ„è¿™ä¸ªé”™è¯¯å—ï¼Ÿ

**Two types of errorÂ ğœ‚Â are considered:**

1. Standard Cauchy distribution(with location parameter 0 and scale parameter 1)
$$Î·ï½\text{Cauchy}(0; 1);$$
2. Normal mixture distribution, denoted by â€œMixtureâ€,
$$Î·ï½ 0.8 Ã— N(0,1) + 0.2 Ã— N(0,10^4)$$

## Data Loss

==Data loss==: Model predictions should match training data. Loss over the dataset is an average of loss over examples:

åš data mining ç›®çš„æ˜¯è¦æ‰¾åˆ°æ•°æ®çš„åˆ†å¸ƒï¼Œè¿™ä¸ªæ—¶å€™çš„æ•°æ®å°±æ˜¯ä¸€ä¸ªå¤§çš„æ¦‚å¿µï¼ŒçœŸæ­£çš„æ•°æ®ï¼Œå±äºpopulation levelï¼Œæ­¤æ—¶çš„æ‰€æœ‰æ•°æ®çš„ç»Ÿè®¡å€¼åŒ…æ‹¬å‡å€¼ï¼ŒåŒ…æ‹¬æŸå¤±å‡½æ•°ï¼Œéƒ½æ˜¯ **expected æœŸæœ›çš„ï¼Œpopulation æ€»ä½“çš„**ã€‚ä½†æ˜¯æˆ‘ä»¬ä¸å¯èƒ½çŸ¥é“çœŸæ­£çš„æ•°æ®åˆ†å¸ƒæ˜¯ä»€ä¹ˆï¼Œå› ä¸ºåœ¨ä¸çŸ¥é“å…·ä½“åˆ†å¸ƒæƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªèƒ½é€šè¿‡æé«˜æé«˜æ•°é‡çš„æ ·æœ¬å»é è¿‘å®ƒï¼Œä½†æ˜¯ observations æ˜¯æ— ç©·æ— å°½çš„ï¼Œæˆ‘ä»¬å‡ ä¹ä¸å¯èƒ½é æœ‰é™çš„ observations å»æ‰¾åˆ°æ•°æ®åˆ†å¸ƒã€‚æ‰€ä»¥æˆ‘ä»¬åªèƒ½è¯´æˆ‘ä»¬æ”¶é›†æ ·æœ¬ï¼Œé€šè¿‡æœ‰é™çš„æ ·æœ¬å»è§‚æµ‹å» observeï¼Œæ­¤æ—¶æˆ‘ä»¬æ‰€å¾—åˆ°ç»Ÿè®¡å€¼åªæ˜¯å»ºç«‹åœ¨æˆ‘ä»¬æ‰€é‡‡çš„æ ·æœ¬ï¼Œæ˜¯ **empirical ç»éªŒ**çš„ã€‚

### ç»“æ„é£é™© & ç»éªŒé£é™©

$$\min_f\Omega(f)+C\sum_{i=1}^nl(f(x_i),y_i)$$

$\Omega(f)$, ==ç»“æ„é£é™©ï¼Œstructural risk==ï¼Œæè¿° $f$ çš„æŸäº›æ€§è´¨ã€‚==æ­£åˆ™åŒ–é¡¹==
**ç»“æ„ç»éªŒæœ€å°åŒ–**å¯ä»¥çœ‹ä½œæ˜¯é‡‡ç”¨äº†**æœ€å¤§åéªŒæ¦‚ç‡ä¼°è®¡**çš„æ€æƒ³æ¥æ¨æµ‹æ¨¡å‹å‚æ•°ï¼Œä¸ä»…ä¾èµ–æ•°æ®ï¼Œæµ·ä¾é æ¨¡å‹å‚æ•°çš„å…ˆéªŒå‡è®¾ã€‚

$\sum\limits_{i=1}^nl(f(x_i),y_i)$, ==ç»éªŒé£é™©ï¼Œempirical risk==ï¼Œæè¿° $f$ ä¸æ•°æ®çš„å¥‘åˆç¨‹åº¦. = ==è®­ç»ƒè¯¯å·® training error==
**ç»éªŒé£é™©æœ€å°åŒ–**å¯ä»¥çœ‹ä½œæ˜¯é‡‡ç”¨äº†**æå¤§ä¼¼ç„¶**çš„å‚æ•°è¯„ä¼°æ–¹æ³•ï¼Œæ›´ä¾§é‡ä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å‹çš„æ½œåœ¨å‚æ•°ï¼Œåªçœ‹é‡æ•°æ®æœ¬èº«ã€‚

$C$ï¼ŒæŠ˜ä¸­

> |å®è´¨è®¡ç®—|æ¦‚ç‡è§’åº¦|ï½|
> |--|--|--|
> |æœ€å°äºŒä¹˜æ³• |æœ€å¤§ä¼¼ç„¶ä¼°è®¡ MAE| $(y-\hat{y})^2$|
> |å²­å›å½’ |æœ€å¤§åéªŒä¼°è®¡ MAP|å¼•å…¥æ­£åˆ™é¡¹ $P(w), w^2$|
>
> æœ€å¤§ä¼¼ç„¶æ³• $\xrightarrow{\text{å¥ å®š}}\\\xrightarrow{\text{æ¦‚ç‡è§£é‡Š}}$ æœ€å°äºŒä¹˜æ³•
æœ€å¤§åéªŒä¼°è®¡ $\xrightarrow{\text{å¥ å®š}}\\\xrightarrow{\text{æ¦‚ç‡è§£é‡Š}}$ å²­å›å½’
> (æœ€å°äºŒä¹˜æ³• & æœ€å¤§ä¼¼ç„¶æ³•),(å²­å›å½’ & æœ€å¤§åéªŒä¼°è®¡) **å½¢å¼å®è´¨ç›¸ç­‰ï¼Œå®è´¨æ€æƒ³ä¸€è‡´ï¼Œä½†å‡ºå‘è§’åº¦ä¸åŒ**
> æœ€å¤§åéªŒä¼°è®¡æ˜¯å¢åŠ äº† $p(w)$å…ˆéªŒï¼Œä½œä¸ºæ­£åˆ™é¡¹å­˜åœ¨ã€‚
> |ï½|æœ€å¤§ä¼¼ç„¶|æœ€å¤§åéªŒä¼°è®¡|
> |--|--|--|
> |ç›®æ ‡å‡½æ•°|$P(x\vert w)$|$P(w\vert x)=\cfrac{P(x\vert w)P(w)}{P(x)}$|
> |å‡è®¾|$\epsilonï½N(0,\sigma^2)$é«˜æ–¯å™ªå£°|$\epsilonï½N(0,\sigma_\epsilon^2)$é«˜æ–¯å™ªå£°<br>$wï½N(0,\sigma_w^2)$é«˜æ–¯å…ˆéªŒ|

ä»ç»“æ„é£é™©æœ€å°åŒ–çš„è§’åº¦ä¸Šçœ‹ï¼Œ$\Omega(f)$ å¸Œæœ›è·å¾—å…·æœ‰ä½•ç§æ€§è´¨çš„æ¨¡å‹(e.g. å¤æ‚æ€§è¾ƒå° or å¼•å…¥é¢†åŸŸçŸ¥è¯† or åŠ å…¥ç”¨æˆ·æ„å›¾)ï¼Œæœ‰åŠ©äºå‰Šå‡å‡è®¾ç©ºé—´ï¼Œä»è€Œé™ä½äº†æœ€å°åŒ–è®­ç»ƒè¯¯å·®çš„è¿‡æ‹Ÿåˆé£é™©ã€‚

!!! danger "å¦‚æœåœ¨ä¸€ä¸ªå®Œå…¨ç›¸åŒçš„è®­ç»ƒé›†ä¸Šè®­ç»ƒå‡ºäº”ä¸ªä¸åŒçš„æ¨¡å‹ï¼Œå“ªæ€•ä»–ä»¬å•ä¸ªå‡†ç¡®ç‡éƒ½å¾ˆé«˜ï¼Œå°†å®ƒä»¬ä½¿ç”¨æŠ•ç¥¨é›†æˆç»„åˆæˆä¸€ä¸ªæ–°çš„åˆ†ç±»å™¨ï¼Œé€šå¸¸ä¹Ÿä¼šå¸¦æ¥æ›´å¥½çš„ç»“æœã€‚å°¤å…¶æ˜¯æ¨¡å‹ä¹‹é—´éå¸¸ä¸åŒï¼Œä¾‹å¦‚(SVM, DT, LR, ...) æ•ˆæœæ›´ä¼˜ã€‚"
    !!! question "å¦‚æœä»–ä»¬æ˜¯ä¸åŒçš„è®­ç»ƒå®ä¾‹ä¸Šå®Œæˆè®­ç»ƒï¼Œé‚£å°±æ›´å¥½äº†"

\\\qquad \rightarrow \begin{cases}R_{LAD}(f)=\mathbb E\Vert Y-f(X)\Vert_1&\text{population level}\\R_n(f)=\frac{1}{n}\sum\limits_{i=1}^n\Vert Y_i-f(X_i)\Vert_1&\text{empirical risk}\end{cases}$

### loss functions

**requirements:** (å…¶å®æœ¬è´¨ä¸Šå°±æ˜¯é¢„æµ‹ç‚¹å’Œå®é™…çœŸå®ç‚¹çš„ **distance measure** )

1. symmetric
2. non-negative
3. identified
4. å°½å¯èƒ½çš„ robust

![](./pics/Loss_3.png){width=80%}

#### 0-1

$$L_i=I(Y_i\neq f(X_i;\theta))$$

<figure markdown="span">![](./pics/Loss_1.png){width=40%}<p>non-continuous, non-smooth<br></p></figure>

**extremely complicated ! The optimization problem is extremely hard !**

#### Least Squares Error, LSE, L2-loss â€”â€” conditional mean

$$L_i=(Y_i-f(X_i;\theta))^2$$

Regression â†’ Ordinary Least Squares (OLS) according to estimation åˆ†ç±»

**Targets**: **conditional mean** $\iff f^*(x)=\mathbb E(Y|X=x)=\min\limits_f\mathbb E\mathbb\{(Y-f(X))^2|X=x\}$

**Properties:**

1. differentiable and convex
    Differentiability allows us to take the **derivative** and locate the **minimum** point. Convexity allows us to claim a **global** **minimizer** (also unique if the objective function is strictly convex).
2. ä¼šæ›´é‡è§† outliers

!!! danger "LSE fail with contaminated data <br> æ•°æ®å­˜åœ¨ outliers çš„æ—¶å€™å°±å®¹æ˜“ä¸ robust"
    - å› ä¸ºå¹³æ–¹æ”¾å¤§äº†å·®åˆ« $\text{large}\rightarrow\text{large}^2,\text{small}\rightarrow\text{small}^2$. å’Œ outlier ç›¸å¯¹åº”çš„ loss å°±ä¼š dominate the empirical risk, åœ¨ regression with outliers é‡Œå°±ä¼šæ›´åå‘ approximate the outliers, the fitted curve has been distorted quite significantly.
    - LSE é¢„æµ‹å‡ºæ¥çš„ conditional meanï¼Œå…¶ä¸­ outlier ç‚¹æœ‰å‚ä¸è®¡ç®—ï¼Œï¼ˆæ¯”èµ· median æ¥è¯´ mean æ˜¯æ›´å®¹æ˜“å—åˆ° outlier å½±å“ï¼Œæ›´ä¸ robust metricï¼‰
    <div class="grid" markdown>
    ![](./pics/Loss_4.png){width=60%}
    ![](./pics/Loss_5.png){width=60%}
    </div>

#### Least Absolute Deviation, LAD â€”â€” conditional median

$$\Vert Y_i-f(X_i)\Vert_1=|Y_i-f(X_i;\theta)|$$

**Targets**: **conditional median** $\iff f^*(x)=\text{median}(Y|X=x)=\min\limits_f\mathbb E\{\vert Y-f(X)\vert\: |X=x\}$

![](./pics/Loss_6.jpeg){width=80%}

- proof $f^*(x)=\text{median}(Y\vert X=x)=\min\limits_f\mathbb{E}\{\Vert Y-f(X)\Vert_1\vert X=x\}$
  Assume:
    - $\forall x, \mathbb{E}[Y|X=x]\lt\infin$(å­˜åœ¨)
    - $F_{Y|X=x}(\cdot):=$ the conditional cdf of $Y|X=x, \begin{cases}\text{cdf of} -\infin=0\\\text{cdf of} +\infin=1\\\text{cdf of median} =\frac{1}{2}\end{cases}$

$$\begin{align*}
\mathcal{L}(f)&=\mathbb{E}\{\Vert Y-f(X)\Vert_1\vert X=x\}\\
&=\int_{-\infin}^{f(x)}f(x)-y\text{d}F_{Y|X=x}(y)+\int_{f(x)}^{+\infin}y-f(x)\text{d}F_{Y|X=x}(y)\\
\cfrac{\partial\mathcal{L}(f)}{\partial f}&=\int_{-\infin}^{f(x)}1\cdot\text{d}F_{Y|X=x}(y)+\int_{f(x)}^{+\infin}-1\cdot\text{d}F_{Y|X=x}(y)\\
&=F_{Y|X=x}(y)\Big\vert_{-\infin}^{f(x)}-F_{Y|X=x}(y)\Big\vert^{+\infin}_{f(x)}\\
&=F_{Y|X=x}(f(x))-0-1+F_{Y|X=x}(f(x))\xlongequal{SET}0\\
\implies & F_{Y|X=x}(f(x))=\cfrac{1}{2}\implies f(x)\text{ is median}
\end{align*}$$

**Properties:**

1. No amplification æ”¾å¤§. $\text{large}\rightarrow\text{large},\text{small}\rightarrow\text{small}$. åœ¨å¾ˆå¤šæ•°æ®çš„æƒ…å†µä¸‹ï¼Œ their contributions are less prominent. ï¼ˆå¦‚æœå¤ªå¤š strong outliers ä¾æ—§ä¼šfailed
2. **try to downplay the importance of the data point with a large deviation.** å°è¯•ä»¥è¾ƒå¤§çš„åå·®æ·¡åŒ–æ•°æ®ç‚¹çš„é‡è¦æ€§ã€‚
3. Non-differentiable.
    exists an alternative approach for solving this problem: using linear programming å•çº¯å½¢æ³•

#### Check Loss function â€”â€” conditional quantile

!!! p "Which class of data do we care aboutï¼Ÿ We can trace the Quartile"

$$L_i=\rho_Ï„(a)=(Ï„-I\{a<0\})*a=\begin{cases}Ï„a&a>0\\(Ï„-1)a&a<0\end{cases}$$

<div class="grid" markdown>
<figure markdown="span">![](./pics/Loss_7.png){width=80%}</figure>
<figure markdown="span">![](./pics/Loss_8.png)</figure>
</div>

**Targets: conditional median** $\iff f^*(x)=Ï„-\text{th quantile of }(Y|X=x)=\argmin_f\mathbb\{\Vert Y-f(X)\Vert_1ï½œX=x\}$

![](./pics/Loss_9.png)

- proof:
  Assume:
  - $\forall x, \mathbb{E}[Y|X=x]\lt\infin$(å­˜åœ¨)
  - $F_{Y|X=x}(\cdot):=$ the conditional cdf of $Y|X=x, \begin{cases}\text{cdf of} -\infin=0\\\text{cdf of} +\infin=1\\\text{cdf of median} =\frac{1}{2}\end{cases}$

$$\begin{align*}
\mathcal{L}(f)&=\mathbb{E}\{\rho_Ï„\cdot\Vert Y-f(X)\Vert_1\vert X=x\}\\
&=\int_{-\infin}^{f(x)}(Ï„-1)(y-f(x))\text{d}F_{Y|X=x}(y)+\int_{f(x)}^{+\infin}Ï„(y-f(x))\text{d}F_{Y|X=x}(y)\\
\cfrac{\partial\mathcal{L}(f)}{\partial f}&=\int_{-\infin}^{f(x)}(1-Ï„)\cdot\text{d}F_{Y|X=x}(y)+\int_{f(x)}^{+\infin}-Ï„\cdot\text{d}F_{Y|X=x}(y)\\
&=(1-Ï„)F_{Y|X=x}(y)\Big\vert_{-\infin}^{f(x)}-Ï„F_{Y|X=x}(y)\Big\vert^{+\infin}_{f(x)}\\
&=(1-Ï„)(F_{Y|X=x}(f(x))-0)-Ï„(1-F_{Y|X=x}(f(x)))\xlongequal{SET}0\\
\implies & F_{Y|X=x}(f(x))=Ï„ \implies f(x)\text{ is Ï„ th quantile of}
\end{align*}$$

  $F_{Y|X=x}(f^*(x))=Ï„,\forall Ï„\in(0,1)\implies f^*(x)=F_{Y|X=x}^{-1}(Ï„)$ **will be the conditional**Â ğ‰â€“th quantile ofÂ ğ’€|ğ‘¿Â =Â ğ’™

#### Hinge Loss

$$L_i=\sum\limits_{jâ‰ y_i}\max(0,s_j-s_{y_i}+1)$$

#### Squared Hinge Loss

$$L_i=\sum\limits_{jâ‰ y_i}\max(0,s_j-s_{y_i}+1)^2$$

#### Softmax

$$L_i=-\log\Big(\cfrac{\exp(s_{y_i})}{\sum\limits_j\exp(s_j)}\Big)$$
æ›´å…³æ³¨å°‘è§çš„é”™è¯¯

## Regularization - measure complexity and penalize

Prevent the model from doingÂ *too*Â well on training data, control å¤æ‚åº¦

## according to problem

### classification

- data: $(X_i,Y_i),i=1,\dots,n,X_i\in\R^p,Y_i$ is categorical
- Classifier: $\mathcal{F}=\{f:f(\cdot)\in \text{dom}(Y)\}$
- input: $X_i\in\R^{p}\iff X\in\R^{n\times p}$
- output: $\hat{y_i}\in \text{dom}(Y)$

å› ä¸ºYæ˜¯å±äºlabelå‹ï¼Œå¹¶æ²¡æœ‰ numerical meaningï¼Œæˆ‘ä»¬åªåœ¨ä¹**whether sample is assigned into the correct label or not**

å›å½’çš„æ—¶å€™å¤„ç†çš„æ˜¯è¯¯å·®ï¼Œæ‰€ä»¥è¦æœ€å°åŒ–ï¼Œè€Œç°åœ¨è€ƒè™‘çš„æ˜¯è”åˆæ¦‚ç‡ï¼Œæˆ‘ä»¬å¸Œæœ›æ¦‚ç‡å°½å¯èƒ½å¤§ï¼Œæ‰€ä»¥è¦æœ€å¤§åŒ–

<figure markdown="span">![](./pics/classi_1.png){width=40%}<p>å¤šåˆ†ç±»<br> Adjust the output of neural network</p></figure>

#### the number of success é”™æœ‰å¤šå°‘çš„è§’åº¦

å› ä¸ºæ˜¯ç±»åˆ«ï¼Œæ‰€ä»¥æ²¡æœ‰numericalæ„ä¹‰ï¼Œåªæœ‰å±äºå’Œä¸å±äºã€‚æ‰€ä»¥æˆ‘ä»¬å…ˆæƒ³åˆ°çš„æ˜¯ï¼šIndictor & 0-1 loss

##### From 0-1 loss

==Empirical Risk with 0-1 Loss==.
$$\min_f R(f) =\cfrac{1}{n}\sum\limits_{i=1}^nI(f(X_iâ‰ Y_i))$$

!!! danger "0-1 loss is non-continuous, non-smooth."
    <div class="grid" markdown>
    <figure markdown="span">![](./pics/Loss_1.png){width=40%}<p>non-continuous, non-smooth</p></figure>
    <p>but we expect: <b>continuous, smooth</b><br> ğŸ’¡ <u>Surrogate Loss function ä»£ç†</u>ã€‚Proper surrogate loss function will lead to a consistent classifier.</p>
    </div>

##### Surrogate Loss function

==Surroogate Loss Function==ã€‚$L_i=\phi(f(X_i,\theta)\times Y_i) $.
$\phi$ is continuous and decreasing.

==Empirical Risk with Surroogate Loss Function==ã€‚
$$\min_f R(f) =\cfrac{1}{n}\sum\limits_{i=1}^n\phi(f(X_i,\theta)\times Y_i)$$

**properties of** $\phi(\cdot):$

1. continuous: èƒ½é€šè¿‡æ¢¯åº¦æ±‚è§£ä¼˜åŒ–
2. decreasing: $f(Î§_i,\theta)Y_i\uparrow\iff \phi(f(X_i,\theta)\times Y_i)\downarrow $
$\begin{cases}Y_i=+1&\xrightarrow{\text{force}} f(X_i,\theta)>0\uparrow\implies\hat{Y_i}=+1 \\Y_i=-1&\xrightarrow{\text{force}} f(X_i,\theta)<0\downarrow \implies \hat{Y_i}=-1\end{cases}$

|  | $\phi(\cdot)$ | Loss Function |
| --- | --- | --- |
| 0-1 loss: | $I(\cdot)$ | $I(y\cdot f(x,\theta)<0)$ |
| Exponential loss<br>(AdaBoost) | $e^{-(\cdot)}$ | $e^{-y\cdot f(x,\theta)}$ |
| Logistic loss  | $\log\{1+e^{-(\cdot)}\}$ | $\log\{1+\exp(-y\cdot f(x,\theta))\}$ |
| Hinge loss<br>(SVM) | $\max\{1-(\cdot),0\}$ | $ \max\{1-y\cdot f(x,\theta),0\}$ |

![](./pics/Loss_2.png){width=50%}

#### The Likelihood ä¼¼ç„¶çš„è§’åº¦ Cross Entropy

==The Likelihood Function==

$\small{[P(_i=(1,0,...)|X_i=x)]^{I(Y_i=(1,0,...))}\times\dots\times [P(Y_i=(0,...,1)|X_i=x)]^{I(Y_i=(0,...,1))}}\\
=\prod\limits_{j=1}^{\text{\#category}}[P(Y_i=j|X=x)]^{I(Y_i=j)}\\
=[\hat{y_{i1}}]^{I(Y_{i1}=\red{1})}\times[\hat{y_{i2}}]^{I(Y_{i2}=\red{1})}\times\dots\times [\hat{y_{ij}}]^{I(Y_{ij}=\red{1})}\times\dots, \red{\begin{cases}\hat{y_i}=(\hat{y_{i1}},...,\hat{y_{ij}}\dots)\\\hat{y_{ij}}=P(Y_i=j|X=x)\\\hat{y_{ij}}\in[0,1],\sum\limits_{j=1}^m\hat{y_{ij}}=1\end{cases}}\\
=[\hat{y}_{i1}]^{Y_{i1}}\times[\hat{y}_{i2}]^{Y_{i2}}\times\dots\times [\hat{y}_{ij}]^{Y_{ij}}\times\dots,\qquad \red{Y_{ij}\in\{0,1\}:=X_i\text{æ˜¯ä¸æ˜¯å±äº}jç±»}$

$$L(Y_i|X_i)=\prod \limits_{j=1}^{\text{\#category}}[\hat{y}_{ij}]^{Y_{ij}}=[\hat{y}_{i1}]^{Y_{i1}}\times[\hat{y}_{i2}]^{Y_{i2}}\times\dots\times [\hat{y}_{ij}]^{Y_{ij}}\times\dots$$

==Log Likelihood Function==. $l(Y_i|X_i)=\log(L(\cdot))=\sum\limits_{j=1}^{\text{\#category}}Y_{ij}\times\log[\hat{y_{ij}}]\\\qquad =Y_{i1}\log[\hat{y}_{i1}]+Y_{i2}\log[\hat{y}_{i2}]+\dots+Y_{ij}\log[\hat{y}_{ij}]+\dots$

##### Cross Entropy

==Cross Entropy Loss==. $\text{CELoss}_i =-\sum\limits_{j=1}^{\text{\#category}}Y_{ij}\times \log \hat{y}_{ij}$

==Empirical Risk with Cross Entropy Loss==. $R(f)=\frac{1}{\text{\#sample}} \sum\limits_{i=1}^{\text{\#sample}}\Big[-\sum\limits_{j=1}^{\text{\#category}}Y_{ij}\times \log \hat{y_{ij}}\Big]=\cfrac{1}{\red{n}} \sum\limits_{i=1}^{\red{n}}\Big[-\sum\limits_{j=1}^{\text{\red{m}}}Y_{ij}\times \log \hat{y_{ij}}\Big]$

$\begin{cases}n:=\text{\#samples},m:=\text{\#catrgories}\\Y_{ij}\in\{0,1\}, \hat{y_{ij}}\in[0,1],\sum\limits_{j=1}^m\hat{y_{ij}}=1\end{cases}$

é¦–å…ˆå®ƒæ˜¯è”åˆæ¦‚ç‡ã€‚æ¦‚ç‡éƒ½æ˜¯1ä»¥ä¸‹çš„æ•°ï¼Œæ‰€ä»¥åƒè”åˆæ¦‚ç‡è¿™ç§æ¦‚ç‡ä¹˜æ³•çš„å€¼ä¼šè¶Šæ¥è¶Šå°ã€‚[æ’å›¾] çš„ç¡®å¦‚æ­¤ã€‚å¦‚æœå€¼å¤ªå°ï¼Œç¼–ç¨‹æ—¶ä¼šå‡ºç°ç²¾åº¦é—®é¢˜â€”â€” **ä¸ºä»€ä¹ˆfloat16ä¼šæŸå®³æ­£ç¡®ç‡**

å¯ä»¥è¯´äº¤å‰ç†µæ˜¯ç›´æ¥è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒï¼Œæˆ–è€…è¯´ä¸¤ä¸ªmodelä¹‹é—´çš„å·®å¼‚ã€‚è€Œä¼¼ç„¶å‡½æ•°åˆ™æ˜¯è§£é‡Šä»¥modelçš„è¾“å‡ºä¸ºå‚æ•°çš„æŸåˆ†å¸ƒæ¨¡å‹å¯¹æ ·æœ¬é›†çš„è§£é‡Šç¨‹åº¦ã€‚å› æ­¤ï¼Œå¯ä»¥è¯´è¿™ä¸¤è€…æ˜¯â€œåŒè²Œä¸åŒæºâ€ï¼Œä½†æ˜¯â€œæ®Šé€”åŒå½’â€å•¦ã€‚

## ğŸ“‘Â ref
- [Understanding the Bias-Variance Tradeoff]
- [åå·®ï¼ˆBiasï¼‰ä¸æ–¹å·®ï¼ˆVarianceï¼‰]
- [ã€æ·±åº¦å­¦ä¹ ã€‘ä¸€æ–‡è¯»æ‡‚æœºå™¨å­¦ä¹ å¸¸ç”¨æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰]
- [Chapter 7 Regression]

[åå·®ï¼ˆBiasï¼‰ä¸æ–¹å·®ï¼ˆVarianceï¼‰]: https://zhuanlan.zhihu.com/p/38853908
[Understanding the Bias-Variance Tradeoff]:http://scott.fortmann-roe.com/docs/BiasVariance.html
[ã€æ·±åº¦å­¦ä¹ ã€‘ä¸€æ–‡è¯»æ‡‚æœºå™¨å­¦ä¹ å¸¸ç”¨æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰]:https://cloud.tencent.com/developer/article/1165263
[Chapter 7 Regression]: https://probability4datascience.com/ch07.html
