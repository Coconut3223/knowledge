# high dimentional DA

==Data mining== is the process of discovering new patterns from LARGE DATA sets using methods of artificial intelligence, machine learning, statistics and database systems.

==Curse of Dimensionality ç»´åº¦ç¾éš¾==ã€‚ ä¼šå¯¼è‡´åˆ†ç±»å™¨å‡ºç°**è¿‡æ‹Ÿåˆ**ã€‚è¿™æ˜¯å› ä¸º<u>åœ¨æ ·æœ¬å®¹é‡å›ºå®šæ—¶ï¼Œéšç€ç‰¹å¾æ•°é‡çš„å¢åŠ ï¼Œå•ä½ç©ºé—´ä¸­çš„æ ·æœ¬æ•°é‡ä¼šå˜å°‘ã€‚</u>æ°å½“çš„ç»´æ•°ç‰¹å¾æ•°å¯¹äºæœºå™¨å­¦ä¹ æ¨¡å‹éå¸¸é‡è¦ã€‚æ·±åº¦å­¦ä¹ é€šè¿‡å¯¹æ ·æœ¬çš„ç‰¹å¾è¿›è¡Œå¤æ‚çš„å˜æ¢ï¼Œå¾—åˆ°å¯¹ç±»åˆ«æœ€æœ‰æ•ˆçš„ç‰¹å¾ï¼Œä»è€Œæé«˜æœºå™¨å­¦ä¹ çš„æ€§èƒ½ã€‚

<div class="grid" markdown>
<figure markdown="span">![](./pics/highD_1.png){width=60%}<figure>
<figure markdown="span">![](./pics/highD_2.png){width=60%}<figure>
<p>å‡è®¾æ ·æœ¬é›†æ˜¯ç”±åœ†å½¢å’Œä¸‰è§’å½¢ç»„æˆçš„20ä¸ªæ ·æœ¬ï¼Œå‡è®¾è¿™äº›æ ·æœ¬å‡åŒ€åœ°åˆ†å¸ƒåœ¨è¿™4ä¸ªåŒºåŸŸï¼Œåˆ™æ¯ä¸ªåŒºåŸŸçš„æ ·æœ¬ä¸ªæ•°çº¦ä¸º5ä¸ªã€‚è‹¥å¸Œæœ›åœ¨äºŒç»´ç©ºé—´ä¸­æ¯ä¸ªåŒºåŸŸçš„æ ·æœ¬æ•°é‡ä¸ä¸€ç»´æ—¶å¤§è‡´ç›¸ç­‰ï¼Œåˆ™éœ€è¦400ä¸ªæ ·æœ¬ï¼›è‹¥æ˜¯ä¸‰ç»´ç©ºé—´ï¼Œåˆ™éœ€è¦8000ä¸ªæ ·æœ¬</p>
</div>

!!! danger "å¾ˆå°‘ **observation nï¼Œ** å¾ˆå¤š **features p** æƒ…å†µä¸‹çš„é«˜ç»´.<br> p is very large, but n is relatively small."
    å°±æ‹¿åŒ»å­¦æ¥è¯´ï¼Œç—…äººæ€»æ˜¯å°‘æ•°çš„ï¼Œä½†æ˜¯ç›¸å…³çš„å› ç´ æ€»æ˜¯ç‰¹åˆ«å¤šçš„ã€‚è­¬å¦‚é‚£ä¸ªåŸºå› æ£€æµ‹ã€‚æˆ‘ä»¬æ€»å¾—è§£å†³è¿™ç§é«˜ç»´é—®é¢˜ã€‚
    > æœ‰569ä¸ªobservationsï¼Œ30ä¸ª featuresã€‚å¯¹äºæ±‚features çš„ covariance matrix $Î£\in S$æ¥è¯´ï¼Œæœ‰$\cfrac{30*29}{2}\approx 430$ä¸ª parameters è¦å» estimateã€‚å¦‚æœ take average å‡ ä¹æ˜¯ä¸€ä¸ªparameter ä¸€ä¸ªobservationï¼Œè¿™å·²ç»ç®—æ˜¯ high dimensional problemï¼Œé™¤éæ•°æ®very clean.

| æ•°æ®é™ç»´ dimensionally reduction | ç‰¹å¾é€‰æ‹© Variable Selection |
| --- | --- |
| å¤šä¸ªç‰¹å¾åˆæˆä¸ºä¸€ä¸ªç‰¹å¾ | åœ¨å¤šä¸ªç‰¹å¾ä¸­é€‰æ‹©æŸä¸ªç‰¹å¾ |
| è·å–æ— æ³•è§£é‡Šçš„ç‰¹å¾ä¸å˜é‡ä¹‹é—´çš„å…³ç³» | å¯è§£é‡Šæ€§å¼º |

## some special notation

==Random Vector== $Z=\begin{bmatrix}Z_1\\\vdots\\Z_p\end{bmatrix}\in\R^p$

- ==Expectation Matrix== $\mathbb EZ=\begin{bmatrix}\mathbb EZ_1\\\vdots\\\mathbb EZ_p\end{bmatrix}$
- ==Covariance Matrix== $\Sigma=Var(Z)\\=\mathbb E\{(Z-\mathbb EZ)(Z-\mathbb EZ)^T\}\\=\begin{bmatrix}Var(Z_1)&Cov(Z_1,Z_2)&\dots&Cov\{Z_1,Z_p\}\\Cov(Z_1,Z_2)&Var(Z_2)&\dots&Cov(Z_2,Z_p)\\\vdots&&\ddots&\vdots\\Cov(Z_p,Z_1)&Cov(Z_p,Z_2)&\dots&Var(Z_p)\end{bmatrix}$
    - $\Sigma \succeq0$
    proof: the sample covariance matrix is non-negative definite.

Correlationåªæ˜¯è€ƒå¯Ÿçº¿æ€§å…³ç³»çš„ç›¸å…³æ€§ï¼Œå¹¶ä¸æ˜¯ä»£è¡¨independent

- $\mathbb E(X+Y)=\mathbb EX+\mathbb EY$
- $W:=A_{p\times p}Z_{p\times 1}+c\in\R^p$, **constant** matrixÂ A, **constant** vector c

    $\mathbb E(AZ+c)=c+A\mathbb EZ$

    $Var(AZ+c)=Var(AZ)=AVar(Z)A^T$

- $\mathbb E(AXB+c)=A\mathbb EXB+c$

1. Positive-definite matrices )

    Eigenvalue decomposition $A=\Gamma\Lambda\Gamma^T=\sum\limits_{i=1}^p\lambda_i\gamma_i\gamma_i^T$

==Multivariate Normal Distribution== $Zï½N(b,\Sigma)$
Suppose $Z=\begin{bmatrix}Î¾_1\\\vdots\\Î¾_p\end{bmatrix}\in\R^p$ is a random vector, $\mathbb EZ=b\in\R^p,Var(Z)=\Sigma\in S^p$
$\forall l\in\R^p,l^TZ\in\R$ ï½ Normal distribution. $\implies Z$ follows Multivariate Normal Distribution.
$\iff Zï½N(b,\Sigma),b=\begin{bmatrix}b_1\\\vdots\\b_p\end{bmatrix},\Sigma=\begin{bmatrix}\sigma_{11}&\dots&\sigma_{1p}\\\vdots&\ddots&\vdots\\\sigma_{p1}&\dots&\sigma_{pp}\\\end{bmatrix}\\
\iff  f_Z(Î¾_1,\dots,Î¾_p)=\cfrac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}\exp\Big(-\cfrac{1}{2}(Î–-b)^T)\Sigma^{-1}(Z-b) \Big)$

**properties:**

1. $\forall\text{ constant }l\in\R^p,c\in\R,\space l^TZ+cï½N(l^Tb+c,l^T\Sigma l)$
2. Partial correlation and conditional independence
$(Î–_1^T,Î–_2^T)^Tï½N\Big(\begin{bmatrix}b_1^T&b_2^T\end{bmatrix}^T,\begin{bmatrix}\Sigma_{11}&\Sigma_{12}\\\Sigma_{21}&\Sigma_{22}\end{bmatrix}\Big)$
$\\\quad \iff \begin{bmatrix}Î¾_{11}\\\vdots\\Î¾_{1p}\\Î¾_{21}\\\vdots\\Î¾_{2p}\end{bmatrix}ï½N\Big(\begin{bmatrix}b_1\\b_2\end{bmatrix},\begin{bmatrix}\Sigma_{11}&\Sigma_{12}\\\Sigma_{21}&\Sigma_{22}\end{bmatrix}\Big)\\\quad\iff \begin{cases}Z_iï½Î(b_i,\Sigma_{ii})\\Z_1|Z_2ï½N\Big(b_1+\Sigma_{12}\Sigma_{22}^{-1}(X_2-b_2),\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}\Big)\end{cases}$

- proof:

1. åœ¨ normal multivariate distribution é‡Œ Covariance = 0  ç­‰åŒäº independent
$cov(Z_1,Z_2)=\sigma_{ij}=0\iff Z_1,Z_2 \text{ are independent}$
   - proof
2. $Î¾_kï½N(b_k,\sigma_{kk}),\begin{cases}Î¾_i+Î¾_jï½N(b_i+b_j,\sigma_{ii}+2\sigma_{ij}+\sigma_{j})\in\R\\Î¾_i-Î¾_jï½N(b_i-b_j)ï½Î(\sigma_{ii}-2\sigma_{ij}+\sigma_{jj})\in\R\end{cases}$

==I.I.D. observations==. $\text{Suppose } X_1,X_2,\dots,X_m\in\R^p \text{ are i.d.d.}$

1. $\mathbb EX=\overline X=\cfrac{1}{n}\sum\limits_{i=1}^nX_i$
2. ==Sample covariance matrix== $S=\begin{cases}\cfrac{1}{n}\sum\limits_{i=1}^n(X_i-\overline X)(X_i-\overline X)^T\\\cfrac{1}{n-1}\sum\limits_{i=1}^n(X_i-\overline X)(X_i-\overline X)^T\end{cases}\in S^{p}$
å…³äºè¿™ä¸ª n-1 æ˜¯å› ä¸ºå‡å€¼å·²çŸ¥ï¼Œæ— åä¼°è®¡
3. ==Central Limit Theorem CLT==. Suppose $X_1,X_2,\dots,X_m\in\R^p \text{ are i.d.d.},\mathbb EX=\mu\in\R^p,Cov(X)=\Sigma\\\qquad\lim\limits_{n\rightarrow\infin}\cfrac{1}{n}\sum\limits_{i=1}^n(X_i-\mu)\rightarrow N(0,\Sigma)$
4. ==Law of Large numbers==. Suppose $X_1,X_2,\dots,X_m\in\R^p \text{ are i.d.d.},EX_i=\mu,Cov(X_i)=\Sigma\\\qquad\begin{cases}\lim\limits_{n\rightarrow\infin}\cfrac{1}{n}\sum\limits_{i=1}^nX_i\rightarrow E(X_i)=\mu\\\lim\limits_{n\rightarrow\infin}\cfrac{1}{n}\sum\limits_{i=1}^n(X_i-\mu)(X_i-\mu)^T\rightarrow\Sigma\end{cases}$

> > (AMA565_L0_T2) SupposeÂ $eÂ ï½Â N(0,Ïƒ^2I)$, what is the distribution of $\hat{Î²}Â = (X^TX)^{âˆ’1}X^T(XÎ²Â +Â e)$? (Assume that the inverse and matrix multiplication are well defined)

---

> > (AMA565_L0_T3) SupposeÂ $XÂ ï½Â NÂ (Î¼,Â Î£)$, and let the eigenvalue decomposition ofÂ $Î£ = (Ïƒ_{ij})_{1â‰¤i,jâ‰¤p}$Â be given asÂ $Î£ = Î“Î›Î“â€²$Â whereÂ Î“Â is an orthogonal matrix andÂ $Î› = \text{diag}\{Î»_1, . . . , Î»_p\}$Â is the matrix of the eigenvalues.
> > 1). What is the distribution ofÂ $Î“â€²X$?> > 2). LetÂ $Î£^{âˆ’Â 1/2}Â = Î“Î›^{âˆ’Â 1/2}Â Î“â€².$ What is the distribution ofÂ $Î£^{âˆ’Â 1/2}Â X$?
> > 3). SupposeÂ pÂ = 2Â and denoteÂ $XÂ = (X_1, X_2)â€²$. In addition,Â $Ïƒ_{11}=Ïƒ_{22} = 1, Ïƒ_{12}=Ï$. What is the distribution of $(Y_1,Y_2), Y_1=\cfrac{(X_1+X_2)}{\sqrt{2+2Ï}}, Y_2 = \cfrac{(X_1 âˆ’ X_2)}{\sqrt{2 âˆ’ 2Ï}} ?$

## Special Matrix

$A=11^T=\begin{bmatrix}1&1&\dots&1\\\vdots&\ddots&\dots&\vdots\\1&\dots&\dots&1\end{bmatrix}\in\R^p=p\cdot \begin{bmatrix}\cfrac{1}{\sqrt p}\\\vdots\\\cfrac{1}{\sqrt{p}}\end{bmatrix}*\begin{bmatrix}\cfrac{1}{\sqrt p}&\dots&\cfrac{1}{\sqrt{p}}\end{bmatrix}+0...$

==AR(1) model== $A=\begin{bmatrix}1&\rho&\rho^2&\dots&\rho^{n-1}\\\rho&1&\rho&\dots&\rho^{n-2}\\\vdots&\vdots&\vdots&\ddots&\vdots\\\rho^{n-1}&\rho^{n-2}&\dots&\dots&1\end{bmatrix} \xrightarrow{e.g,}\begin{bmatrix}1&0.9&0.9^2&0.9^3\\0.9&1&0.9&0.9^2\\0.9^2&0.9&1&0.9\\0.9^3&0.9^2&0.9&1\end{bmatrix}$

## Variable Selection

ğŸ“‘Â ref

- [å¦‚ä½•è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼ˆç†è®ºç¯‡ï¼‰æœºå™¨å­¦ä¹ ä½ ä¼šé‡åˆ°çš„â€œå‘â€]
- [Are screening methods useful in feature selection? An empirical study]

[Are screening methods useful in feature selection? An empirical study]: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0220842
[å¦‚ä½•è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼ˆç†è®ºç¯‡ï¼‰æœºå™¨å­¦ä¹ ä½ ä¼šé‡åˆ°çš„â€œå‘â€]:https://baijiahao.baidu.com/s?id=1604074325918456186&wfr=spider&for=pc

ç‰¹å¾
|useful, important features|irreverent features|redundant feature|
|--|--|--|
|æˆ‘ä»¬å¸Œæœ›èƒ½ï¼šextract ä»–ä»¬ï¼Œä»–ä»¬èƒ½ä¸»å¯¼æ•´ä¸ª signals|æ— å…³ç‰¹å¾ã€‚æœ‰ä»–æ²¡ä»–éƒ½ä¸€æ ·|å†—ä½™ç‰¹å¾ã€‚Delete|

==redundant variable==. that preditor with 0 coefficient  $\beta_k=0\implies$  unimportant and meaningless

Variable, Predictor, in the model, plays two roles: **improving the model flexibility and adversely affecting the model stability**. Redundant variables are not helpful in prediction, thus should be removed.

**ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦ Figure out what important variables are and Delete redundant featuresï¼Ÿ**

å› ä¸ºå¦‚æœæˆ‘ä»¬ä¸å¤„ç†æ‰ redundant variablesï¼Œ é‚£äº› noises created by redundant variables maybe dominate the signals, causing trouble to ask for allocating those useful signals.
æ¯”å¦‚ï¼Œé€šè¿‡æˆ¿å±‹çš„é¢ç§¯ï¼Œå§å®¤çš„é¢ç§¯ï¼Œè½¦åº“çš„é¢ç§¯ï¼Œæ‰€åœ¨åŸå¸‚çš„æ¶ˆè´¹æ°´å¹³ï¼Œæ‰€åœ¨åŸå¸‚çš„ç¨æ”¶æ°´å¹³ç­‰ç‰¹å¾æ¥é¢„æµ‹æˆ¿ä»·ï¼Œé‚£ä¹ˆæ¶ˆè´¹æ°´å¹³ï¼ˆæˆ–ç¨æ”¶æ°´å¹³ï¼‰å°±æ˜¯å¤šä½™ç‰¹å¾ã€‚è¯æ®è¡¨æ˜ï¼Œæ¶ˆè´¹æ°´å¹³å’Œç¨æ”¶æ°´å¹³å­˜åœ¨ç›¸å…³æ€§ï¼Œæˆ‘ä»¬åªéœ€è¦å…¶ä¸­ä¸€ä¸ªç‰¹å¾å°±å¤Ÿäº†ï¼Œå› ä¸ºå¦ä¸€ä¸ªèƒ½ä»å…¶ä¸­ä¸€ä¸ªæ¨æ¼”å‡ºæ¥ã€‚ï¼ˆå¦‚æœæ˜¯çº¿æ€§ç›¸å…³ï¼Œé‚£ä¹ˆæˆ‘ä»¬åœ¨ç”¨çº¿æ€§æ¨¡å‹åšå›å½’çš„æ—¶å€™ï¼Œä¼šå‡ºç°ä¸¥é‡çš„å¤š**é‡å…±çº¿æ€§é—®é¢˜**ï¼Œå°†ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆã€‚ï¼‰
ç‰¹å¾é€‰æ‹©è¿˜å¯ä»¥ä½¿æ¨¡å‹è·å¾—æ›´å¥½çš„è§£é‡Šæ€§ï¼ŒåŠ å¿«æ¨¡å‹çš„è®­ç»ƒé€Ÿåº¦ï¼Œä¸€èˆ¬çš„ï¼Œè¿˜ä¼šè·å¾—æ›´å¥½çš„æ€§èƒ½

**å¸¸è§çš„æ–¹æ³•åŒ…æ‹¬è¿‡æ»¤æ³•ï¼ˆFilter, Screeningã€åŒ…è£¹æ³•ï¼ˆWarpperï¼‰ï¼ŒåµŒå…¥æ³•ï¼ˆEmbeddingï¼‰ã€‚**

### Filter, Screening, è¿‡æ»¤æ³•

é€‰æ‹©ä¸€äº› important features, which is a **subset** of all features, ä½†æˆ‘ä»¬ä¸æ˜¯ç®€å•çš„é€‰æ‹©ï¼Œè€Œæ˜¯è®¾ç½®ä¸€ä¸ª threshold å°½é‡ save all signalsã€‚ä½† No free lunch, it just a trade.

**SAME Assumptions:**

$\begin{cases}\text{centered X: } \mathbb EX=0,Cov(X)=\Sigma, \text{centered Y: } \mathbb EY=0\implies \beta_0=0\\(X_i,Y_i)\text{ are IDD}\\(X_i,Y_i,\epsilon_i)\text{ are independent}\\\red {\Xi}:=\text{error between estimator and truth}\in\R^n=\begin{bmatrix}\epsilon_1&\dots&\epsilon_n\end{bmatrix}^T\\\mathbb E\Xi=0,Var(\Xi)=\sigma^2I\in\R^{n\times n}\end{cases}$

|  | Perfect Models | More Redundant Variables | Less Important Variables |
| --- | --- | --- | --- |
| p | $p=p_0$ | $p>p_0$ | $p<p_0$ |
|  | Correct  | Correct  | Wrong |
| $\epsilon_i$ | $N(0,\sigma^2)$ | $N(0,\sigma^2)$ | $\red {\tilde\epsilon_i ,E(\tilde\epsilon_i)\neq0}$ |
| $Bias(\hat{Y}_{new})$ | âŒ | âŒ | â­• $E\tilde{\beta}-\beta)^TX_{new}$ |
| $E\{(Y-\hat{Y})^2\}$ | $\approx\sigma^2(1+\cfrac{p_0}{n})$ | $\approx\sigma^2(1+\cfrac{p}{n})$ | $\approx\sigma^2+(\beta_{p+1}^2+\dots+\beta_{p_o^2})\lambda_{min}(\Sigma)$ |

#### Perfect Models with General Variables

==Perfect model ($p=p_0$) variables==. $Y_i=\beta_1x_{i,1}+\dots+\beta_{p_0}x_{i,p_0}+\red{\epsilon_i}$
$\iff Y_i=\beta^TX_i+\epsilon_i, \begin{cases}Y_i\in\R,\\\beta=\begin{bmatrix}\beta_1&\dots&\beta_{p_0}\end{bmatrix}^T\in\R^{p_0}\\X_i=\begin{bmatrix}x_{i,1}&\dots&x_{i,p_0}\end{bmatrix}^T\in\R^{p_0}\\[1em]Cov(X)=\Sigma\in\R^{p_0\times p_0}\\\red{\epsilon_i\text{ are IID}ï½N(0,\sigma^2)}\end{cases}$
$\implies Y=X\beta+\Xi,\begin{cases}Y\in\R^n\\ X\in\R^{n\times p_0 }=\begin{bmatrix}X_1^T&\dots&X_n^T\end{bmatrix}^T\\\beta\in\R^{p_0}\end{cases}$

==the estimator==. $ \hat{\beta} = (\mathbb X^T\mathbb X)^{-1}\mathbb X^T\mathbb Y=\beta+(\mathbb X^T\mathbb X)^{-1}\mathbb X^T \red {\Xi }$
$\\\quad \begin{cases}\mathbb X:= \text{training dataset with n traing samples } X_i\\\mathbb X^T\mathbb  X =\sum\limits_{i=1}^nX_iX_i^T,\\X_i \text{ are IID }ï½N(0,\Sigma)\implies\mathbb  X^T\mathbb X=\sum\limits_{i=1}^n(X_i-0)(X_i-0)^T=\red{n\Sigma}\\\end{cases}$

$\hat{\beta}=\beta+(n^{-1}\mathbb X^T\mathbb X)^{-1}n^{-1}\mathbb X^T\Xi\approx \beta+\Sigma^{-1}n^{-1}\mathbb X^T\Xi\implies\\[1em]\hat{\beta}-\beta=(n^{-1}\mathbb X^T\mathbb X)^{-1}n^{-1}\mathbb X^T\Xi\approx\Sigma^{-1}n^{-1}\mathbb X^T\Xi$

!!! p "å‰è€… $(n^{-1}\mathbb X^T\mathbb X)$ æ˜¯æ ·æœ¬ç®—å‡ºæ¥çš„ï¼Œåè€…  $\Sigma$  æ˜¯åˆ†å¸ƒçš„æ–¹å·®ï¼Œå‰è€… converge into åè€…ï¼Œå‡å°‘äº† randomnessï¼Œæ‰€ä»¥æ˜¯ approximately"

**For a new random observation** $X_{new}$
$$\begin{align*}Y_{new}&=\beta^TX_{new}+\epsilon_{new}\\\hat{Y}_{new} &=\hat{\beta}^TX_{new}
\end{align*}$$

$(X_{new},Y_{new},\epsilon_{new})$ is independent of $(X_i,Y_i,\epsilon_i)$

$$\begin{align*}
Y_{new}-\hat{Y}_{new}&=\underline{\red {\epsilon_{new}}+\beta^{\red{T}}X_{new}}-\underline{\hat{\beta}^{\red T}X_{new}}\\
&=\red {\epsilon_{new}}+(\beta-\hat{\beta})^{\red T}X_{new}\\
&\approx\red {\epsilon_{new}}-\underline{n^{-1}\Xi^T\mathbb X\Sigma^{-1}}X_{new}
\end{align*}$$

Square of Prediction Error

$$
\begin{align*}(Y_{new}-\hat{Y}_{new})^2&\approx(\red {\epsilon_{new}}-\underline{n^{-1}\Xi^T\mathbb X\Sigma^{-1}}X_{new})^2\\[1em]&\approx\red {\epsilon_{new}}^2-2\red {\epsilon_{new}}\cdot n^{-1}X_{new}^T\Sigma^{-1}\mathbb X^T\Xi+n^{-2}(\Xi^T\mathbb X\Sigma^{-1}X_{new})^2\end{align*}
$$

Take expectation:

$$
\begin{align*}\mathbb E\{(Y-\hat{Y})^2\}&=\mathbb E\{(Y-\hat{\beta}^TX)^2\}\\&\approx\mathbb E\Big(\red {\epsilon}^2-2\red {\epsilon}\cdot n^{-1}X^T\Sigma^{-1}\mathbb X^T\Xi+n^{-2}(\Xi^T\mathbb X\Sigma^{-1}X)^2\Big)\\&\approx\sigma^2-0+\cfrac{p_0}{n}\sigma^2\\&\approx\sigma^2(1+\cfrac{p_0}{n})\end{align*}
$$

$\implies$ **When n is large, the perfect model has the smallest prediction error**

#### Working Model with More Redundant Variables

å¦‚æœæˆ‘ä»¬ä¸é€‰æ‹©é‡è¦ç‰¹å¾ï¼Œæˆ‘ä»¬å°è¯•ä¸ºæŠ˜æœ‰äº‹ç‰©æ·»åŠ ä¼°è®¡ç³»æ•°ï¼Œé‚£ä¹ˆè¯¯å·®å°†æ±‡æ€»æƒå°†æ±‡æ€»ï¼Œå³æ¯æ¬¡æˆ‘ä»¬ä¼°è®¡æŸäº›ä¸œè¥¿æ—¶ï¼Œæ‚¨éƒ½ä¼šåˆ›ä¸€ä¸ªé”™è¯¯ã€‚

==Wroking model with more redundant variables ($p$ variables, $\red{p>p_0}$)==

 $Y_i=\beta_1x_{i,1}+\dots+\beta_{p_0}x_{i,p_0}+0\times x_{i,p_0}+\dots+0\times x_{i,p}+\red{\epsilon_i},$
 $\iff Y_i=\beta^TX_i+\epsilon_i,i=1,\dots,n, \\\qquad\begin{cases}Y_i\in\R,\\\beta=\begin{bmatrix}\beta_1&\dots&\beta_{p_0}&0&\dots&0\end{bmatrix}^T\in\R^p,X_i=\begin{bmatrix}x_{i,1}&\dots&x_{i,p_0}&x_{i,p_0}&\dots&x_{i,p}\end{bmatrix}^T\in\R^p\\Cov(X)=\Sigma\in\R^{p\times p}\\\red{\epsilon_i\text{ are IID}ï½N(0,\sigma^2)}\end{cases}$

==the estimator== $\hat{\beta} = (\mathbb X^T\mathbb X)^{-1}\mathbb X^T\mathbb Y=\beta+(\mathbb X^T\mathbb X)^{-1}\mathbb X^T \red {\Xi }\\\quad \begin{cases}\mathbb X\in\R^{n\times p},\mathbb X^T\mathbb  X =\sum\limits_{i=1}^nX_iX_i^T\in\R^{p\times p},\\X_i \text{ are IID }ï½N(0,\Sigma)\implies\mathbb  X^T\mathbb X=\sum\limits_{i=1}^n(X_i-0)(X_i-0)^T=\red{n\Sigma}\in\R^{p\times p}\\\end{cases}$

Expected Square of Prediction Error

$$\mathbb E\{(Y-\hat{Y})^2\}=\mathbb E\{(Y-\hat{\beta}^TX)^2\}\approx\sigma^2(1+\cfrac{p}{n})>\sigma^2(1+\cfrac{p_0}{n}),p>p_0
$$

proof of

$\implies$  **the more redundant variables, the worse the prediction is.**
$\implies$  **if n is very large, it is ok to put all features in model as well since $\cfrac{p_0}{n}$  would vanish**

#### Working Model with Less Important Variables

==Wroking model with less important variables ($p$ variables, $\red{p<p_0, \beta_{p_0}\neq0}$==
$Y_i=\beta_1x_{i,1}+\dots+\beta_p\times x_{i,p}+\red{\tilde{\epsilon_i}}$
$\iff Y_i=\beta^TX_i+\red{\tilde{\epsilon_i}},i=1,\dots,n, \\\qquad\begin{cases}Y_i\in\R,\\\beta=\begin{bmatrix}\beta_1&\dots&\beta_{p}\end{bmatrix}^T\in\R^p,X_i=\begin{bmatrix}x_{i,1}&\dots&x_{i,p}\end{bmatrix}^T\in\R^p\\Cov(X)=\Sigma\in\R^{p\times p}\\\red{!!E(\tilde{\epsilon})\neq 0\impliedby\text{wrong model without enough variables}}\\\lambda_{\min}(\Sigma)>0\end{cases}$

==the estimator== $\hat{\beta}=\begin{bmatrix}\hat{\beta_1}&\dots&\hat{\beta_p}\end{bmatrix}^T\in\R^p,\red{p<p_0}\\\xrightarrow{å˜å½¢}\red{\tilde{\beta}}=\begin{bmatrix}\hat{\beta}\\0\end{bmatrix}=\begin{bmatrix}\hat{\beta_1}&\dots&\hat{\beta_p}&0&\dots&0\end{bmatrix}^T\in\R^{p_0}$

$\implies\hat{Y}_{new}=\hat{\beta}_{1\times p}{X_{new}}_{p\times1}=\tilde{\beta}^T_{1\times p_0}{X_{new}}_{p_0\times1}\\\qquad\qquad=\hat\beta_1x_1+\dots+\hat{\beta_p}x_p+0\times x_p+\dots+0\times x_{p_0}$

**There is a bias in the prediction for a given X.**

$Bias(\hat{Y}_{new})=\mathbb EY_{new}-\mathbb E\hat{Y}_{new}=\mathbb E\{(\beta-\tilde{\beta})^TX_{new}+\epsilon_{new}\}=(\mathbb E\tilde{\beta}-\beta)^TX_{new}$

Expected Square of Prediction Error

$$
\begin{align*}\mathbb E\{(Y-\hat{Y})^2\}&=\sigma^2+\mathbb E\{(\beta-\tilde{\beta})^T\Sigma(\beta-\tilde{\beta})\}\\[1em]&\ge\sigma^2+\Vert\beta-\tilde\beta\Vert^2\lambda_{min}(\Sigma)\\[1em]&\ge\sigma^2+(\beta_{p+1}^2+\dots+\beta_{p_0^2})\lambda_{min}(\Sigma)\end{align*}
$$

- proof of

$\implies$  If the working model does not include all the important variables (those with $Î²_{k}â‰  0,k=1,\dots,p_0$), the prediction error (lower bound) is also bigger than the model with exactly the important variables.
$\implies$ **æ ·æœ¬æ•° n å†å¤§ä¹Ÿæ‹¯æ•‘ä¸äº†è¿™ä¸ªerror å› ä¸º$(\beta_{p+1}^2+\dots+\beta_{p_0^2})\lambda_{min}(\Sigma)$ is constantï¼Œè€Œä¸”è¿™åªæ˜¯ä¸‹ç•Œ lower bound**

#### Candidate Models for p+1 predictor $1ï¼Œ x_1, ..., x_p$

<figure markdown="span">![](./pics/FS_1.png){width=80%}<p>un-centralized</p></figure>

Suppose we have n samples. Consider any sub-model (A)
$(A):= Y =\beta_0+Î²_1x'_1 +...+Î²_qx'_q+Îµ, \space\{x'_1,\dots,x'_q\}\sub\{x_1,\dots,x_p\}$

==RRS of A==. $RSS(A)=\sum\limits_{i=1}^n\{Y_i-\hat{Y}_{A,i}\}^2$.
$\hat{Y}_{A,i}:=$ the fitted value(estimation) of $Y_i$ generated by model (A

!!! warning "Fitted error (RSS) å¯ä»¥å»è¡¡é‡how good model areï¼Œ ä½†æ˜¯ cannot be used as one criterion for the selection."

1. For any two models A and B, if A is a sub-model of B, then $RSS(A) â‰¥ RSS(B).$ åªè¦ Aæ˜¯ B çš„å­é›†ï¼Œé‚£ä¹ˆ RSS(A) ä¸€å®šâ‰¥ RSS(B)ã€‚
2. è€Œä¸”è¿™ä¸ªRSSæ˜¯åœ¨ training set 1-n ä¸Šè¿›è¡Œï¼Œå¦‚æœæ˜¯ overfitting çš„è¯ï¼Œerrorå†å°ï¼Œä½†æ˜¯åœ¨é²æ£’æ€§è¿˜æ˜¯å¾ˆåƒåœ¾çš„ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸èƒ½ç”¨åœ¨è®­ç»ƒé›†ä¸Šçš„RSSå» compare

### Model Selection for LR

For example, an empirical method like Cross-Validation, Bootstrap methods or sample penalties such as AIC, BIC, Mallow's CP.

[Model Selection: AIC/BIC and Cross-Validation gives different conclusion]

==Cross validation==. å› ä¸ºè¦æ¯”è¾ƒä¸€äº›æ¨¡å‹ï¼Œå¦‚æœæ¯ä¸ªæ¨¡å‹éƒ½æ‹¿ä¸€äº›è¿›è¡Œè®­ç»ƒç„¶åæµ‹éªŒè¯é›†çš„å‡†ç¡®ç‡ã€‚å½“è®­ç»ƒé›†çš„ n éå¸¸å¤§çš„æ—¶å€™ï¼Œå°±å¾ˆå®¹æ˜“ time-consumingã€‚

==K-fold==. å’Œäº¤å‰éªŒè¯æ¯”ï¼Œæ˜¯ computation more efficientï¼Œ ä½†æ›´model is less stable

==AIC==

/BIC

the computational efficiency of AIC/BIC or when the sample size is relatively small for cross-validation
AIC and BIC explicitly penalize the number of parameters, cross-validation not, so again, it's not surprising that they suggest a model with fewer parameters (though nothing prohibits cross-validation from picking a model with fewer parameters).

[Model Selection: AIC/BIC and Cross-Validation gives different conclusion]: https://stats.stackexchange.com/questions/578982/model-selection-aic-bic-and-cross-validation-gives-different-conclusion

## Dimensionality Reductionï¼Œæ•°æ®é™ç»´

æ•°æ®é™ç»´å…¶å®è¿˜æœ‰å¦ä¸€ä¸ªå¥½å¤„ï¼šæ•°æ®å¯è§†åŒ–ã€‚å› ä¸ºè¶…è¿‡ä¸‰ç»´çš„æ•°æ®å°±æ— æ³•å¯è§†åŒ–äº†ã€‚æ•°æ®é™ç»´æœ€å¸¸ç”¨çš„æ–¹æ³•æ˜¯ä¸»æˆåˆ†åˆ†æã€‚

!!! danger "æˆ‘ä»¬æƒ³æ‰¾åˆ°é‡è¦ä¿¡å·çš„ä½ç½®ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ‰¾åˆ°é‡è¦ä¿¡å·æˆ–å¼ºä¿¡å·ï¼Œæˆ–è€…è¿™äº›å¼±ä¿¡å·ç°åœ¨æ˜¯é›¶ï¼Œå°±æ‰”æ‰ï¼Œæˆ‘ä»¬å¸Œæœ›ä¸ºè¿™äº›å¼ºä¿¡å·æä¾›é€‚å½“çš„ä¼°è®¡ã€‚"
    æˆ‘ä»¬è‚¯å®šæ²¡æœ‰ enough informationï¼Œå› ä¸ºåŒæ—¶å­˜åœ¨ç€ noiseã€‚å†³å®šæˆ‘ä»¬æ˜¯å¦èƒ½å®Œæˆç›®æ ‡å°±æ˜¯ï¼šwhether the important signals in data are stronger than noisesã€‚æˆ‘ä»¬ç°åœ¨å‡å®šè¿™ä¸ª important signals are **stronger** than noises. æ¥ä¸‹æ¥å°±è¦æƒ³å¦‚ä½•å°†important information å‰¥ç¦» noiseï¼Ÿ

### Principal Component Analysis, PCA, ä¸»æˆåˆ†åˆ†æ

The basic idea is to transform the p random variables into <u>linear combinations</u> called ==Principal Components==. Extracting linear combinations from multivariate data, a subset of PCs <u>captures most of the variability </u> in the data.

æ­£äº¤å˜æ¢æŠŠç”±çº¿æ€§ç›¸å…³å˜é‡è¡¨ç¤ºçš„è§‚æµ‹æ•°æ®è½¬æ¢ä¸ºå°‘æ•°å‡ ä¸ªç”±çº¿æ€§æ— å…³å˜é‡è¡¨ç¤ºçš„æ•°æ®ï¼Œ**çº¿æ€§æ— å…³**çš„å˜é‡ç§°ä¸º==ä¸»æˆåˆ†==

#### KEY: Maximize the variance

!!! p "Maximize the variance"
    Identify key components which can <u>maximize the information</u> with a reasonable dimension.
    Find **unit-vector g** to transform X into Y with the target that <u>maximizes the variance of Y</u>.

Suppose $X=\begin{bmatrix}X_1\\\vdots\\X_p\end{bmatrix}\in\R^p$ is a random vector with ==Covariance Matrix== $Var\{X\}=\Sigma=\begin{bmatrix}\sigma_{1}^2&\dots&\sigma_{1p}\\\vdots&\ddots&\vdots\\\sigma_{p1}&\dots&\sigma_{p}^2\\\end{bmatrix}$

Look for <u>the linear transformations</u>:

$$
\begin{align*}\begin{cases}Y_1=g_{11}X_1+\dots+g_{1p}X_p\\Y_2=g_{21}X_1+\dots+g_{2p}X_p\\\vdots\\Y_p=g_{p1}X_1+\dots+g_{pp}X_p\end{cases}\iff\begin{bmatrix}Y_1\\\vdots\\Y_p\end{bmatrix}&=\begin{bmatrix}g_{11}&\dots&g_{1p}\\\vdots\\g_{p1}&\dots&g_{pp}\end{bmatrix}\begin{bmatrix}X_1\\\vdots\\X_p\end{bmatrix}\\&=\begin{bmatrix}g_1^T&\dots&g_p^T\\\end{bmatrix}\begin{bmatrix}X_1\\\vdots\\X_p\end{bmatrix}\end{align*}\\\vec{g_i}=\begin{bmatrix}g_{i1}\\\vdots\\g_{ip}\end{bmatrix},\Vert \vec{g_i}\Vert_2=1,\forall i=1,...p
$$

==property of  r.vector== $Y=g^TX, g\in\text{constant}\\\implies Var(Y)=g^TVar(X)g=g^T\Sigma g$

**Target:**

$$g=\max\limits_{\Vert g\Vert_2=1} Var(Y)=\max\limits_{\Vert g\Vert_2=1} g^T\Sigma g\tag{1}$$

==eign about postive definite A==.$\Sigma=\sum\limits_{i=1}^{p}\lambda_i\gamma_i^T\gamma_i=\Gamma^T\Lambda\Gamma$  specially suppose $\lambda_1>\dots>\lambda_p>0$ in <u>anascending order</u>

1. $g_1:=\max\limits_{\Vert g\Vert_2=1} g^T\Sigma g\implies g_1=\gamma_1 \text{ of }\Sigma$
$\implies g_1$ is the **direction** **where the variance is maximized.** <u>ï¼ˆnot PC</u>
$\implies g_1^Tx$ is the **1st PC**
2. $g_2:=\max\limits_{\Vert g\Vert_2=1,\:g_2^Tg_1=0} g^T\Sigma g\implies g_2=\gamma_2 \text{ of }\Sigma$
$\implies g_2$ is the one that maximizes the variance among all directions **orthogonal** to $g_1$
3. proof: **i th-PC = i th eigen vector**
Let $\lambda_i:=$ the i-th largest eigen-value of $\Sigma$, $\gamma_i:=$ the eigen-vector corresponding to $\lambda_i$. Therefore, $\{\gamma_1,\gamma_n\}$ are one set of the basis of $\R^p$ <u>ç‰¹å¾å‘é‡æ˜¯ç‰¹å¾ç©ºé—´çš„ä¸€ç»„ basic vectorsã€‚</u>
$\implies\forall g\in\R^p,\exist c_1, c_2,\dots, c_p\text{ s.t. } g=c_1\gamma_1+c_2\gamma_2+c_n\gamma_n$

$$\begin{align*}\implies g^T\Sigma g&=\sum\limits_{i=1}^n\sum\limits_{j=1}^nc_ic_j\gamma_i^T\Sigma\gamma_j\\
&\xlongequal{\Sigma\gamma_i=\lambda_i\gamma_i}\sum\limits_{i=1}^n\sum\limits_{j=1}^pc_ic_j\gamma_i^T\gamma_j\lambda_j\\
&\xlongequal[\lambda_i^T\lambda_i=1]{\lambda_i^T\lambda_j=0,\forall i\neq j} \sum_{i=1}^nc_i^2\lambda_i\\
&\le\lambda_1\sum_{i=1}^nc_i^2\\
&\xlongequal[\Vert g\Vert_2^2=1=c_1^2+\dots+c_n^2]{\text{when }c_2=\dots=c_n=0 }\lambda_1
\end{align*}$$

$$\implies g=\gamma_1\iff g^T\Sigma g=\lambda_1$$

**Conclusion:**

1. the best direction is **the direction of eigenvectors**
2. the **variance** of the direction is **Eigen value**
$Var(Y_i)=g_i^T\Sigma g_i=g_i^T\lambda_i g_i = \lambda_i\\tr(\Sigma)=tr(\Gamma\Lambda\Gamma^T)=tr(\Lambda\Gamma\Gamma^T)=tr(\Lambda)=\sum\limits_{i=1}^p\lambda_i$
3. What is the relationship between $Y_i \& Y_j, i\neq j$
**orthogonal and are the Eigen-vector of Sigma**
4. What if X is following a multivariate normal distribution?
==Multivariate Normal Distribution== $Zï½N(b,\Sigma),\forall l\in\R^p,l^TZ\in\Rï½$ Normal distri... $\implies Z$ ï½ Multivariate Normal Distri...
If X is following a multivariate normal distribution, $g_i^TXï½N$

**Advantages:**

1. Identify **key components** which can **maximize the information** with a reasonable dimension. å‘ç°æ•°æ®ä¸­çš„åŸºæœ¬ç»“æ„ï¼Œå³æ•°æ®ä¸­å˜é‡ä¹‹é—´çš„å…³ç³»,èƒ½è¿‘ä¼¼åœ°è¡¨è¾¾
2. **Reduce the dimension** of other forms of analysis.
3. Linearity is assumed.

**Limits:**

1. It can be **more difficult to interpret** than using a subset of the original variables.
2. It uses only covariances/correlations but not higher-order moments. This can be extended to ==independent component analysis ICA==

#### PCA Transformation

$Y_i = g_i^TX\implies Y=\Gamma^TX$

the 1 PC $Y_1=g_1^TX, g_1:= $ the eigen-vector with the 1 largest eigen-value $\lambda_1$
the 2 PC $Y_2=g_2^TX, g_2:=$ the eigen-vector with the 2 largest Eigen-value $\lambda_2$
$$\vdots\\
\begin{bmatrix}Y_1\\\vdots\\Y_p\end{bmatrix}=\begin{bmatrix}g_1^T\\\vdots\\g_p^T\end{bmatrix}_{p\times p}\begin{bmatrix}X_1\\\vdots\\X_p\end{bmatrix}\iff \Gamma=\begin{bmatrix}g_1&\dots&g_p\end{bmatrix}\in\R^{p\times p}$$

##### Practical Use

1. <u>**Standardize è§„èŒƒåŒ–**</u> the data, ä½¿å¾—æ•°æ®æ¯ä¸€å˜é‡çš„å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1
2. <u>**SVD æ­£äº¤åˆ†è§£**</u> of the sample **covariance M/correlation M**
3. **Sort** the eigenvalues in descending order and choose the K largest eigenvectors (plots, the proportion of variances interpreted etc.)
4. <u>**Linear Transform å˜æ¢**</u> X into Y(Dimension Reduction!)

åŸæ¥ç”±çº¿æ€§ç›¸å…³å˜é‡è¡¨ç¤ºçš„æ•°æ®ï¼Œé€šè¿‡æ­£äº¤å˜æ¢å˜æˆç”±è‹¥å¹²ä¸ªçº¿æ€§æ— å…³çš„æ–°å˜é‡è¡¨ç¤ºçš„æ•°æ®ã€‚æ–°å˜é‡æ˜¯å¯èƒ½çš„æ­£äº¤å˜æ¢ä¸­å˜é‡çš„æ–¹å·®çš„å’Œï¼ˆä¿¡æ¯ä¿å­˜ï¼‰æœ€å¤§çš„ï¼Œ**æ–¹å·®è¡¨ç¤ºåœ¨æ–°å˜é‡ä¸Šä¿¡æ¯çš„å¤§å°**ã€‚å°†æ–°å˜é‡ä¾æ¬¡ç§°ä¸ºç¬¬ä¸€ä¸»æˆåˆ†ã€ç¬¬äºŒä¸»æˆåˆ†ç­‰ã€‚

!!! p "**how large dimensions we keepï¼š**"
    è¿™é‡Œæ‰€è¯´çš„**ä¿¡æ¯æ˜¯æŒ‡åŸæœ‰å˜é‡çš„æ–¹å·®**ã€‚
    å¯ä»¥è®¾ç½®ä¸€ä¸ª certain level, maybe 85%ï¼Œ90%ï¼Œç„¶åæˆ‘ä»¬çœ‹ <u>the cumulative proportion of k PCs</u>, å› ä¸ºè¿™å°±æ˜¯è¯´ è¿™k ä¸ª PCs èƒ½å¤Ÿåœ¨å¤šå¤§ç¨‹åº¦ä¸Šæè¿°è¿™äº›ç‚¹çš„å·®å¼‚é—´éš”ã€‚è€Œåœ¨å‰©ä¸‹çš„PCsé‡Œï¼Œè¿™äº›ç‚¹å¤§å¤šæ˜¯é‡å ï¼Œå¹¶ä¸å…·æœ‰å¾ˆé«˜çš„å·®å¼‚ä¿¡æ¯ä»·å€¼ã€‚åªè¦è¿™ä¸ª cumulativeè¾¾åˆ°äº†è¿™ä¸ª certain level æˆ‘ä»¬å°±é‡‡ç”¨å‰ k ä¸ª PCs

!!! p "covariance M vs correlation M? â€”â€” <kbd>Scale</kbd>"
    ==covariance M==. $Cov(x,y)=\cfrac{1}{n}\sum\limits_{i=1}^n(x-\overline{x})(y-\overline{y})$
    ==correlation M==. $\rho_{xy}=\cfrac{Cov(x,y)}{\sqrt{Var(x)}\sqrt{Var(y)}}$
    [Covariance Vs Correlation: Here are the Difference You Should Know ,Simplilearn]
    å½“æˆ‘ä»¬è¦å»é™¤**ç‰¹å¾å€¼é‡çº²çš„åŒºåˆ«** æˆ‘ä»¬ä½¿ç”¨ correlationï¼Œ<kbd>scale=true</kbd>
    å¦‚æœä¸å»ï¼Œå°±æ˜¯covarianceï¼Œ<kbd>scale=false</kbd>ï¼ˆé»˜è®¤
    > > Suppose $x_1$ is a length measured either in cm. or mm., $x_2$ is a weight measurement in gm. The covariance M with $y_1$ in cm. is $\begin{bmatrix}80&44\\44&80\end{bmatrix}$ the Covariance M with $y_1$ in mm. is $\begin{bmatrix}8000&440\\440&80\end{bmatrix}$

[Covariance Vs Correlation: Here are the Difference You Should Know ,Simplilearn]:https://www.simplilearn.com/covariance-vs-correlation-article

#### Graphical: Rotate the data without scaling

æ•°æ®é›†åˆä¸­çš„æ ·æœ¬ç”±å®æ•°ç©ºé—´ï¼ˆæ­£äº¤åæ ‡ç³»ï¼‰ä¸­çš„ç‚¹è¡¨ç¤ºï¼Œç©ºé—´çš„ä¸€ä¸ªåæ ‡è½´è¡¨ç¤ºä¸€ä¸ªå˜é‡ï¼Œè§„èŒƒåŒ–å¤„ç†åå¾—åˆ°çš„æ•°æ®**åˆ†å¸ƒåœ¨åŸç‚¹é™„è¿‘**ã€‚å¯¹åŸåæ ‡ç³»ä¸­çš„æ•°æ®è¿›è¡Œä¸»æˆåˆ†åˆ†æç­‰ä»·äºè¿›è¡Œ**åæ ‡ç³»æ—‹è½¬å˜æ¢ï¼Œå°†æ•°æ®æŠ•å½±åˆ°æ–°åæ ‡ç³»çš„åæ ‡è½´ä¸Šã€‚**

!!! p "**Graphical:** $Y=\Gamma^TX$: Multiplication by an orthogonal matrix: **Rotation**!"
    <u>proof of rotation</u>: $\Vert y_1-y_2\Vert_2=\Vert x_1-x_2\Vert_2, $ without scaling $\forall x_1,x_2,$ any two points in space

    $$\begin{align*}\Vert y_1-y_2\Vert_2^2&=(y_1-y_2)^T(y_1-y_2)\\&=(x_1-x_2)^T\Gamma\Gamma^T(x_1-x_2)\\&\xlongequal[\Gamma\Gamma^T=I]{\Gamma^T=\Gamma^{-1}}(x_1-x_2)^T(x_1-x_2)=\Vert x_1-x_2\Vert_2^2\end{align*}$$

æ–°åæ ‡ç³»çš„ç¬¬ä¸€åæ ‡è½´ã€ç¬¬äºŒåæ ‡è½´ç­‰åˆ†åˆ«è¡¨ç¤ºç¬¬ä¸€ä¸»æˆåˆ†ã€ç¬¬äºŒä¸»æˆåˆ†ç­‰ï¼Œæ•°æ®åœ¨æ¯ä¸€è½´ä¸Šçš„åæ ‡å€¼çš„å¹³æ–¹è¡¨ç¤ºç›¸åº”å˜é‡çš„æ–¹å·®ï¼›å¹¶ä¸”ï¼Œè¿™ä¸ªåæ ‡ç³»æ˜¯åœ¨æ‰€æœ‰å¯èƒ½çš„æ–°çš„åæ ‡ç³»ä¸­ï¼Œ**åæ ‡è½´ä¸Šçš„æ–¹å·®çš„å’Œæœ€å¤§çš„**

**æ–¹å·®å’Œæœ€å¤§:**
ä¸»æˆåˆ†åˆ†ææ—¨åœ¨é€‰å–æ­£äº¤å˜æ¢ä¸­æ–¹å·®æœ€å¤§çš„å˜é‡ï¼Œä½œä¸ºç¬¬ä¸€ä¸»æˆåˆ†ï¼Œæ—‹è½¬å˜æ¢ä¸­**åæ ‡å€¼çš„å¹³æ–¹å’Œæœ€å¤§**çš„è½´, æ—‹è½¬å˜æ¢ä¸­é€‰å–**ç¦»æ ·æœ¬ç‚¹çš„è·ç¦»å¹³æ–¹å’Œæœ€å°**çš„è½´

<div class="grid" markdown>
<figure markdown="span">![](./pics/PCA_1.png){width=45%}<p>transformationï¼š<u>æ—‹è½¬å˜æ¢</u></p><figure>

<p>åŸåæ ‡ç³»:å¾ˆæ˜æ˜¾æœ‰æ­£å‘çº¿æ€§ç›¸å…³ï¼Œ13è±¡é™æœ€å¤š<br>æ–°åæ ‡ç³»:çº¿æ€§æ— å…³ï¼Œ1234è±¡é™éƒ½å·®ä¸å¤š</p>
</div>

å¦‚æœä¸»æˆåˆ†åˆ†æåªå–ç¬¬ä¸€ä¸»æˆåˆ†ï¼Œå³æ–°åæ ‡ç³»çš„y1è½´ï¼Œé‚£ä¹ˆç­‰ä»·äºå°†æ•°æ®æŠ•å½±åœ¨æ¤­åœ†é•¿è½´ä¸Šï¼Œç”¨è¿™ä¸ªä¸»è½´è¡¨ç¤ºæ•°æ®ï¼Œå°†äºŒç»´ç©ºé—´çš„æ•°æ®å‹ç¼©åˆ°ä¸€ç»´ç©ºé—´ä¸­ã€‚

| 1st PC | 2nd PC |
| --- | --- |
| æ–¹å·®æœ€å¤§ | ä¸ç¬¬ä¸€åæ ‡è½´æ­£äº¤ï¼Œä¸”æ–¹å·®æ¬¡ä¹‹ |
| ç¬¬ä¸€åæ ‡è½´ $y_1$ | ç¬¬äºŒåæ ‡è½´ $y_2$ |
| æ¤­åœ†çš„é•¿è½´ | æ¤­åœ†çš„çŸ­è½´ |

### Linear Discriminant Analysis, LDA, çº¿æ€§åˆ¤åˆ«åˆ†æ

LDAçš„ç›®æ ‡æ˜¯**æå–ä¸€ä¸ªæ–°çš„åæ ‡ç³»ï¼Œå°†åŸå§‹æ•°æ®é›†æŠ•å½±åˆ°ä¸€ä¸ªä½ç»´ç©ºé—´ä¸­ã€‚**
å’ŒPCAçš„ä¸»è¦åŒºåˆ«åœ¨äºï¼ŒLDAä¸ä¼šä¸“æ³¨äºæ•°æ®çš„æ–¹å·®ï¼Œè€Œæ˜¯ä¼˜åŒ–ä½ç»´ç©ºé—´ï¼Œ**ä»¥è·å¾—æœ€ä½³çš„ç±»åˆ«å¯åˆ†æ€§**ã€‚æ„æ€æ˜¯ï¼Œæ–°çš„åæ ‡ç³»åœ¨ä¸ºåˆ†ç±»æ¨¡å‹æŸ¥æ‰¾**å†³ç­–è¾¹ç•Œ**æ—¶æ›´æœ‰ç”¨ï¼Œ<u>éå¸¸é€‚åˆç”¨äºæ„å»ºåˆ†ç±»æµæ°´çº¿</u>ã€‚

**ä¼˜ç‚¹ï¼š**åŸºäºç±»åˆ«å¯åˆ†æ€§çš„åˆ†ç±»
- æœ‰åŠ©äºé¿å…æœºå™¨å­¦ä¹ æµæ°´çº¿çš„è¿‡æ‹Ÿåˆï¼Œä¹Ÿå«é˜²æ­¢ç»´åº¦è¯…å’’ã€‚
- LDAä¹Ÿä¼šé™ä½è®¡ç®—æˆæœ¬ã€‚

!!! danger "Fisherâ€™s LDA and Bayesâ€™ LDA are essentially different! They are equivalent under the <u>Gaussian assumption with a common Î£ for the two-class case</u>"

Both Fisherâ€™s LDA and Bayes rule reduce to:

|  | empirical estimators |
| --- | --- |
| $Î¼_X$ | $\overline{X}$ |
| $Î¼_Y$ | $\overline{Y}$ |
| $Î£$ | $Î£$ |

!!! danger "ä¸å¯å°†çº¿æ€§åˆ¤åˆ«åˆ†æä¸==éšç‹„åˆ©å…‹é›·åˆ†é…LatentDirichlet Allocation, LDA== ç›¸æ··æ·†ã€‚"
    éšç‹„åˆ©å…‹é›·åˆ†é…ç”¨äºæ–‡æœ¬å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œä¸çº¿æ€§åˆ¤åˆ«åˆ†ææ²¡æœ‰å…³ç³»ã€‚

| ç±»å†… within-class |  ç±»é—´ between-class
| --- | --- |
 S_w | S_b

#### Fisherâ€™s LDA

LDAçš„ç›®æ ‡æ˜¯æå–ä¸€ä¸ªæ–°çš„åæ ‡ç³»ï¼Œå°†åŸå§‹æ•°æ®é›†æŠ•å½±åˆ°ä¸€ä¸ªä½ç»´ç©ºé—´ä¸­ï¼Œä»¥è·å¾—æœ€ä½³çš„ç±»åˆ«å¯åˆ†æ€§ã€‚

**Targetï¼š** è·å¾—æœ€ä½³çš„ç±»åˆ«å¯åˆ†æ€§
Find the lineÂ $P_ZÂ =Â w^TZ$Â that best separates the two classes.
$$w =\max\limits_w \cfrac{w^TS_Bw}{w^TS_Ww}$$
Force $\begin{cases}S_B=(\mu_X-\mu_Y)(\mu_X-\mu_Y)^T&\text{ between-class}\uparrow\\S_W=\Sigma&\text{ within-class}\downarrow\end{cases}$

1. the center of the two after transformation linear projection be as **far** away as possible $S_B\uparrow$
2. the variance of two classes to be as **small** as possible  $S_w \downarrow$

<figure markdown="span">![](./pics/LDA_1.png){width=60%}<p>LDA:æœ€ä½³çš„ç±»åˆ«å¯åˆ†æ€§<br>å‡è®¾ï¼šæ­£æ€åˆ†å¸ƒ</p></figure>

[æ©Ÿå™¨å­¸ç¿’: é™ç¶­(Dimension Reduction)- ç·šæ€§å€åˆ¥åˆ†æ( Linear Discriminant Analysis)]

#### Bayesâ€™ LDA

!!! p "è´å¶æ–¯çš„ä¼˜ç‚¹ï¼šä¸éœ€è¦çŸ¥é“å…·ä½“çš„åˆ†å¸ƒ"

$f_X (Â·):=$ pdf  for Class-X, $f_Y (Â·) := $ pdf  for Class-Y

==Bayes rule==. $Î´(Z) = I\{Ï€_1f_X(Z) > Ï€_2f_Y(Z)\}$.
$\begin{cases}\pi_i \text{ prior possibility },\pi_1+\pi_2=1 \\f_i(Z)\text{ likelihoood function}\end{cases}$

For simplicity letâ€™s assume that $Ï€_1 = Ï€_2 = 1/2.$ without any assumption.

$Î´(Z) =I \{[Zâˆ’(Î¼_X+\mu_Y)/2]^T Î£^{âˆ’1}(Î¼_Xâˆ’\mu_Y)>0\}\\\hat{Î´}(Z) =I \{[Zâˆ’(\overline{X}+\overline{Y} )/2]^T \hat{Î£}^{âˆ’1}(\overline{X} âˆ’ \overline{Y})>0\}$
Z will be assigned as $ \begin{cases}X&Î´(Z) =1\\Y&Î´(Z) =0\end{cases}$

### Quadratic Discriminant Analysis, QDA

Assume:Â $Y_1âˆ¼N(Î¼_1,Î£_1), Y_2âˆ¼N(Î¼_2,Î£_2)$ The two classes have **different** covariance matrices!

$\delta(x) = (xâˆ’Î¼)^TÎ©(xâˆ’Î¼)+Î´^T(xâˆ’ Î¼)+Î·\\\begin{cases}Î¼=(Î¼_1+ Î¼_2)/2\space\text{ (mean)}\\\Omega=\Sigma_2^{-1}-\Sigma^{-1}\space\text{ the difference of the two precision matrices}\\\delta=(\Sigma_1^{-1}+\Sigma_2^{-1})(\mu_1-\mu_2)\\Î·=2\log(\pi_1/\pi_2)+\frac{1}{4}(\mu_1-\mu_2)^T\Omega(\mu_1-\mu_2)+\log|\Sigma_2|-\log|\Sigma_1|\end{cases}$

å› ä¸ºæ˜¯å…³äºxçš„äºŒæ¬¡å‡½æ•°ï¼Œæ‰€ä»¥æ˜¯ quadratic äºŒæ¬¡

[](https://towardsdatascience.com/linear-discriminant-analysis-explained-f88be6c1e00b)

> > (T1 in Chap1.1 in AMA565) Suppose the covariance M of a p-dimensional random vectorÂ XÂ isÂ $Î£ = \text{diag}\{1, 2, . . . , p\}.$ What are the Principal Components ofÂ X?
> > (T2 in Chap1.1 in AMA565) *Suppose the covariance matrix of a p-dimensional random vectorÂ XÂ isÂ $Î£ =Â {11}^T$. What are the Principal Components of X?
> > (T3 in Chap1.1 in AMA565) *Suppose $X âˆ¼ N(Î¼_1,Î£),Y âˆ¼ N(Î¼_2,Î£).\:f_1(x):=\text{ pdf of }X, f_2(x):= \text{ pdf of }Y$, and let Ï€_i be the prior probability that X is coming from class i, i = 1, 2. Show that $Ï€_1f_1(x)/[Ï€_2f_2(x)] > 1\iff (Î£^{âˆ’1}(Î¼_1 âˆ’ Î¼_2))T (x âˆ’ (Î¼_1 + (Î¼_2)/2)) > c$ Derive c.*

[æ©Ÿå™¨å­¸ç¿’: é™ç¶­(Dimension Reduction)- ç·šæ€§å€åˆ¥åˆ†æ( Linear Discriminant Analysis)]: https://chih-sheng-huang821.medium.com/æ©Ÿå™¨å­¸ç¿’-é™ç¶­-dimension-reduction-ç·šæ€§å€åˆ¥åˆ†æ-linear-discriminant-analysis-d4c40c4cf937
