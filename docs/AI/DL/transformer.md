# Transformer

## BERT related

**BERT**， **B**idirectional **E**ncoder **R**epresentation from **T**ransformer

### original

!!! summary ""
    **Transformer** SA +
    **Bidirectional** 完形填空 MLM
    |BERT|v.s.|
    |--|--|
    |MLM，|gpt，L2R
    |Transformer|RNN

- advangtages

> BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.

[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]()
