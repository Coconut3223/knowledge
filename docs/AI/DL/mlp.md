# MLP

- ==Deep Non-parametric Regression==ã€‚ä¹‹å‰éƒ½æ˜¯ function-assumed å‡ºæ¥çš„ç»“æœæ˜¯closed-formedï¼Œ ä½†æ˜¯ç°åœ¨å°±æ˜¯ **Nonparametric regression** ä¸åœ¨ä¹å®ƒæ˜¯ä»€ä¹ˆ the shape ofÂ the functional relationships is not predetermined ï¼Œå°±æ˜¯ç”¨ deep learning å» approximate å®ƒã€‚

    - ä¸ºä»€ä¹ˆèƒ½ **approximateï¼Ÿ**
    Non-linearity is introduced by activation functions.
    AnyÂ **continuous functions**Â defined on **a compact set** can be approximatedÂ **arbitrarily** wellÂ by aÂ shallow neural networkÂ if **the shallow neural network isÂ arbitrarily wide. å¦‚æœæµ…ç¥ç»ç½‘ç»œæ˜¯ä»»æ„å®½çš„ï¼Œé‚£ä¹ˆåœ¨ç´§é›†ä¸Šå®šä¹‰çš„ä»»ä½•è¿ç»­å‡½æ•°éƒ½å¯ä»¥è¢«æµ…ç¥ç»ç½‘ç»œä»»æ„è¿‘ä¼¼ã€‚(æœ‰ç†è®ºè¯æ˜)**
    ç¥ç»ç½‘ç»œå› ä¸ºæ¶‰åŠåˆ°æ›´æ·±å±‚å°±æ˜¯2*3ä¸ªå‚æ•°ï¼Œä½†æ˜¯ä¸€èˆ¬å°±æ˜¯2+3ä¸ªå‚æ•°ï¼Œæ‰€ä»¥ç¥ç»ç½‘ç»œä¼šæ›´å¼ºï¼Œåœ¨ä¸€äº›ç¡®å®å¤æ‚çš„æ•°æ®åˆ†å¸ƒä¸Šã€‚

## architecture

**Feedforward Neural Networks** FNN å‰å‘å›é¦ˆ **Layers in MLP are fully connected**

### Perceptron, æ„ŸçŸ¥æœº

<div class="grid" markdown>
<p><mark>Perceptron</mark>, æ„ŸçŸ¥æœºæ˜¯æ¥å—å¤šä¸ªè¾“å…¥åå°†æ¯ä¸ªå€¼ä¸å„è‡ªçš„æƒé‡<b>ç›¸ä¹˜ï¼Œæœ€åè¾“å‡ºæ€»å’Œ</b>çš„æ¨¡å‹</p>
<figure markdown="span">![](./pics/MLP_1.png){width=60%}</figure>
</div>

### Mathematical Definition

The architecture of an MLP is expressed as **aÂ composition of a series of functions**

$$
\begin{align*}f_\theta(x)=&\mathcal{A_L\circ\sigma \circ A_{L-1}\circ\sigma \circ\cdots\circ \sigma \circ A_1(x) }\\&\mathcal{A_i}(x)=W_ix+b_i\\&\theta=(W_1,b_1,...\dots,W_L,b_L)\end{align*}
$$

![](./pics/MLP_2.png){width=60%}

|  | Neural Network |
| --- | --- |
| Input x | $\large x\in\R^{d_0}$ at 0-th layer |
| Output | at L-th layer |
| layers | L |
| hidden layer | $h_i(x)=\mathcal{A_i \circ A_{i-1}\circ\sigma \circ\cdots\circ \sigma \circ A_1(x) },i=1...L-1$ |
| $\theta$ | $(W_1,b_1,...\dots,W_L,b_L)$ |
| the depth of network | L-1 (number of hidden layers. |
| theÂ width of  network | $\max\{d_1 , ... , d_{L-1}\}.$ |
| theÂ size of  network | $\text{num}( \theta)$ |
|  | the i-th Layer |
| linear transformation | $\mathcal{A_i}(x)=W_ix+b_i$ |
| Input x | $\large x\in\R^{d_{i-1}}$ |
| weigth matrix | $ W_i\in\large \R^{d_i\times d_{i-1}}$ |
| bias vector | $b_i\in\large\R^{d_i}$ |
| activation function |  $\sigma$ |
| TheÂ widthÂ of the i-th layer | $d_i$ |

## Back Propagation

Loop over instances:

1. The forward steps
Given the input, make predictions layer-by-layer, starting from the first layer)
2. The backward steps
Calculate **the error in the output**
Update the weights layer-by-layer, starting from the final layer

> > The neuron is modelled by a unit connected by weighted links $w_i$ to other units ğ‘–. SupposeÂ $W=(w_0,w_1,w_2,w_3)=(-0.4336,0.8622,0.3188,-1.0377),\: z(w)=w_0+\sum\limits_{i=1}^nw_ix_i,\:o(w)=\text{sigmoid}(z(w)),\: L(w)=\cfrac{1}{2}(o(w)-y)^2.$
> > Now we have a sample (x,y) with $x = (x_1,x_2,x_3)^T = (0.5377,1.8339,-2.2588)^T, y = 0$.
> > ![](./pics/MLP_3.png)

## in classification problem

$\begin{cases}\text{Data: }(X_i,Y_i),i=1,\dots,n,X_i\in\R^p,Y_i \text{ is categorical }\\\text{classifier: }\mathcal F=\{f:f(\cdot)\text{ is categorical }\}\end{cases}$

**éš¾ç‚¹ï¼š**

1. åˆ†ç±»æ˜¯labelç±»å‹ï¼Œè¾“å‡ºçš„æ˜¯labelï¼Œ è€Œ output of NN is a continuous value
2. æ€ä¹ˆåº”ç”¨ multi-classificationï¼Œ how to represent Y   $\rightarrow$ Matrix

**How to represent Y and** $\hat{Y}$

ä»ä¸€ç»´çš„1ï¼Œ2ï¼Œ3ï¼Œ4ï¼Œâ€¦ï¼Œåˆ°äºŒç»´çš„ 0-1 bool çŸ©é˜µ

- Example
    1. \#category = 2: $Y_i\in\{+1,-1\}, Y_i\begin{cases}+1\rightarrow (1,0)\\-1\rightarrow (0,1)\end{cases}$
    2. \#category = 4: $Y_i\in\{1,2,3,4\}, Y_i\begin{cases}1\rightarrow (1,0,0,0)\\2\rightarrow (0,1,0,0)\\3\rightarrow (0,0,1,0)\\4\rightarrow (0,0,0,1)\end{cases}$

ä¸€èˆ¬æœ€åè¾“å‡ºå±‚ï¼Œç¥ç»å…ƒçš„æ•°é‡å°±åˆšå¥½æ˜¯ category çš„æ•°é‡ï¼Œå¹¶ä¸”æ˜¯æ¯ä¸€ä¸ªç¥ç»å…ƒ model class probabilities. SO

1. $z_i=h(X_i,\theta)\in[0,1]$ $\rightarrow$ h = Sigmoid
2. $\sum\limits_{i=1}^{\#category}\hat{y}_i=\sum\limits_{i=1}^{\#category}g(z_i)=1\rightarrow$  g = Softmax
