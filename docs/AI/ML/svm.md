# SVM

SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.

!!! p "find a best hyperplane with the largest margin to separate different class samples."
    ```mermaid
    graph  LR
    A[Linear SVM]
    B[soft margined ]
    C[kernel trick<br>ä½ç»´å®Œæˆè¿ç®—]
    D[non-linear SVM<br>æŠ•å°„é«˜ç»´]
    E{çº¿æ€§å¯åˆ†}
    F{å™ªå£°å­˜åœ¨<br>æ³›åŒ–â¬‡ï¸}
    G[hard margined]
    Z[SVM<br>çº¿æ€§è¶…å¹³é¢<br>å™ªå£°æ•æ„Ÿ] --> E --Y-->A
    E --N--> D --è®¡ç®—å†…ç§¯å›°éš¾--> C
    Z --> F --N--> G
    F --Y--> B
    ```

## ç†è®º

==æœ€å¤§é—´éš”è¶…å¹³é¢, Maximal Margin Hyperplan, Optimal Hyperplane==ï¼Œåœ¨æ ·æœ¬ç©ºé—´æ‰¾ä¸€ä¸ªè¶…å¹³é¢ï¼Œå°†ä¸åŒç±»åˆ«çš„æ ·æœ¬åˆ†å¼€ã€‚
==å†³ç­–è¾¹ç•Œé—´éš”, margin, $\gamma$==ï¼Œ$B_{-1}$ å’Œ $B_{1}$ä¹‹é—´çš„é—´éš” $=$ ä¸¤ä¸ªå¼‚ç±»å†³ç­–å‘é‡åˆ°å†³ç­–è¾¹ç•Œçš„è·ç¦»ä¹‹å’Œã€‚å…·æœ‰è¾ƒå¤§é—´éš”çš„å†³ç­–ä¾¿æ·å…·æœ‰**æ›´å¤§çš„æ³›åŒ–è¯¯å·®**ã€‚
==æ”¯æŒå‘é‡, support vector==ï¼Œè·ç¦»å†³ç­–è¾¹ç•Œæœ€è¿‘çš„è®­ç»ƒæ ·æœ¬ $(x_{-1},-1),(x_1, 1)$ åˆ†åˆ«ä½äºè¶…å¹³é¢$B_{-1}$ å’Œ $B_{1}$ä¸Š,ä»¤ç­‰å¼æˆç«‹ã€‚$\begin{cases}
B_{-1}:& w^Tx_{-1}+b=-1\\B_1:&w^Tx_{1}+b=1
\end{cases}$

![](./pics/SVM_4.png){width=60%}

**SVM å¯¹å™ªå£°æ•æ„Ÿçš„åŸå› **
SVM çš„ç›®çš„ï¼šæ±‚å‡ºä¸æ”¯æŒå‘é‡æœ‰æœ€å¤§åŒ–è·ç¦»çš„ç›´çº¿ï¼Œä»¥æ¯ä¸ªæ ·æœ¬ä¸ºåœ†å¿ƒï¼Œè¯¥è·ç¦»ä¸ºåŠå¾„ä½œåœ†ï¼Œå¯ä»¥è¿‘ä¼¼ä¸ºåœ†å†…çš„ç‚¹ä¸è¯¥æ ·æœ¬å±äºç›¸åŒåˆ†ç±»ã€‚å¦‚æœå‡ºç°äº†å™ªå£°ï¼Œé‚£ä¹ˆè¿™ä¸ªå™ªå£°æ‰€å¸¦æ¥çš„é”™è¯¯åˆ†ç±»å°†æœ€å¤§åŒ–ã€‚

## linear SVM

Assumptionï¼šæ ·æœ¬çº¿æ€§å¯åˆ†ï¼Œæ ¹æ®**ç»“æ„é£é™©æœ€å°åŒ–åŸç†**ï¼Œæœ€å¤§åŒ–å†³ç­–è¾¹ç•Œçš„é—´éš” $\Leftrightarrow$ æœ€åæƒ…å†µä¸‹æ³›åŒ–è¯¯å·®æœ€å°ã€‚

!!! p "å¯¹å¶é—®é¢˜"
    $\min f\xRightarrow{è½¬åŒ–}\max \log f$
    - **ä¼˜ç‚¹**
      - æ›´å®¹æ˜“æ±‚è§£
      - æ±‚è§£å‡ºç°äº†å‘é‡å†…ç§¯çš„å½¢å¼ï¼Œæ›´å®¹æ˜“å¼•å‡ºæ ¸å‡½æ•°çš„æ¦‚å¿µã€‚

### æœ€å¤§è¾¹ç¼˜æ¨å¯¼

$$\begin{align*}
B_{1}:&w^Tx_{1}+b=-1\\B_2:&w^Tx_2+b=1\end{align*}\implies
w^T(x_2-x_1)=2=w^T\overrightarrow{x_2x_1}\\
w^T\overrightarrow{x_2x_1}=\Vert w^T\Vert*\Vert\overrightarrow{x_2x_1}\Vert*\cos<w^T, \overrightarrow{x_2x_1}>=\Vert w^T\Vert * \Vert\gamma\Vert=2\\
\implies \Vert\gamma\Vert=\cfrac{2}{\Vert w\Vert} $$

$$\text{Question:} \max \gamma = \max_{w^T,b}\cfrac{2}{\Vert w^T\Vert}, \text{s.t.}\begin{cases} w^Tx_i+b\gt1&y_i=+1\\w^Tx_i+b\le-1&y_i=-1
\end{cases}\\
\Leftrightarrow \min_{w^T,b}\cfrac{1}{2}\Vert w^T\Vert, \text{ s.t. }\space y_i(w^Tx_i+b)\ge 1,  \forall i
$$

$(x_i,y_i)$ is a sample, which y is its actual type, $w^Tx+b$ is its prediction.

ä¸ºäº†<u>åœ¨æ±‚å¯¼çš„æ—¶å€™æ–¹ä¾¿çº¦è°ƒç³»æ•° + ä½¿ç”¨ KKT æ¡ä»¶</u>ï¼Œ$\min_{w^T,b}\cfrac{1}{2}{w^T}^{\red{2}}, \text{ s.t. }\space \red{-}(y_i(w^Tx_i+b)-1)\red{\le} 0,  \forall i$

!!! danger "**å¸Œæœ›**ï¼šæ‰¾åˆ°ä¸€ä¸ª $f(x):=w^Tx+b$ ä½¿å¾—æ‰€æœ‰å±äºç±»åˆ« 1 çš„ç®—å‡ºæ¥æ˜¯ $\ge 1$çš„ï¼Œæ‰€æœ‰å±äºç±»åˆ« -1 çš„ç®—å‡ºæ¥æ˜¯ $\le -1$çš„ã€‚ä½†æ˜¯ä¸æ»¡è¶³äºæ­¤ï¼Œè¿˜å¸Œæœ›æ‰¾åˆ°åœ¨æ‰€æœ‰ç¬¦åˆä»¥ä¸Šæ¡ä»¶çš„ $f$ä¸­ï¼Œ$\Vert w\Vert$ æœ€å°ã€‚"

!!! p "æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•ï¼šå°†æ¡ä»¶èå…¥åˆ°ç›®æ ‡å‡½æ•°å½“ä¸­"
    $$\min_{x} f(x) \space\text{ s.t} \begin{cases} h(x)=0\\g(x)\red{\le}0
    \end{cases}\Leftrightarrow \min_{w^T,x}f+\lambda h + \mu g=:\mathcal{L}(x, \lambda, \mu)$$
    To solve, $\cfrac{\partial\mathcal{L}}{\partial x}=\cfrac{\partial\mathcal{L}}{\partial \lambda}=\cfrac{\partial\mathcal{L}}{\partial \mu}\xlongequal{SET}0$
    To achive optimal value, $\begin{cases} \frac{\partial\mathcal{L}}{\partial x}=0\\h(x)=0\\\mu g(x)=0
    \end{cases}$

$$\mathcal{L}(w^T,b, \alpha):= \cfrac{1}{2} {w^T}^2-\sum_{i=1}^n\alpha_i(y_i(w^Tx_i+b)-1)\\\space\\\min_{w^T,b}\max_{\alpha}\mathcal{L}(w^T,b, \alpha)$$

æ ¹æ®æ‹‰æ ¼æœ—æ—¥çš„å¯¹å¶æ€§ï¼Œ

$$\max_{\alpha}\min_{w^T, b}\mathcal{L}(w^T,b, \alpha)$$
æ±‚ $\cfrac{\partial\mathcal{L}}{\partial w^T}=\cfrac{\partial\mathcal{L}}{\partial b}\xlongequal{SET}0$

$$\begin{align*}\cfrac{\partial\mathcal{L}}{\partial w^T}&=w^T-\sum_{i=0}^n\alpha_iy_ix_i&=0\\
\cfrac{\partial\mathcal{L}}{\partial b}&=-\sum_{i=0}^n\alpha_iy_i&=0
\end{align*}$$

å¾—åˆ° ${w^T}^*=\sum\limits_{i=0}^n\alpha_iy_ix_i$ å›ä»£ $\mathcal{L}(w^T,b, \alpha)$, æ¶ˆæ‰ $w^T, b$

$$\begin{align*}
\mathcal{L}(w^T,b, \alpha)&=\frac{1}{2} {w^T}^2-\sum_{i=1}^n\alpha_i(y_i(w^Tx_i+b)-1)\\
&=\frac{1}{2}\Big(\sum_{i=0}^n\alpha_iy_ix_i\Big)^2
-\sum_{i=1}^n\alpha_iy_i\cdot\big(\sum\limits_{j=0}^n\alpha_jy_jx_j\big)\cdot x_i+\sum_{i=1}^n\alpha_i\\
&=-\frac{1}{2}\Big(\sum_{i=0}^n\alpha_iy_ix_i\Big)^2+\sum_{i=1}^n\alpha_i\\
&=-\frac{1}{2}\sum_{i=0}^n\sum_{j=0}^n\alpha_i\alpha_jy_iy_jx_i^Tx_j+\sum_{i=1}^n\alpha_i\end{align*}$$

$$\max_{\alpha}-\frac{1}{2}\Big(\sum_{i=0}^n\alpha_iy_ix_i\Big)^2+\sum_{i=1}^n\alpha_i$$

æ±‚ $\cfrac{\partial\mathcal{L}}{\partial \alpha}\xlongequal{SET}0$

$$\cfrac{\partial\mathcal{L}}{\partial \alpha_i}=-(\sum_{i=0}^n\alpha_iy_ix_i)\cdot y_ix_i+1=0$$

è§£å¾—

$$\begin{align*}
{w^T}^*&=\sum_{i=0}^n\alpha_iy_ix_i\\
b^*&=\frac{1}{2}\Big[\max_{i:y=1}{w^T}^*x_i+\min_{i:y=-1}{w^T}^*x_i\Big]\\
f(x)&=\Big(\sum_{i=0}^n\alpha_iy_ix_i\Big)\cdot x+b\\
&=\sum_{i=0}^n\alpha_iy_i<x_i, x> + b
\end{align*}$$

è€Œä»¥ä¸Šçš„è¿‡ç¨‹éœ€è¦æ»¡è¶³ **KKT æ¡ä»¶** $\begin{cases}
a_i\ge0&\text{å¯è¡Œæ€§}\\y_i(w^Tx+b)-1\ge0&\text{å¯è¡Œæ€§}\\\alpha_i\cdot(y_i(w^Tx+b)-1)=0  &\text{äº’è¡¥æ¾å¼›æ€§}  
\end{cases}$

$\implies \forall (x_i,y_i), \begin{cases}
\alpha_i=0 &\text{æ ·æœ¬å¯¹å‡½æ•°æ— å½±å“}\\
\alpha_i\neq 0, y_i(w^Tx+b)=1&\text{æ”¯æŒå‘é‡ï¼Œæ ·æœ¬ä½äºå†³ç­–è¾¹ç•Œä¸Š}
\end{cases}$
$\implies$ è®­ç»ƒå®Œæˆåï¼Œå¤§éƒ¨åˆ†çš„è®­ç»ƒæ ·æœ¬éƒ½ä¸éœ€è¦ä¿ç•™ï¼Œ<u>æœ€ç»ˆæ¨¡å‹ä»…ä¸å†³ç­–å‘é‡æœ‰å…³</u>ã€‚

> >å·²çŸ¥ä¸€ä¸ªè®­ç»ƒæ•°æ®é›†ï¼Œæ­£ä¾‹ç‚¹ $x_1=(3,3)^T,x_2=(4,3)^T$ï¼Œè´Ÿä¾‹ç‚¹ $x_3=(1,1)^T$ï¼Œ æ±‚æœ€å¤§é—´éš”åˆ†ç¦»å¹³é¢ã€‚
>
> $x\in\R^2, f(x)=w_1x_1+w_2x_2+b$
> $$\min\frac{1}{2}(w_1^2+w_2^2), \text{ s.t. }\begin{cases}1*(3w_1+3w_2+b)\ge1\\1*(4w_1+3w_2+b)\ge1\\-1*(1w_1+1w_2+b)\ge1\end{cases}$$
> æ„é€ å¯¹å¶é—®é¢˜
> $$\max\limits_\alpha-\cfrac{1}{2}\sum\limits_{i=1}^3\sum\limits_{j=1}^3\alpha_i\alpha_jy_iy_jx_i^Tx_j+\sum\limits_{i=0}^3\alpha_i\\\text{s.t.}\alpha_i\ge0,\sum\limits_{i=1}^3\alpha_iy_i=\alpha_1+\alpha_2-\alpha_3=0$$
> ä»£å…¥æ•°æ®å¾—ï¼š
> $\mathcal{L}(\alpha)=\alpha_1+\alpha_2+\alpha_3-\frac{1}{2}(18\alpha_1^2+25\alpha_2^2+2\alpha_3^2+2*21\alpha_1\alpha_2-2*7\alpha_2\alpha_3-2*6\alpha_1\alpha_3)\\=\alpha_1+\alpha_2+\alpha_3-9\alpha_1^2-\frac{25}{2}\alpha_2^2-\alpha_3^2-21\alpha_1\alpha_2+7\alpha_2\alpha_3+6\alpha_1\alpha_3$
> ä»£å…¥ $\alpha_3=\alpha_1+\alpha_2$
> $$\mathcal{L}(\alpha)=2\alpha_1+2\alpha_2-4\alpha_1^2-\frac{13}{2}\alpha_2^2-10\alpha_1\alpha_2$$
> æ±‚å¯¼ $\cfrac{\partial\mathcal{L}}{\alpha_1}=\cfrac{\partial\mathcal{L}}{\alpha_2}\xlongequal{SET}0$
> $$\begin{cases}2-8\alpha_1-10\alpha_2=0\\2-13\alpha_2-10\alpha_1=0\end{cases}\implies\begin{cases}\alpha_1=1.5\\\alpha_2=-1&\alpha_i\ge0\times
\end{cases}$$
> æå€¼åœ¨è¾¹ç•Œä¸­å–å¾—
> å½“ $\alpha_1=0, \mathcal{L}(\alpha)=2\alpha_2-\cfrac{13}{2}\alpha_2^2$ åœ¨ $(0, \frac{2}{13})$ å–æœ€å¤§å€¼ 0.1538
> å½“ $\alpha_2=0, \mathcal{L}(\alpha)=2\alpha_1-4\alpha_2^2$ åœ¨ $(\frac{1}{4}, 0)$ å–æœ€å¤§å€¼ 0.25
> $\implies \alpha_3=\frac{1}{4}+0=\frac{1}{4}$
> $\alpha_1=\alpha_3>0\implies x_1,x_3$æ˜¯æ”¯æŒå‘é‡
> ä»£å…¥å…¬å¼ï¼š
> $$\begin{cases}w^*=\sum_{i=1}^3\alpha_iy_ix_i=\cfrac{1}{4}\cdot1\cdot\begin{bmatrix}3\\3\end{bmatrix}+0+\cfrac{1}{4}\cdot-1\cdot\begin{bmatrix}1\\1\end{bmatrix}=\begin{bmatrix}1/2\\1/2\end{bmatrix}\\b=-\frac{1}{2}(3\cdot\frac{1}{2}+3\cdot\frac{1}{2}+1\cdot\frac{1}{2}+\cdot\frac{1}{2})=-2\end{cases}$$
> $f(x)=\cfrac{1}{2}x_1+\cfrac{1}{2}x_2-2$
> <u>è¶…å¹³é¢</u>=$\cfrac{1}{2}x_1+\cfrac{1}{2}x_2-2=0$

--

> >å·²çŸ¥ä¸€ä¸ªè®­ç»ƒæ•°æ®é›†ï¼Œè´Ÿä¾‹ç‚¹ $x_1=(1,0)^T,x_2=(0,1)^T$ï¼Œæ­£ä¾‹ç‚¹ $x_3=(2,1)^T$ï¼Œ æ±‚æœ€å¤§é—´éš”åˆ†ç¦»å¹³é¢ã€‚
>
> ==å¯¹å¶é—®é¢˜è§£æ³•==
> $x\in\R^2, f(x)=\begin{bmatrix}w_1\\w_2\end{bmatrix}^T\begin{bmatrix}x_1\\x_2\end{bmatrix}+b$
> $$\min_{w^T,b}\frac{1}{2}(w_1^2+w_2^2)\text{ s.t. }\begin{cases}-1*(w_1+b)\ge1\\-1*(w_2+b)\ge1\\1*(2w_1+w_2)\ge1\end{cases}$$
> æ„é€ å¯¹å¶é—®é¢˜
>$$\max_\alpha-\frac{1}{2}\sum_{i,j=1}^3\alpha_i\alpha_jy_iy_jx_i^Tx_j+\sum_{i=1}^3\alpha_i\text{ s.t. }\alpha_i\ge0, \sum_{i=0}^3\alpha_iy_i=-\alpha_1-\alpha_2+\alpha_3=0$$
> $\mathcal{L}(\alpha)=-\frac{1}{2}(\alpha_1^1+\alpha_2^2+5\alpha_3^2+2*2\alpha_1\alpha_3+2*2\alpha_2\alpha_3)+\sum\limits_{i=1}^3\alpha_i$
> ä»£å…¥ $\alpha_3=\alpha_1+\alpha_2$
> $$\mathcal{L}(\alpha)=2\alpha_1+2\alpha_2-\alpha_1^2-2\alpha_2^2-3\alpha_1\alpha_2$$
> æ±‚åå¯¼ $\frac{\partial\mathcal{L}(\alpha)}{\partial\alpha_1}=\frac{\partial\mathcal{L}(\alpha)}{\partial\alpha_2}\xlongequal{SET}0$
> $$\begin{cases}2-2\alpha_1-3\alpha_2=0\\2-4\alpha_2-3\alpha_1=0\end{cases}\begin{cases}\alpha_1=-2\lt0\times\\\alpha_2=2 \end{cases}$$
> æå€¼åœ¨è¾¹ç•Œå–å¾— $\begin{cases}\alpha_1=0&\mathcal{L}=2\alpha_2-2\alpha_2^2, \max_{(0, 1/2)}=1/2\\\alpha_2=0&\mathcal{L}=2\alpha_1-\alpha_1^2,\max_{(1,0)}=1\end{cases}$
> $(\alpha_1,\alpha_2,\alpha_3)=(1,0,1), \alpha_1=\alpha_3>0, x_1,x_3$æ”¯æŒå‘é‡
> ä»£å…¥å…¬å¼ï¼š
> $$\begin{cases}w^*=1*-1*\begin{bmatrix}1\\0\end{bmatrix}+0+1*1*\begin{bmatrix}2\\1\end{bmatrix}=\begin{bmatrix}1\\1\end{bmatrix}\\b^*=\frac{1}{2}(1*1+0+1*2+1*1)=-2
\end{cases}$$
> $f(x)=\begin{bmatrix}1\\1\end{bmatrix}^T\begin{bmatrix}x_1\\x_2\end{bmatrix}-2$
> åˆ†ç¦»è¶…å¹³é¢ $\begin{bmatrix}1&1\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix}-2=x_1+x_2-2=0$
>
---
> > Consider the classification problem with data $\{(x_i,y_i)\}_{i=1}^m$ , where for each i $x_i\in\R, y_i\in\{Â±1\}$. Recall the hinge loss $Ï†_h(x) = \max\{0, 1âˆ’x\}$. Consider the SVM on $R^1$.
> > In particular, we set $Î» = 1, m = 2, (x_1,y_1)=(1,âˆ’1), (x_2,y_2)=(-2,1)$ Find $(c^*, b^*)$
> >
> > $$\begin{align*}(c^*,b^*)&=\min_{c,b\in\R}\Epsilon_\lambda(c,b)\\\Epsilon_\lambda(c,b)&=\frac{1}{m}\sum_{i=1}^m\phi_h(y_i(cx_i+b))+\lambda_c^2
\end{align*}$$
>
> $\max\{0,1-x\}=\frac{1}{2}(1-x)+\frac{1}{2}|1-x|$
> $y_i(cx_i+b)=\begin{cases}-1(c+b)\\1(-2c+b)\end{cases}$
> $\Epsilon_\lambda(c,b)=\frac{1}{4}\times\frac{1}{2}\times\Big((1+c+b)+|1+c+b|+(1+2c-b)+|1+2c-b|\Big)+c^2\\
=\frac{1}{8}\Big((2+3c)+|1+c+b|+|1+2c-b|\Big)+c^2$
> $f(c,b):=|1+c+b|+|1+2c-b|$
> $\implies b^*\in[-1-c,1+2c], f(c,b)=|1+c+b+1+2c-b|=|2+3c|\xlongequal{SET}0\implies c^*=-\frac{2}{3}$
> $\Epsilon_\lambda(c,b)=\frac{1}{8}\Big(2+3c+|2+3c|\Big)+c^2$
> case 1 $2+3c<0$

!!! question "åé¢ä¸å¤ªçŸ¥é“"

> > è€ƒè™‘ä½¿ç”¨ linear SVM å¯¹å¦‚ä¸‹ä¸¤ç±»å¯åˆ†çš„æ•°æ®è¿›è¡Œåˆ†ç±»ï¼š$\begin{cases}+1:&(1,1)(2,2),(2,0)\\-1&(0,0),(1,0),(0,1)\end{cases}$
> > 1.åœ¨å›¾ä¸­ä½œå‡º6ä¸ªæ ·æœ¬ç‚¹ï¼Œæ„é€ å…·æœ‰æœ€ä¼˜è¶…å¹³é¢å’Œæœ€ä¼˜é—´éš”çš„æƒé‡å‘é‡ã€‚å“ªäº›æ˜¯æ”¯æŒå‘é‡ï¼Ÿ<br>2.é€šè¿‡å¯»æ‰¾æ‹‰æ ¼æœ—æ—¥ä¹˜å­ $\alpha$ æ¥æ„é€ åœ¨å¯¹å¶ç©ºé—´ä¸Šçš„è§£ï¼Œå¹¶ä¸(1)æ±‚çš„ç»“æœè¿›è¡Œæ¯”è¾ƒã€‚

## Non-linear SVM + kernel trick

å¦‚æœåŸå§‹ç©ºé—´æ˜¯æœ‰é™ç»´ï¼Œå³æ•°ç›®æœ‰é™ï¼Œé‚£ä¹ˆä¸€å®šå­˜åœ¨ä¸€ä¸ª==æ›´é«˜ç»´==çš„ç‰¹å¾ç©ºé—´ä½¿æ ·æœ¬çº¿æ€§å¯åˆ†ã€‚

!!! danger "å°†æ ·æœ¬ä»åŸå§‹ç©ºé—´æ˜ å°„åˆ°ä¸€ä¸ª==æ›´é«˜ç»´==çš„ç‰¹å¾ç©ºé—´ï¼Œä½¿å¾—æ ·æœ¬åœ¨æ˜ å°„çš„é«˜ç»´ç©ºé—´å†…çº¿æ€§å¯åˆ†ã€‚<br> ä½ç»´ $x\in\chi\rightarrow \phi(x)\in\mathcal{F}$ é«˜ç»´"
    ğŸ’¡ a **best** **hyperplane** with **the largest margin** in **transformed feature space**
    1. the largest margin æ˜¯åœ¨ **transformed feature space** é‡Œçš„è®¡ç®—
    2. åœ¨ **transformed** : **hyperplane(Linear)**; **original**: non-linear
    1. è¿™ä¸ª **transformed feature space å¯ä»¥æ˜¯åŒç»´åº¦ï¼Œä¹Ÿå¯ä»¥æ›´é«˜ç»´**

è®¾ $\begin{cases}(x, y)\xrightarrow{\text{\red{é«˜ç»´æ˜ å°„}}}(\phi(x), y)\\
f(x)\xrightarrow{\text{\red{é«˜ç»´æ˜ å°„}}}f(\phi(x))=w^T\phi(x)+b=\sum\limits_{i=1}^n\alpha_iy_i<\phi(x_i), \phi(x)>+b
\end{cases}$

åŸå§‹é—®é¢˜ï¼š$\min\limits_{w^T,b}\frac{1}{2}{w^T}^2, \text{ s.t.}\space y_i(w^T\phi(x_i)+b)\ge 1,  \forall i$
å¯¹å¶é—®é¢˜ï¼š$\max\limits_\alpha-\cfrac{1}{2}\sum\limits_{i=0}^n\sum\limits_{j=0}^n\alpha_i\alpha_jy_iy_j\red{<\phi(x_i)^T\phi(x_j)>}+\sum\limits_{i=1}^n\alpha_i, \text{ s.t. }\begin{cases}
\sum\limits_{i=1}^n\alpha_iy_i=0\\\alpha_i\ge0
\end{cases}$

![](./pics/SVM_1.png){width=60%}
![](./pics/SVM_2.png){width=60%}
![](./pics/SVM_3.png){width=60%}

!!! p "$\red{<\phi(x_i)^T\phi(x_j)>}$ æ˜¯ $x_i, x_j$ æ˜ å°„åˆ°é«˜ç»´ç©ºé—´ä¹‹åçš„å†…ç§¯ã€‚ç”±äºç»´åº¦å¾ˆé«˜è®¡ç®—å›°éš¾ï¼Œæ‰€ä»¥æå‡º ==æ ¸å‡½æ•°== $\kappa(x_i,x_j)$"

### æ ¸å‡½æ•° kernel trick

So we need kernel tricks to complete the inner product of vectors in the low-dimensional space to reduce the computation cost.
ä¸æ˜¾å¼åœ°è®¾è®¡æ˜ å°„ $\phi(x)$ï¼Œè€Œæ˜¯è®¾è®¡ä¸€ä¸ªæ ¸å‡½æ•° $\kappa(x_i,x_j)$ï¼Œæ¥è§£å†³æ˜ å°„åˆ°é«˜ç»´åº¦ç©ºé—´åå‡ºç°çš„<u>è®¡ç®—</u>ä¸Šç»´åº¦çˆ†ç‚¸çš„é—®é¢˜ï¼Œ**å‡å°‘è®¡ç®—ä»£ä»·**ã€‚
$$\kappa(x_i,x_j)=<\phi(x_i),\phi(x_j)>= \phi(x_i)^T\phi(x_j), \forall x\in\chi, \phi(x)\in\mathcal{F} $$

!!! p "$\kappa(x_i,x_j)$ åœ¨<u>åŸå§‹ç©ºé—´</u>çš„è®¡ç®— $\Leftrightarrow x_i,x_j$ åœ¨é«˜ç»´ç©ºé—´çš„å†…ç§¯ã€‚"
    !!! danger "==é«˜ç»´è¡¨ç°,ä½ç»´è®¡ç®—=="
        ç‰¹å¾æ˜¯==ä»ä½ç»´è½¬åˆ°é«˜ç»´==ï¼ï¼ï¼ï¼<br> æ ¸æŠ€å·§æ˜¯åœ¨ä½ç»´ç©ºé—´å®Œæˆé«˜ç»´ç©ºé—´å‘é‡å†…ç§¯çš„==è®¡ç®—==ã€‚åªæ˜¯åœ¨è®¡ç®—ä¸Šå–å·§äº†ä¸€ä¸‹ã€‚

é‡å†™ï¼š
- å¯¹å¶é—®é¢˜ï¼š $\max\limits_\alpha-\cfrac{1}{2}\sum\limits_{i=0}^n\sum\limits_{j=0}^n\alpha_i\alpha_jy_iy_j\red{\kappa(x_i,x_j)}+\sum\limits_{i=1}^n\alpha_i, \text{ s.t. }\begin{cases}
\sum\limits_{i=1}^n\alpha_iy_i=0\\\alpha_i\ge0\end{cases}$
- å‡½æ•° $f(x)=\sum\limits_{i=1}^n\alpha_iy_i\red{\kappa(x_i,x)}+b$

#### æ ¸å‡½æ•°è®¾è®¡
æ»¡è¶³ï¼š
$$\begin{bmatrix}
\kappa(x_1,x_1)& \dotsb&\kappa(x_1,x_n)\\
\vdots& \ddots&\vdots\\
\kappa(x_n,x_1)& \dotsb&\kappa(x_n,x_n)\end{bmatrix}\succeq0$$
- å¯¹ç§°
- æ ¸çŸ©é˜µåŠæ­£å®š

**é”™è¯¯çš„æ ¸å‡½æ•°**ï¼Œ å°†æ ·æœ¬æ˜ å°„åˆ°äº†ä¸€ä¸ªä¸åˆé€‚çš„ç‰¹å¾ç©ºé—´ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚

#### å¸¸è§æ ¸å‡½æ•°

|å|$\kappa(x_i, x_j)$|notes
|--|--|--|
çº¿æ€§æ ¸|$x_i^Tx_j$|for çº¿æ€§å¯åˆ†
å¤šé¡¹å¼æ ¸|$(x_i^Tx_j)^d$|$d\ge1$
é«˜æ–¯æ ¸|$\exp(-\cfrac{\Vert x_i-x_j\Vert^2}{2\sigma^2})$|$\sigma$ := å¸¦å®½
æ‹‰æ™®æ‹‰æ–¯æ ¸|$\exp(-\cfrac{\Vert x_i-x_j\Vert}{\sigma})$|$\sigma>0$
Sigmoidæ ¸|$\tanh(\beta x_i^Tx_j+\theta)$|$\beta>0,\theta<0$

### è½¯é—´éš” SVM

!!! p Qï¼š"å™ªå£°æ•°æ® outlier"
    SVM è¦æ±‚æ‰€æœ‰çš„æ ·æœ¬éƒ½æ»¡è¶³çº¦æŸã€‚å½“æ•°æ®ä¸­å­˜åœ¨å™ªå£°æ•°æ®ï¼Œæœ¬èº«å°±åç¦»äº†æ­£å¸¸ä½ç½®ï¼Œ å¯¼è‡´åˆ’åˆ†çš„è¶…å¹³é¢è¢«æŒ¤æ­ªäº†ï¼Œé™ä½äº† SVM çš„æ³›åŒ–æ€§èƒ½ã€‚ï¼ˆç”šè‡³è¯´ä½¿å¾—æ‰¾ä¸åˆ° SVM

    !!! danger "Unfortunately, SVM is allergic to the noises so the robustness of the algorithm is not good." 
        we need to add the slack variables \$\xi_i$ to allow all training examples not satisfying the constraints strictly. Meanwhile, we set a small C to control the deviation of examples and constraints

==è½¯é—´éš” SVM==ï¼š
1. å…è®¸ä¸€äº›æ•°æ®ç‚¹ä¸æ»¡è¶³çº¦æŸ $y_i(w^Tx_i+b)\ge1$ï¼Œå³å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šåç§»è¶…å¹³é¢ã€‚
2. åŒæ—¶ä½¿å¾—ä¸æ»¡è¶³çº¦æŸçš„æ•°æ®ç‚¹å°½å¯èƒ½å°‘ã€‚</u>

$$\mathcal{L}(w^T,b, \alpha):= \frac{1}{2} {w^T}^2\red{+C}\sum_{i=1}^n\red{l_{0|1}(}y_i(w^Tx_i+b)-1\red{)}\\\space\\\min_{w^T,b}\max_{\alpha}\mathcal{L}(w^T,b, \alpha)\\
\red{C>0,l_{0|1}(z)=\begin{cases}0&\text{otherwise}\\1&z<0
\end{cases}}$$

$\red{C}$ æ§åˆ¶ç€ç›®æ ‡å‡½æ•°ä¸å¼•å…¥æ­£åˆ™é¡¹ä¹‹é—´çš„æƒé‡ã€‚å½“ $C\rightarrow\infin$ ä¼šè¿«ä½¿æ‰€æœ‰æ ·æœ¬æ»¡è¶³çº¦æŸã€‚
$$\Leftrightarrow \mathcal{L}(w^T,b, \alpha):= \frac{1}{2} {w^T}^2\space\text{s.t.} y_i(w^Tx_i+b)\ge1$$

$\red{l_{0|1}(z)}$ 0-1æŸå¤±å‡½æ•°ï¼Œè¡¨ç°æ•ˆæœæœ€å¥½ï¼Œä½†æ˜¯éå‡¸ã€éè¿ç»­ã€æ•°å­¦æ€§è´¨ä¸å¥½ï¼Œä½¿å¾—æ±‚è§£å›°éš¾ï¼Œå› æ­¤ä¹Ÿå¯ä½¿ç”¨å…¶ä»–å‡½æ•°ä½œä¸ºæ›¿æ¢ã€‚

ç‰¹ç‚¹|å‡¸çš„ã€è¿ç»­ã€æ˜¯ $l_{0ï½œ1}(z)$ ä¸Šç•Œ
|--|--|
hinge | $\max(0, 1-z)$
exponential | $\exp(-z)$
logistic | $\log(1+\exp(-z))$

#### hinge
$$\mathcal{L}(w^T,b, \alpha):= \frac{1}{2} {w^T}^2\red{+C}\sum_{i=1}^n\red{\max(0, 1-}y_i(w^Tx_i+b)\red{)}$$
å¼•å…¥æ¾å¼›å˜é‡ $\xi$

$$\frac{1}{2} {w^T}^2+C\sum_{i=0}^n\red{\xi_i}\\
\text{s.t.} y_i(w^Tx_i+b)\ge\red{1-\xi_i},\space \xi_i\gt0$$

$\red{\xi_i}$, æ¯ä¸ªæ ·æœ¬éƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„**æ¾å¼›å˜é‡**ï¼Œæ¥è¡¨ç¤ºè¯¥æ ·æœ¬ä¸æ»¡è¶³çº¦æŸçš„ç¨‹åº¦ã€‚
ä½¿ç”¨**æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•**ï¼š

$$\mathcal{L}(w^T,b,\xi, \alpha,\beta):= \frac{1}{2} {w^T}^2+C\sum_{i=0}^n\red{\xi_i}-\sum_{i=0}^n\alpha_i(y_i(w^Tx_i+b)-1+\xi_i)-\sum_{i=0}^n\beta_i\xi_i\\
\alpha_i, \beta_i\ge0, \text{æ‹‰æ ¼æœ—æ—¥ä¹˜å­}$$

æ±‚ $\cfrac{\partial\mathcal{L}(\cdot)}{\partial w^T}=\cfrac{\partial\mathcal{L}(\cdot)}{\partial b}=\cfrac{\partial\mathcal{L}(\cdot)}{\partial \xi}\xlongequal{SET}0$

$$\begin{align*}
\cfrac{\partial\mathcal{L}(\cdot)}{\partial w^T}&=w^T-\sum_{i=0}^n\alpha_iy_ix_i&=0 &\implies w^T=\sum_{i=0}^n\alpha_iy_ix_i \\
\cfrac{\partial\mathcal{L}(\cdot)}{\partial b}&=\sum_{i=0}^n
\alpha_iy_i&=0\\
\cfrac{\partial\mathcal{L}(\cdot)}{\partial \xi_i}&=C-\alpha_i-\beta_i&=0&\implies C=\alpha_i+\beta_i
\end{align*}$$

!!! p "å”¯ä¸€çš„å·®åˆ«åœ¨äºå¯¹å¯¹å¶å˜é‡çš„çº¦æŸä¸åŒ"
    |è½¯é—´éš”|ç¡¬é—´éš”|
    |--|--|
    $0\le\alpha_i\red{\le C}$|$0\le\alpha_i$

> > æ ¹æ® KKT å¯¹å¶äº’è¡¥æ¡ä»¶ï¼Œåˆ†æå¸¦æ¾å¼›å˜é‡çš„ SVM ä¼˜åŒ–é—®é¢˜ä¸­ï¼Œå›¾ä¸Šæ‰€ç¤ºçš„ $ABCDEFGHIJKL$è¿™å‡ ä¸ªç‚¹çš„æ¾å¼›å˜é‡ $\xi_i$, æ‹‰æ ¼æœ—æ—¥å› å­$(\alpha_i,\mu_i)$,æ­£åˆ™åŒ–å˜é‡ $C$ä¹‹é—´çš„å…³ç³»ã€‚æ³¨æ„ï¼š$0\le\alpha_i\le C,\mu_i\ge0,\xi_i\ge0,$ å¯¹å¶äº’è¡¥æ¡ä»¶ä¸º $\alpha_i(1-y_i(w^Tx_i)-\xi_i)=0,\mu_i\xi_i=0$
