# Artificial Intelligence, AI

!!! quote "==data_drived== çš„è¿™ä¸ªæ¦‚å¿µ"
    ä¸–ç•Œç¬æ¯ä¸‡å˜ï¼Œå¦‚ä½•ä½¿AIå…·æœ‰äººè„‘çš„æ™ºèƒ½ä»è€Œå¤„ç†åƒå˜ä¸‡åŒ–çš„é—®é¢˜ï¼Œå®šæ­»ä¸€ä¸ªå®šä¹‰å’Œå†™æ­»ä¸€ä¸ªç¨‹åºæ°¸è¿œä¸æ˜¯ bestã€‚æˆ‘ä»¬éœ€è¦æ•°æ®é©±åŠ¨ã€‚==MLå’ŒDLæ­£æ˜¯åŒå‡ºè¿™ä¸€è„‰==ã€‚

AI isÂ intelligenceÂ exhibited byÂ machines, rather than humans or other animals (natural intelligence,Â NI).

!!! quote "==AI== is a big concept, ==ML== is just one subarea of AI. ==DL== is just a part of ML, which uses DNN mapping function."

![](./pics/AI_1.png)

**List six subareas of AI:**
Knowledge Data science, Reasoning, Statistics, Nature Language Process (NLP), Planning, Computer Vision (CV), Neuroscience, Machine Learning (ML), Robotics and so on.
**the differences between traditional ML & DL?**

![](./pics/AI_2.png)

|diff|Traditional  ML|DL|
|--|--|--|
|feature|feature engineering|feature learning|
|domain-specific knowledge|more|less|
|model capacity |low |high |

## Terminology

- records è®°å½• = instance å®ä¾‹ = sample æ ·æœ¬ = feature vector  ç‰¹å¾å‘é‡ = example æ ·ä¾‹(ä¸¥æ ¼ï¼šæœ‰äº† label çš„ sample)
- attribute å±æ€§ = feature ç‰¹å¾
attribute value å±æ€§å€¼
attribute space å±æ€§ç©ºé—´ = sample space æ ·æœ¬ç©ºé—´ï¼ˆç»´æ•° dimension $d$
- label æ ‡ç­¾ prediction é¢„æµ‹
label space æ ‡ç­¾ç©ºé—´
- trainingï¼štraining sample - training set
- testingï¼štesting sample
- ground truth çœŸå® learner å­¦ä¹ å™¨ hypotheis å‡è®¾
å­¦ä¹ å™¨ä»çœŸå®ä¸­å­¦ä¹ å‡è®¾
- version space ç‰ˆæœ¬ç©ºé—´ï¼šåŒæ—¶å¤šä¸ªç¬¦åˆè®­ç»ƒé›†çš„å‡è®¾ã€‚
- inductive bias å½’çº³åå¥½ã€‚ç®—æ³•åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å¯¹æŸç§ç±»å‹å‡è®¾çš„åå¥½ã€‚å­¦ä¹ ç®—æ³•è‡ªèº«åœ¨ä¸€ä¸ªå¯èƒ½å¾ˆåºå¤§çš„å‡è®¾ç©ºé—´ä¸­å¯¹å‡è®¾è¿›è¡Œé€‰æ‹©çš„å¯å‘å¼æˆ–â€ä»·å€¼è§‚â€œ
    > åå¥½ å¤æ‚ | ç®€å• å¥¥å¡å§†å‰ƒåˆ€ **Occamâ€˜s razor**
    > åå¥½ æŸä¸ªç‰¹å¾ï¼Œæƒé‡è®¾ç½®

## Task

- ==Classification åˆ†ç±»==ã€‚ binary, multi-class
Decision Tree, Naive Bayes, k-Nearest Neighbor (KNN), Support Vector Machines (SVM), Logistic Regression
- ==Regression å›å½’==ã€‚real-valued, multi-output, functional
- ==Ranking==ã€‚ pointwise, pairwise, listwise
- ==Transcription è½¬å½•==ã€‚éç»“æ„åŒ–çš„æ•°æ® â¡ï¸ ç¦»æ•£çš„æ–‡æœ¬å½¢å¼ã€‚
    > æ–‡å­—è¯†åˆ«, è¯­éŸ³è¯†åˆ«
- ==Machine Translation æœºå™¨ç¿»è¯‘==ã€‚Aè¯­è¨€ç¬¦å·**åºåˆ—** â¡ï¸ B è¯­è¨€ç¬¦å·**åºåˆ—**
- ==ç»“æ„åŒ–è¾“å‡º==ã€‚è¾“å‡º ğŸŸ° å‘é‡ï½œå…¶å®ƒåŒ…å«å¤šä¸ªå€¼çš„æ•°æ®ç»“æ„ï¼Œå¹¶ä¸”æ„æˆè¾“å‡ºçš„ä¸åŒå…ƒç´ ä¹‹é—´å…·æœ‰é‡è¦å…³ç³»ã€‚
    > ç†è®ºä¸ŠåŒ…æ‹¬è½¬å½•&ç¿»è¯‘
    > è¯­æ³•åˆ†æï¼šå¥å­â¡ï¸è¯­æ³•ç»“æ„æ ‘ï¼Œæ ‡è®°æ ‘çš„èŠ‚ç‚¹ä¸º åŠ¨ï½œåï½œå‰¯
    > å›¾åƒçš„åƒç´ çº§åˆ†å‰²ï¼šå°†åƒç´ åˆ†é…åˆ°ç‰¹å®šç±»åˆ«ã€‚ï¼šæ ‡æ³¨èˆªæ‹ä¸­çš„é“è·¯ã€‚
    > ä¸ºå›¾ç‰‡æ·»åŠ æè¿°ã€‚
- ==å¼‚å¸¸æ£€æµ‹==ã€‚åœ¨ä¸€ç»„äº‹ä»¶æˆ–å¯¹è±¡ä¸­ç­›é€‰å‡ºï¼Œå¹¶æ ‡è®°**ä¸æ­£å¸¸ï½œéå…¸å‹**çš„ä¸ªä½“ã€‚
    > ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹
- ==åˆæˆï½œé‡‡æ ·==ã€‚ç”Ÿæˆä¸€äº›å’Œè®­ç»ƒæ•°æ®ç›¸ä¼¼çš„æ–°æ ·æœ¬ã€‚å¸Œæœ›è¾“å‡ºçš„ç»“æœæ˜¯æ›´åŠ è‡ªç„¶å’ŒçœŸå®ã€‚
    > è§†é¢‘æ¸¸æˆè‡ªåŠ¨ç”Ÿæˆå¤§å‹ç‰©ä½“æˆ–é£æ™¯çš„çº¹ç†
    > è¯­éŸ³åˆæˆã€ç»“æ„åŒ–è¾“å‡ºã€‘
- ==ç¼ºå¤±å€¼å¡«è¡¥==ã€‚ç»™å®šä¸€ä¸ªæ–°æ ·æœ¬ï¼Œå¡«è¡¥æ ·æœ¬ä¸­ç¼ºå¤±çš„å…ƒç´ ã€‚
- ==å»å™ª==ã€‚è¾“å…¥ ğŸŸ° ç»è¿‡æœªçŸ¥æŸåè¿‡ç¨‹åçš„**æŸåæ ·æœ¬** $\tilde{x}$ï¼›è¾“å‡º ğŸŸ° å¹²å‡€æ ·æœ¬ $x$ï½œæ¡ä»¶æ¦‚ç‡ $p(x|\tilde{x})$
- ==å¯†åº¦ä¼°è®¡ï½œæ¦‚ç‡è´¨é‡å‡½æ•°ä¼°è®¡==ã€‚ã€‚ã€‚ã€‚ã€‚
  
## ML

==â€œMachine learning== is the field of study that gives computers the ability to  learn without being explicitly programmed.â€ â€” Arthur Samuel (1959)
â€œA computer program is said to learn from experience E with respect to  some class of tasks T and performance measure P, if its performance at  tasks in T, as measured by P, improves with experience E.â€ â€” Tom  Mitchell (1998)

### åˆ†ç±»

==Supervised learning==ã€‚learning $x\xrightarrow{P(y|x)} y$ with labelsï½œtargets.

!!! warning "ç”Ÿæˆæ¨¡å‹ & åˆ¤åˆ«æ¨¡å‹ $\in$ Supervised learning"

- ==ç”Ÿæˆæ¨¡å‹==ã€‚å­¦ä¹ å¾—åˆ°**è”åˆæ¦‚ç‡åˆ†å¸ƒ** $P(x,y)$ ç„¶åæ±‚æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒã€‚ä»¥ ç»Ÿè®¡å­¦ & bayes ä¸ºç†è®ºåŸºç¡€ã€‚
    > æœ´ç´ è´å¶æ–¯ï¼Œæ··åˆé«˜æ–¯æ¨¡å‹ï¼Œéšé©¬å°”å¯å¤«æ¨¡å‹
- ==åˆ¤åˆ«æ¨¡å‹==ã€‚å­¦ä¹ å¾—åˆ°**æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ** $P(y|x)$
    > SVMï¼Œå†³ç­–æ ‘ã€‚LRï¼Œã€‚ã€‚ã€‚ã€‚

==Unsupervised learning==: learning $P(x)$ without labels.

- ==clustering èšç±»==
- representation
- self-supervised
- generative models/AIGC

!!! p "ç›‘ç£å­¦ä¹  & æ— ç›‘ç£å­¦ä¹  ç•Œé™æ˜¯æ¨¡ç³Šçš„ã€‚"
    å› ä¸ºæ²¡æœ‰å®¢è§‚çš„åˆ¤æ–­æ¥åŒºåˆ†ç›‘ç£è€…æä¾›çš„å€¼**æ˜¯ featureï½œtarget**ã€‚é€šä¿—åœ°è¯´ï¼Œæ— ç›‘ç£å­¦ä¹ çš„å¤§å¤šæ•°å°è¯•æ˜¯æŒ‡ä»ä¸éœ€è¦äººä¸ºæ³¨é‡Šçš„æ ·æœ¬çš„åˆ†å¸ƒä¸­æŠ½å–ä¿¡æ¯ã€‚
    > $p(x)=\prod\limits_{i=1}^np(x_i|x_1,\dots,x_{i-1})$ æ— ç›‘ç£å­¦ä¹  $p(x)$ å¯è¢«åˆ†è§£æˆ n ä¸ªç›‘ç£å­¦ä¹ é—®é¢˜ã€‚
    > $p(y|x)=\cfrac{p(x,y)}{\sum\limits_{y'}p(xy')}$ æœ‰ç›‘ç£å­¦ä¹  $p(y|x)$ ç”¨æ— ç›‘ç£å­¦ä¹ çš„æ³•å­å…ˆå­¦ä¹ è”åˆåˆ†å¸ƒã€‚

==Semi-supervised learning åŠç›‘ç£å­¦ä¹ ==ã€‚ä¸€äº›æ ·æœ¬æœ‰ï¼Œä¸€äº›æ ·æœ¬æ²¡æœ‰ã€‚
==Reinforcement learning å¼ºåŒ–å­¦ä¹ ==ã€‚machine takes an action; teacher provides rewards

- control
- pricing
- games

![](./pics/reinforcementL_1.png){width=80%}

==Semi-supervised / active learning==

Evaluation  words
**Representation**: how to encode the raw data?
**Generalization**: how well can we do on unseen data?
**Interpretation**: how to explain the findings?
**Complexity**: how much time and space?
**Efficiency**: how many samples?
**Privacy**: how to respect data privacy?
**Robustness**: how to degrade gracefully under (malicious) error?

==Big data== is a collection of data sets so large and  complex that it becomes difficult to process using on-  hand database management tools or traditional data  processing applications.

**Characteristics of Big Data:** **4V**
Volume
Velocity
Variaty
Veracity

![](./pics/BigData_1.png){width=80%}

## Assumption

==ç‹¬ç«‹åŒåˆ†å¸ƒ independent & identically $(i.i.d.)$==ã€‚å‡è®¾æ ·æœ¬ç©ºé—´ä¸­å…¨ä½“æ ·æœ¬æœä»ä¸€ä¸ªæœªçŸ¥ â€œåˆ†å¸ƒ distributionâ€ $D$ï¼Œç°æœ‰çš„æ¯ä¸ª sample éƒ½æ˜¯ç‹¬ç«‹åœ°ä»è¿™ä¸ªåˆ†å¸ƒä¸Šé‡‡æ ·å¾—åˆ°çš„ã€‚

å…³äºåˆ†ç±»

æœ‰å¾ˆå¤šçš„åˆ†ç±»æ¨¡å‹ï¼Œä»–ä»¬å„è‡ªæœ‰å„è‡ªçš„ä½œç”¨åŸç†ï¼Œä¹Ÿæœ‰å„è‡ªçš„å‡ ä½•è¡¨ç¤ºã€‚ä½†å½’æ ¹å…¶æœ¬è´¨è¿˜æ˜¯estimate distribution.

<div class="grid" markdown>
<figure markdown="span">![](./pics/classi_3.png)<p>è´å¶æ–¯ é•¿è¿™æ ·</p></figure>
<figure markdown="span">![](./pics/classi_2.png){width=70%}<p>SVM é•¿è¿™æ ·ï¼Œè¿™ä¸ªå…¶å®ä¹Ÿæ˜¯ä¸€ä¸ªåˆ†å¸ƒ</p></figure>
</div>

## identifiability of model æ¨¡å‹çš„å¯è¯†åˆ«æ€§

[Statistical Modelling and Identifiability of Parameters](https://www.analyticsvidhya.com/blog/2021/05/statistical-modelling-and-identifiability-of-parameters/)

å‚æ•°çš„ ==å¯è¯†åˆ«æ€§ identifiability== ä½¿æˆ‘ä»¬èƒ½å¤Ÿè·å¾—è¯¥å‚æ•°å€¼çš„ç²¾ç¡®ä¼°è®¡å€¼ã€‚åœ¨æ²¡æœ‰å¯è¯†åˆ«æ€§çš„æƒ…å†µä¸‹ï¼Œå³ä½¿æœ‰æ— é™æ¬¡è§‚æµ‹ï¼Œæˆ‘ä»¬ä¹Ÿæ— æ³•ä¼°è®¡å‚æ•°Î¸çš„çœŸå®å€¼ã€‚
> Xï½N(a+b, Ïƒ) is not identifiable
> Xï½N(Î¼, Ïƒ) is identifiable
> æ¯ä¸ªä¸åŒçš„ Î¼ éƒ½æŒ‡å‘ä¸€ä¸ªä¸åŒçš„æœŸæœ›ï¼ŒæŒ‡å‘ä¸åŒçš„æ­£æ€åˆ†å¸ƒï¼ŒåŒæ ·åœ°æ¯ä¸€ä¸ªæ‹¥æœ‰ä¸åŒæœŸæœ›çš„æ­£æ€åˆ†å¸ƒéƒ½èƒ½å”¯ä¸€æ±‚å‡ºä¸€ä¸ª Î¼ã€‚ä½†æ˜¯å¯¹ä¸åŒçš„ a å’Œ b æ¥è¯´ï¼Œa+b ä¹Ÿè®¸æ˜¯ç›¸åŒçš„ï¼Œæ„å‘³ç€ä¸€ä¸ªæ­£æ€åˆ†å¸ƒå¹¶ä¸èƒ½å”¯ä¸€æ¨æ–­å‡º (a, b, Ïƒ)çš„å€¼ã€‚ï¼ˆe.g. (1, 1, Ïƒ) = (0, 2, Ïƒ)

## DNN

!!!p "why it is difficult for neural networks to be deep before 2010s."
    1. No enough computation resources,
    2. No enough training data,
    3. Gradient vanish problem and no advanced optimization techniques

## condition

!!! p "Product Rule, Chain Rule and Bayesâ€™ Rule?"
    Product Rule: P(y)P(x|y)=P(x,y)
    Chain Rule: P(A1A2A3â€¦An)= P(A1|A2A3â€¦An) P(A2|A3A4â€¦An)â€¦ P(An-1| An) P(An)
    Bayesâ€™ Rule: $P(F|E)=\cfrac{P(E|F)P(F)}{P(E|F)P(F)+ P(E|\neg F)P(\neg F)}$

> â“Suppose 1 in 1000 persons has a certain disease. We have two test methods: 1ï¸âƒ£  detect the disease (produce positive results) in 99% of the diseased persons and in 5% of the healthy persons (false alarm). 2ï¸âƒ£ : detect the disease (produce positive results) in 90% of the diseased persons and in 1% of the healthy persons (false alarm). Which test method is better? Give the reason.
>
> ğŸ’¡
>
> D:={Diseased person}; H={Healthy Persons}, +I ={Persons with positive results with Test method I}, +II ={Persons with positive results with Test method II }
$P(D)=0.001,\\ P(+I|D)=0.99, P(+I|H)=0.05,\\ P(+II|D)=0.90, P(+II|H)=0.01$
We compute the probability of a diseased person that can be detect by Test method I or Test method II, repectively, which is|
$P(D|+I)= \cfrac{P(+I|D)P(D)}{P(+I|D) P(D)+ P(+I|H) P(H)}=\frac{0.99\times 0.001}{0.99\times0.001+0.05\times0.999}=0.0194$
$P(D|+II)= \cfrac{P(+II|D)P(D)}{P(+II|D) P(D)+ P(+II|H) P(H)}=\frac{0.90\times 0.001}{0.90\times0.001+0.01\times0.999}=0.0826$
>
> $P(D|+ I)< P(D|+ II)$ It means if a person has the disease, it has more chance of being detected by Test Method II. So Test method II is better.

## search

!!! p "A search algorithm is admissible if it is guaranteed to find a minimal path to a solution whenever such a path exists."

### j

!!! p "Q4. the motivations of employing heuristics in solving AI problems?"
    1.A problem may not have an exact solution because of ambiguities in problem statement or available data
    2.A problem may have an exact solution, but the computational cost (time and space) of finding it may be prohibitive.

!!! p "informed search method"
    Hill-climbing search; A* algorithm; Best-first.

#### Best-first search

Best-first search is neither complete nor optimal âœ…

![](./pics/BestFS_1.png){width=40%}

|step|open|closed|notes|
|--|--|--|--|
|1 | [S70] | [ ] | S=0+70|
|2 | [A80, B85] | [S70] | A=45+35 < B=55+30|
|3 | [B85, C100] | [A80, S70] | B85 < B_A=45+30+30<br>B85 < C= 45+25+30 |
|4 | [G100, C100] |  [B85, A85, S70] |G=55+45 =C100|
|5 |[]|[G100, C100, B85, A85, S70] |Goal reached. Stop.|

Solution path : S-B-G

!!! danger "Best-first $\xRightarrow{\forall n, h(n)\leq h^*(n)}$ A \*algorithm<br> Algorithm A* is a special best-first search algorithm âœ”ï¸"
    > example1

    |$n$|$h(n)$<br>heuristic estimation|?|$h^*(n)$<br> actual n$\rightarrow$ goal G|
    |--|--|--|--|
    S| 70 | < | 55 +45
    A| 35 | < | 25 + 35
    B| 30 | < | 45
    C| 30 | < | 35
    G| 0 | = | 0
    > All h(n)<=h*(n),so it is an A* algorithm and it is admissible. The path S-B-G is the shortest path.

### BNN, Bayesian Belief Net

A Bayesian Belief Net consists of a graph and some local conditional probabilities.

!!! p ""
    Serial connection:      $P(AVB)=P(B|V)P(V|A)P(A)$
    Diverging Connection:   $P(AVB)=P(B|V)P(A|V)P(V)$
    Converging Connection: $P(AVB)=P(V|AB)P(A)P(B)$
    Modus Ponens rule $\cfrac{a,a\implies b}{b}$

> ![](./pics/BNN_1.png){width=40%}
>
> $P(ABC)=P(AB)P(C)=.6*.8*.7=.336$
>
> $P(A)=.6*.8+.4*.2=.56$
>
> $P(AC)=P(A)P(C)=.56*.7=.392$
>
> $P(D|B)=P(D|BC)P(C)+(PD|B\neg C)P(\neg C)=.8*.7+.2*.3=.62$
> $P(D|\neg B)=P(D|\neg BC)P(C)+(PD|\neg B\neg C)P(\neg C)=.1*.7+.1*.3=.1$
> $P(B|D)=\cfrac{P(BD)}{P(D)}=\cfrac{P(D|B)P(B)}{P(D|B)P(B)+P(D|\neg B)P(\neg B)}=\cfrac{.62*.8}{.62*.8+.1*.2}=.9612$
> $P(B|\neg D)=\cfrac{P(B\neg D)}{P(\neg D)}=\cfrac{P(\neg D|B)P(B)}{1-P(D)}=\cfrac{(1-P(D|B))P(B)}{1-P(D)}=.6281$

### Breadth-firth search

!!! danger "If the search space contains very deep branches without solution, breadth-first search will be a better choice than depth-first search.  T"
    æœç´¢ç©ºé—´åŒ…å«éå¸¸æ·±çš„åˆ†æ”¯ $\neq$ ç›®æ ‡åœ¨å¾ˆæ·±çš„åˆ†æ”¯é‡Œã€‚
    <u>contains very deep branches without solution</u> è¿™ä¸ªæ·±çš„æ”¯æ˜¯æ²¡æœ‰è§£ï¼Œé‚£ä¹ˆè§£åœ¨æµ…å±‚çš„èŠ‚ç‚¹é‡Œ $\implies$ BFS better

!!! p "breadth-first search"
    ==storage cost==
    - branch factor $b$
    - storage cost $s$ bytes/node
    - search speed $v$ nodes/second
    - goal depth $d$
    $$\sum_{i=0}^db^i\times s$$
    > â“ For a searching tree, assume that the branch factor is b=10, the storage cost is 1000 bytes/node and the searching speed is 10,000 nodes/second. With breadth-first search, what is the required storage space at depth 3?
    ğŸ’¡ At depth 3, the number of created nodes is 1+10+100+1000=111,1, and thus the storage space is 111,1ï‚´1000=11,11K bytes.

    ==time complxity==
    - branch factor $b$
    - goal depth $d$
    $$b^{d+1}$$

## ç¦»æ•£

!!! p "Why in some cases we need to use First Order Logic (FOL) rather than Propositional Logic (PL)?"
    1.Propositional logic (PL) is too â€œcoarseâ€ to easily describe properties of objects.
    2.First order logic (FOL) is to extend the expressiveness of PL.

!!! p ""
    Forward chaining in first order logic is a data-driven algorithm.
    Backwards chaining in first order logic is a goal-driven algorithm.

!!! p "$\forall$ & $\exist$"
    - $\forall x\forall y=\forall y\forall x$
    - $\exist x, y = \neg \forall x\neg y$
    - $\exist x\forall y\neq \forall y \exist x$

Satisfiable: A sentence is satisfiable if there is some interpretation for which it is true.
Unsatisfiable: A sentence is unsatisfiable if there is no interpretation for which it is true.
Valid: A sentence is valid if it is true for every
interpretation.

> Compute the loss of eight-puzzle
> ![](./pics/image.png){width=80%}
>
> ||1|2|3|4|5|6|7|8|
> |--|--|--|--|--|--|--|--|--|
> |State1|âŒ 1|âŒ 1|âœ”ï¸|||âŒ 1|âŒ 1|âŒ 2|
> |State2|âŒ 1|âŒ 1|âœ”ï¸|||||âŒ 2|
> ---
>
> ||Tiles out of place<br> ä¸å¯¹çš„æœ‰å‡ ä¸ª|Sum of distances out of places<br> æ”¹å›æ¥éœ€è¦å‡ æ­¥|
> |--|--|--|
> |State1|5|6|
> |State2|3|4|

## dt

|?|High Entropy|Low Entropy|
|--|--|--|
|Distribution of variable| uniform like  | may have many peaks and valleys|
|histogram |Flat|may have many lows and highs|
|Values sampled from it|  less predictable|more predictable|
|information(about label) | Less| More|

> We flip two different coins independently for 16 times, which have the following results:
>
> Sequence 1 : 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 [0:1=12:4]
>
> Sequence 2 : 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 [0:1=8:8]
> Compute the information content (entropy) of the outcome of tossing these two coins, respectively.
>
> I(conin_toss_1)= -0.75log2(0.75)-0.25log2(0.25)=0.811 bits
> I(conin_toss_2)= -0.5log2(0.5)-0.5log2(0.5)=1 bits

## kmeans

!!! danger "Manhattan distance is <u>usually</u> larger than Euclidean distance. âœ”ï¸"
    å½“å­˜åœ¨ä¸‰è§’å½¢å½¢çŠ¶ï¼Œæ–œè¾¹æ¯”ç›´è§’è¾¹åŠ å’Œè¦å°

## parameter tuning

### Guess

![](./pics/paramT_1.png){width=80%}

#### Grid Search

1. specify a **list** of possible hyperparameter values éœ€è¦äººä¸ºè®¾å®šè¶…å‚æ•°çš„èŒƒå›´
2. **Grid Search** will train models with every possible combination of the provided hyperparameter values and assess the performance of each trained model using a specified metric (e.g., the accuracy of predictions on a test data set). **Grid Search** å°±ä¼šç”¨è®¾å®šåŒºé—´å†…çš„**å„ç§ç»„åˆ**è¿›è¡Œè®­ç»ƒå¹¶æ ¹æ®äººä¸ºæŒ‡å®šçš„metricè¿›è¡Œassess

**Limitationï¼š**

1. äº‹å…ˆå¾ˆéš¾çŸ¥é“æœ€ä¼˜æ˜¯åœ¨å“ªé‡Œï¼Œæ‰€ä»¥æ˜¯å¦æ‰¾åˆ°æœ€ä¼˜å¾ˆä¾èµ–äººä¸ºé€‰æ‹©çš„èŒƒå›´ï¼Œå¹¶ä¸”åªæ˜¯è¿›è¡ŒèŒƒå›´å†…çš„compareï¼Œå¹¶ä¸ç¡®å®šæ˜¯å¦å…¨å±€æœ€å°
2. å®¹æ˜“loss controlï¼Œå¦‚æœæ˜¯2ä¸ªhyper-parameterï¼Œæ¯ä¸ªæœ‰3ä¸ªå¤‡é€‰éƒ½è¦è®­ç»ƒ$3^2$ä¸ªmodel

#### Random Search

**Idea** ==Monte Carlo methodï¼Œè’™ç‰¹å¡æ´›æ³•ï¼Œç»Ÿè®¡æ¨¡æ‹Ÿæ³•==.
æ‰€æ±‚è§£é—®é¢˜å¯ä»¥è½¬åŒ–ä¸ºæŸç§éšæœºåˆ†å¸ƒçš„ç‰¹å¾æ•°ï¼Œæ¯”å¦‚éšæœºäº‹ä»¶å‡ºç°çš„æ¦‚ç‡ï¼Œæˆ–è€…éšæœºå˜é‡çš„æœŸæœ›å€¼ã€‚é€šè¿‡éšæœºæŠ½æ ·çš„æ–¹æ³•ï¼Œä»¥éšæœºäº‹ä»¶å‡ºç°çš„é¢‘ç‡ä¼°è®¡å…¶æ¦‚ç‡ï¼Œæˆ–è€…ä»¥æŠ½æ ·çš„æ•°å­—ç‰¹å¾ä¼°ç®—éšæœºå˜é‡çš„æ•°å­—ç‰¹å¾ï¼Œå¹¶å°†å…¶ä½œä¸ºé—®é¢˜çš„è§£ã€‚è¿™ç§æ–¹æ³•å¤šç”¨äºæ±‚è§£å¤æ‚çš„å¤šç»´ç§¯åˆ†é—®é¢˜ã€‚

**Loop: 1. Random guess 2. Check and compare 3. Update.**

1. provide statistical **distributions** of hyperparameter values äººä¸ºè®¾å®šè¶…å‚æ•°çš„åˆ†å¸ƒ
2. **Random Search** **randomly** **samples** hyperparameter values from the defined distributions and then tests them by generating a model. **Random Search** å°±ä¼šåœ¨è®¾å®šåˆ†å¸ƒå†…**éšæœºsample**è¿›è¡Œè®­ç»ƒå¹¶æ ¹æ®äººä¸ºæŒ‡å®šçš„metricè¿›è¡Œassess

**Advantageï¼š**

1. éšæœºæœç´¢æœ‰æ•ˆåœ°æœç´¢äº†æ¯”ç½‘æ ¼æœç´¢æ›´å¤§çš„é…ç½®ç©ºé—´ã€‚å› ä¸ºæ˜¯éšæœºå–æ ·
2. æ‰¾åˆ°è¿™äº›æ˜¾æ€§è¶…å‚æ•°çš„æœ€ä½³å€¼å°†æ¯”è·å¾—æ‰€æœ‰è¶…å‚æ•°çš„æœ€ä½³ç»„åˆæ›´æœ‰æ€§ä»·æ¯”
3. é‡è¦çš„è¶…å‚æ•°å› æ•°æ®é›†è€Œå¼‚ã€‚ç½‘æ ¼æœç´¢å°±ä¼šå¾ˆéš¾å…·ä½“åˆ¶å®šæŸä¸€ä¸ª

## tuning parameter work

![](./pics/LRs_12.png)
![](./pics/LRs_13.png)
