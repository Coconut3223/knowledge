# (Apache åŸºé‡‘ä¼šçš„) Hadoop

==Hadoop==æ˜¯ä¸€ä¸ª **å®ç°äº†mapreduceç®—æ³•** çš„å¼€æºçš„åˆ†å¸ƒå¼å¹¶è¡Œç¼–ç¨‹æ¡†æ¶ã€‚open-source implementation of frameworks for reliable, scalable, distributed computing and data storage.

**Goals/Requirements:**

1. Abstract with high scalability and availability å®ƒæ˜¯ä¸€ä¸ª**é«˜å¯æ‰©å±•æ€§å’Œå¯ç”¨æ€§**æ¡†æ¶
2. Facilitate the storage and processing of large and/or rapidly growing data sets å¯¹å¤§æ•°æ®è¿›è¡Œå‚¨å­˜å’Œå¤„ç†
    - å¤§æ•°æ®åŒ…æ‹¬ ç»“æ„åŒ–å’Œéç»“æ„åŒ– Structured and non-structured data
    - Processä¸­ï¼šMove computation rather than data æ›´å¤šçš„è¿˜æ˜¯æ•°æ®çš„ç§»åŠ¨è¯»å–
3. Use commodity hardware with little redundancy å°±æ˜¯è¯´é‚£ç§æœ€åŸºæœ¬é…ç½®çš„æ ‡é…å‡ºå”®çš„ä¸œè¥¿ï¼Œå› ä¸ºä¸åŠ ä»»ä½•é…ç½®å°±æ˜¯æ ‡å‡†æœ‰ä¸€å®šèƒ½åŠ›ä½†æ˜¯ä¸è´µ
    - Simple programming models
4. **Fault-tolerance** å®¹é”™æ€§æ˜¯æœ€é‡è¦çš„

## architecture

**Hadoop Ecosystemï¼š**
==Hadoop Common==ã€‚ A set of components and interfaces for distributed file systems and general I/O; ç»„ä»¶å’Œæ¥å£
==Hadoop Distributed File System, HDFS==ã€‚ A distributed file system that runs on large clusters of commodity machines; åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ
==Hadoop MapReduce==ã€‚A distributed data processing model and execution environment that runs on large clusters of commodity machines;

**Hadoop Frameworkï¼š**
==HBase==ã€‚A distributed, column-oriented database.
Table storage for semi-structured data

- Row/column store
- Billions of rows/millions on columns
- Column-oriented - nulls are free
- Untyped - stores byte

==Zookeeper==ã€‚Coordinating distributed applicationsã€‚

==Pig==ã€‚High-level language for data analysis

- Expresses sequences of MapReduce jobs
- Data model: nested â€œbagsâ€ of items
- Provides relational (SQL) operators (JOIN, GROUP BY, etc.)
- Easy to plug in Java functions

==Hive==ã€‚SQL-like Query language and Meta-store

- Used for majority of Facebook jobs
- â€œRelational databaseâ€ built on Hadoop
- Maintains list of table schemas
- SQL-like query language (HiveQL)
- Can call Hadoop Streaming scripts from HiveQL
- Supports table partitioning, clustering, complex data types, some optimizations

==Mahout==ã€‚Machine learning

![](./pics/Hd_1.png)
![](./pics/Hd_2.png)

### Hadoop Distributed File System

**Problems and Solutions of HDFS.**

- Data is too big to store on one machine
ğŸ’¡HDFS: store the data on **multiple machines**
- High end machines are too expensive
ğŸ’¡HDFS: Run on commodity hardware
- Many commodig hardnare will fail occasional
ğŸ’¡HDFS: Intelligent enaugh to handle harduard failure
- How to handle date on failed hardward
ğŸ’¡HDFS: Data replication

**Goals of HDFS:**

- Verg Large Distributed File System ==Scalability==
10K nodes, 100 million files, 10PB
- Assumes Commodity Hardware ==Failure Tolerane==
    - Files ane replicated to handle hardware  failures
    - Detect failures and recover from them
- Optimized from Batch Processing ==High Throughaut==
    - Data locations exposed so that computations can move to where data resides
    - Provide very high aggregate bandwidth
- With interfaces to move application operation closer to data to reduce data communication costs
- HDFS is designed to just work, to process Large data sets with write-once-read-many (WORM) semantics. it's not for low latency access
- Can be built out of commodity hardware

**Distributed**, with some **centralization**

==A Hadoop Cluster== includes a single master and multiple slave nodes.

Main nodes run ==TaskTracker TT== to accept and reply to MapReduce tasks, and alsoÂ ==DataNode DN==Â to store needed blocks closely as possible
Central control node runsÂ ==NameNode==Â to keep track of HDFS directories & files, and ==JobTrack== to dispatch compute tasks toÂ TaskTracker.
**When you create a MapReduce job**, at the very beginning, you have to seek the metadata from Namenode. Normally the namenode will maintain the metadata into his main memory, so we can search. You can retrieve the metadata immediately because everything is on the main memory. Normally on one namenode okay.
==Main nodes== of cluster are where most of the computational power and storage of the system lies

- No need for RAID on normal nodes (Raid is a very expensive storage system supporting parallel IO operations.)
- **Data Coherency**Â Write-once-read-many (WORM) access modelÂ Client can only append (not override) to existing files

![](./pics/hdfs_1.png)
![](./pics/hdfs_2.png)
![](./pics/hdfs_3.png)
![](./pics/hdfs_4.png)
![](./pics/hdfs_5.png)

#### Architecture

|  |  | ä»æ¶æ„ä½ç½®åˆ† |
| --- | --- | --- |
| Central Control Node <br>The Master | NameNode | keep track of HDFS directories & files, Central manager for the file system Namespace |
| ^ | JobTracker | dispatch compute tasks toÂ TaskTracker, Central manager for running MapReduce jobs |
| The Slaves | Main nodes<br>TaskTracker, TT | accept and reply to MapReduce tasks. accept and run map, reduce and shuffle |
| ^ | DataNode, DN | store needed blocks closely as possible |

|  |   | ä»åŠŸèƒ½åˆ† |
| --- | --- | --- |
| Store Data | NameNode | metadata, Transaction log, |
| ^ | DataNode, DN | actual data  |
| MapReduce Engine | JobTracker | splits up data into smaller tasks(â€œMapâ€) sends it to the TaskTracker process in each node |
| ^ | TaskTracker, TT | reports back to the JobTracker node;reports on job progress;sends data (â€œReduceâ€) or requests new jobs |

##### DataNode, DN

- Individual machines in the cluster
- Typically 2 level architecture. ä¸¤å±‚æ¶æ„
- 30-40 nodes/rack. ä¸€ä¸ªracké‡Œ30-40ä¸ªnodes
- Nodes are commodity PCs. éƒ½æ˜¯å•†å“PC
- Uplink from one rack is 3-4 gigabit.Â rackä¹‹é—´äº’ä¼ æ˜¯3-4åƒå…†
- Rack-internal is 1 gigabit. rackå†…éƒ¨çš„nodes æ˜¯1åƒå…†
- Can run on any underlying filesystem (ext3/4, NTFS, etc) å¯ä»¥åœ¨ä»»ä½•åº•å±‚å¹³å°ä¸Šä½¿ç”¨

**with Client:**

- Serves data and metadata to Clients
- Periodically sends a report of all existing blocks to the NameNode å‘Šè¯‰NameNodeè‡ªå·±å­˜äº†å•¥æ•°æ®

**Store:**

- actual data in the local file system(e.g. ext3) åœ¨æœ¬åœ°å‚¨å­˜å®é™…æ•°æ®
- Stores metadata of local block (e.g. CRC)
- NameNode replicates blocks 2x in local rack, 1x elsewhere. å…³äºè‡ªå·±è¿™ä¸€å°rackçš„local dataä¼šåœ¨è‡ªå·±è¿™å­˜ä¸¤ä»½å¤‡ä»½ï¼Œä¸€ä»½åœ¨åˆ«çš„racké‚£é‡Œï¼Œç”±NameNodeè®°å½•

**with others DN:**

- Forwards data to other specified DataNodes

##### NameNode

- **Single Namespace for entire cluster**
- The server holding the NameNode instance is **quite crucial**, as there is **only one**. éå¸¸é‡è¦åªæœ‰ä¸€ä¸ª

**Store:**

- **metadata for the files** å‚¨å­˜å…ƒæ•°æ®
    - like **the directory structure** of a typical FS(file system). ï¼Œæ¯”å¦‚è¯´dataçš„ç¼–å·å’Œdataçš„å‚¨å­˜ä½ç½®ã€‚ä¸å‚¨å­˜dataï¼Œè€Œæ˜¯dataçš„æ¬è¿å·¥ï¼ŒçŸ¥é“åœ¨å“ªé‡Œç„¶åæ¬åˆ°DataNode
    - **The entire metadata is in main memory**
    - **No demand paging of metadata**
    - List of files, List of Blocks for each file, List of DataNodes for each block, File attributes, e.g., creation time, replication factor
- **Transaction log** for file deletes/adds, etc.
    - Does not use transactions for whole blocks or file streams, **only metadata**. æ–‡ä»¶åˆ é™¤/æ·»åŠ ç­‰çš„äº‹åŠ¡æ—¥å¿—ã€‚ä¸å°†äº‹åŠ¡ç”¨äºæ•´ä¸ªå—æˆ–æ–‡ä»¶æµï¼Œä»…ä½¿ç”¨å…ƒæ•°æ®ã€‚ç±»ä¼¼åƒåœ¾æ¡¶ï¼Œåœ¨å…ƒæ•°æ®é‚£é‡Œæ ‡æ“ä½œï¼Œæ„ä¹‰ä¸Štransactionï¼Œä½†å®é™…å‚¨å­˜ä¸Šè¿˜æ˜¯é‚£æ ·

**with client:**

- is responsible for maintaining the file namespace and directing clients to datanodes;

**with DataNode:**

- Maps a file name to a set of blocks
- Maps a block to the DataNodes where it resides
- Handles **creation of more replica blocks** when necessary after a DataNode failureã€‚
    - **Replication Engine for Blocks**
    - å½“æŸä¸ªDataNodeæŒ‚äº†ä¹‹åå°±æ¬è¿dataâ€”â€”åˆ›å»ºå‰¯æœ¬ï¼Œç¡®ä¿dataæœ‰å®šé‡çš„replications in DataNode
- **Cluster Configuration Management**

#### Strategy

##### Block Placement - Data Pipeline

- **Large blocks stored across a cluster** æ¯ä¸€ä¸ªfileéƒ½è¢«åˆ†æˆå¾ˆå¤šä¸ªblockå‚¨å­˜åœ¨ä¸€ä¸ªé›†ç¾¤
    - Individual files are broken into blocks of a fixed size (64MB by default);
    - Blocks are stored across a cluster of one or more machines;
    - **High degree of data replication on multiple DataNodes** æ¯å—éƒ½è¢«é«˜åº¦å¤åˆ¶å‚¨å­˜åœ¨ä¸åŒDataNodeä¸Š
    - è‡³å°‘ä¸‰ä»½ (3x by default)Â ï¼š1st on local nodeï¼›2nd on a remote rackï¼›3rd on the same remote rackï¼›Additional replicas are randomly placed
- **Blocks are moved primarily across DataNodes** å—çš„å¤‡ä»½çš„å†™å…¥ä¸»è¦é DNä¹‹é—´ä¼ é€’
    - clientÂ retiresÂ aÂ listÂ ofÂ DataNodesÂ onÂ whichÂ toÂ place replicasÂ ofÂ aÂ blockÂ fromÂ NameNodes
    - Client writes block to the first DataNode
    - The first DataNode forwards the data to the next node in the
    - When all replicas are written, the Client moves on to write the next block in file

![](./pics/hdfs_6.png)

- **Nearest replicas are retrieved by Client in network** å®¢æˆ·ç«¯æ£€ç´¢æœ€è¿‘å¤‡ä»½çš„ä½ç½®ï¼Œé€šè¿‡ç½‘ç»œç›´æ¥è®¿é—®
    - ClientÂ connectsÂ toÂ NN toÂ requestÂ toÂ read data
    - NNÂ tells clientÂ whichDNS to find theÂ data block
    - ClientÂ readÂ blocksÂ ofÂ **nearest**Â replicaÂ **directly** fromÂ DNS
    - InÂ caseÂ ofÂ node failures.Â clientsÂ connectsÂ toÂ another DNÂ thatÂ seversÂ theÂ missingÂ block
    - Client accesses data directly from DataNode
        - **why notÂ ask clients toÂ read blocks through NN?**
            - presentÂ NNÂ fromÂ beingÂ theÂ bottleneckÂ ofÂ theÂ cluster
            - Allow HDFS to scale toÂ large number of concurrent clients
            - Spreadthe dataÂ trafficÂ acrossÂ theÂ cluster

### Mapreduce

å› ä¸º Targeted towards many reads of filestreams, Writes are more costly é’ˆå¯¹åœ¨data process è¯»æ¯”å†™æ›´é‡è¦
MapReduce is very good at dealing with warm proper property: Read once and write once and read many times right.

- Written in Java, also supports Python and Ruby

**Fine-grained Map and Reduce tasks:**

- Improved load balancing
- Faster recovery from failed tasks

### Parallelism in Hadoop

1. multi-threadedï¼Ÿ
No need to handle multi-threaded code
Each Mapper / Reducer is typically single threaded
Allows for restarting of failed jobs
Runs entirely independent of each other in separate JVMs (Java Virtual Machines)

### Load Balancing

Goal: % disk full on DataNodes should be similar
ä¸è¶…è¿‡balanceçš„æ—¶å€™ï¼šå°±åŠ æ›´å¤š Mappers/Reducers!

**Map:**

Usually as many as the number of HDFS blocks being processed, this is the default, else the number of maps can be specified as a hint. The number of maps can also be controlled by specifying the minimum split size. The actual sizes of the map inputs are computed by:Â $\max(\min(\text{\#block},\cfrac{data}{\#maps}), min\_split\_size$

**Reduces:**

Unless the amount of data being processed is small
0.95*num_nodes*mapred.TaskTracker.tasks.maximum
What happens if the number of reducers is less than the distinct of number keys?
The total running time is dominated by Reducer who deals with most intermediate pairs
Solution: use domain knowledge to decide the partitioning scheme
åˆ°äº†éœ€è¦åŠ DataNodeçš„æ—¶å€™ï¼š

Usually run when new DataNodes are added

Cluster is online when Rebalancer is active

Rebalancer is throttled to avoid network congestion

### Locality Optimization, local ä¼˜åŒ–

Because some of machines are very busy, I cannot assign so many reduced to you the whole job, right? So this system level optimization should be considered locality optimization.

<u>Problem</u>ï¼šFor large data, bandwidth to data is a problem forÂ **Batch Processing**
By one reading, everything will be transferred from disc into the main memory. So this server time are expensive
<u>Solution</u>ï¼šMap-Reduce + HDFS is a very effective with the **Master scheduling policy**

**Master scheduling policy:**

- Map-Reduce queries HDFS for locations of input data
- Map tasks scheduled so HDFS input block replicas are on the same machine or the same rack
- Map tasks are scheduled close to the inputs when possible

**Benefits:**

- Thousands of machines read input at local disk speedï¼Œ
    - It's not random access partial from here, partial from there, take a lot of time, it's everything is already local localised on the same place and you just by once reading everything can be available.
- Provides very high aggregate bandwidth
- Eliminate network bottleneck æ¶ˆé™¤ç½‘ç»œé€Ÿåº¦ç“¶é¢ˆ
Bandwideh at Different Lenels
Process on the same node
Different nodes on the same rack
Nodes on different racks in the same data center( cluster)
Nodes in different centers

### Fault Tolerance

!!! p "é¦–å…ˆæ˜ç¡®ä¸€ç‚¹ï¼šFailure is norm, not an exception"

æˆ‘ä»¬é‡‡å–çš„æ˜¯åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œè¿™ä¹ˆå¤šå°PCéƒ½åŒæ—¶éƒ½ä¸å‡ºé—®é¢˜æ¦‚ç‡ä¸º0ï¼Œå“ªæ€•ä¸€å°æŠ¥é”™æ¦‚ç‡æå°ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸€å®šè¦åšå¥½ç¡®ä¿ **Fault Tolerance** çš„å·¥ä½œï¼ŒåŒ…æ‹¬:

1. replication æœ‰å¤‡ä»½
2. checksum æ—¶åˆ»æ£€æŸ¥æ˜¯å¦å‡ºé”™
3. Data transfer bandwidth is critical (location of data) æ•°æ®ä¼ è¾“å¸¦å®½è‡³å…³é‡è¦ï¼ˆæ•°æ®ä½ç½®ï¼‰

**Automatic re-execution on failure**Â§Â In a large cluster, some nodes are always slow or flaky

Framework re-executes failed tasks

| Failure | Failure Recovery |
| --- | --- |
| Worker failure | Detect DataNode failure via periodic heartbeats  |
|  | Detect Data correctness via Checksums |
|  | Re-execute in-progress map/reduce tasks |
| Master failure | Resume from Execution Log |

**Worker failure:**

**DataNode** failure

**Detect** via **periodic heartbeats**

DataNodes send heartbeats a Block Report fromÂ eachÂ DataNode to the NameNode. Once every 3 seconds via a TCP handshake

**Recover.**

Chooses new DataNodes for new replicas
Balances disk usage
Balances communication traffic to DataNodes

**Data** correctness

**Detect** via **Checksums**

Client computes checksum per 512 bytes.Â Â DataNode stores the checksum

**Master failure:**

å±äº Single point of failure å•ç‚¹æ•…éšœ
Transaction Log stored in multiple directories:
A directory on the local file system
A directory on a remote file system (NFS/CIFS)
Need to develop a real HA (high availability) solution

****Secondary NameNode****

åŒæ­¥é‹è¡Œ The other can be replaced immediately without interrupting.

- Copies FSImage and Transaction Log from NamNnode to a temporary directory
- Merges FSImage and Transaction Log into a new FSImage in temporary directory
- Uploads new FSImage to the NameNodeÂ§Â Transaction Log on NameNode is purged

![](./pics/hdfs_7.png)
![](./pics/hdfs_8.png)
